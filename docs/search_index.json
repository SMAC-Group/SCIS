[
["index.html", "An Introduction to Inertial Sensors Stochastic Calibration Chapter 1 Introduction", " An Introduction to Inertial Sensors Stochastic Calibration Stéphane Guerrier, Roberto Molinari, Haotian Xu, Ahmed Radi, Gaetan Bakalli, Yuming Zhang and Mucyo Karemera 2018-05-19 Chapter 1 Introduction TO DO (Introduction to the text) "],
["timeseries.html", "Chapter 2 Introduction to Time Series Analysis", " Chapter 2 Introduction to Time Series Analysis TO DO "],
["a-review-of-the-properties-of-statistical-estimators.html", "Chapter 3 A Review of the Properties of Statistical Estimators", " Chapter 3 A Review of the Properties of Statistical Estimators TO DO "],
["allan-variance-calibration-techniques.html", "Chapter 4 Allan Variance Calibration Techniques 4.1 Spectral Ambiguity of the AV 4.2 Properties of the Allan Variance 4.3 Estimation", " Chapter 4 Allan Variance Calibration Techniques The Allan Variance (AV) is a statistical technique originally developed in the mid-1960s to study the stability of precision oscillators (see e.g. Allan 1966). It can provide information on the types and magnitude of various superimposed noise terms (i.e. composite stochastic processes). This method has been adapted to characterize the properties of a variety of devices including inertial sensors (see El-Sheimy, Hou, and Niu 2008). The AV is a measure of variability developed for long term memory processes and can in fact be interpreted as a Haar wavelet coefficient variance (see Percival and Guttorp 1994). We will discuss this connection further on. Definition 4.1 (Allan Variance) We consider the AV at dyadic scales (\\(\\tau_j\\)) starting from local averages of the process which can be denoted as \\[\\begin{equation*} \\bar{X}_{t}^{(j)} \\equiv \\frac{1}{\\tau_j} \\sum_{i = 1}^{\\tau_j} X_{t - \\tau_j + i}\\, , \\label{mean.noav} \\end{equation*}\\] where \\(\\tau_j \\equiv 2^j, \\; j \\in \\left\\{x \\in \\mathbb{N} \\, : \\; 1 \\leq x &lt; \\log_2 (T) - 1 \\right\\}\\) therefore determines the number of consecutive observations considered for the average. Then, the AV is defined as \\[\\begin{equation*} \\text{AV}_j \\left(X_t \\right) \\equiv \\frac{1}{2} \\, \\mathbb{E}\\left[ \\left(\\bar{X}_{t}^{(j)} - \\bar{X}_{t-\\tau_j}^{(j)} \\right)^2 \\right]. \\end{equation*}\\] Remark 4.1 (Alternative scale definition) The definition of the AV is actually valid for \\(\\tau_j = \\lfloor2^j\\rfloor\\) with \\(j \\in \\left\\{x \\in \\mathbb{R} \\, : \\; 1 \\leq x &lt; \\log_2 (T) - 1 \\right\\}\\). In some case, it could use to consider this alternative definition (see e.g. El-Sheimy, Hou, and Niu 2008) but we shall restrict ourself here to the case where \\(j \\in \\left\\{x \\in \\mathbb{N} \\, : \\; 1 \\leq x &lt; \\log_2 (T) - 1 \\right\\}\\). Remark 4.2 (Notation of the Allan Variance) For notational simplicity, we may sometimes replace \\(\\text{AV}_j \\left(X_t \\right)\\) by simply \\(\\phi_j^2\\) when the dependence of the AV to the process \\((X_t)\\) is evident. As highlighted earlier, the AV is, among others, a widely and commonly used approach in engineering for sensor calibration as it is linked to the properties of the process \\((X_t)\\) as shown in the following lemma (see e.g. Percival and Walden 2006 for a proof). Lemma 4.1 (AV connection to PSD) For a stationary process \\((X_t)\\) with PSD \\(S_{X}(f)\\) we have \\[\\begin{equation*} \\phi_j^2 \\equiv \\text{AV}_j \\left(X_t \\right) = 4 \\int_0^{\\infty} \\frac{\\sin^4(\\pi f \\tau_j)}{(\\pi f \\tau_j)^2} S_{X}(f) df. \\label{eq:allanvariancePSD_LInk} \\end{equation*}\\] Therefore, this result establishes a direct connection between the AV and PSD. A natural question is therefore whether the mapping PSD \\(\\mapsto\\) AV is one-to-one. Greenhall (1998) (see Theorem 1) showed that this is actually not the case. This is illustrated in the following section ????? (ADD REF). 4.1 Spectral Ambiguity of the AV Consider two (independent) stochastic processes \\((X_t)\\) and \\((Y_t)\\) with respective PSD \\(S_X(f)\\) and \\(S_Y(f)\\). Suppose that \\(S_X(f) \\neq S_Y(f)\\), then the two processes will have the same AV if \\[\\begin{equation*} \\Delta \\equiv \\int_0^{\\infty} \\frac{\\sin^4(\\pi f \\tau_j)}{(\\pi f \\tau_j)^2} \\Phi(f) df = 0, \\end{equation*}\\] where \\(\\Phi(f) \\equiv S_{X}(f) - S_{Y}(f)\\). To show that it is possible that \\(\\Delta = 0\\) when \\(\\Phi(f) \\neq 0\\), we will use the following critical identity: \\[\\begin{equation} \\label{ident:av:indet} \\sin^4(x) = \\sin^2(x) - \\frac{1}{4} \\sin^2(2x). \\end{equation}\\] First, we note that \\(\\Delta\\) may be expressed using () as follows: \\[\\begin{equation*} \\begin{aligned} \\Delta &amp;= \\int_{0}^{\\infty} \\frac{\\sin^4\\left(\\tau \\pi f \\right)}{\\left(\\tau \\pi f \\right)^2} \\Phi(f) df \\\\ &amp;= \\lim_{n \\rightarrow -\\infty} \\int_{2^{n}}^{\\infty} \\frac{\\sin^2\\left(\\tau \\pi f \\right) - \\frac{1}{4} \\sin^2\\left(2 \\tau \\pi f \\right) }{\\left(\\tau \\pi f \\right)^2} \\Phi(f) df . \\end{aligned} \\end{equation*}\\] Second, by the change of variable \\(u = 2f\\) in the second term we obtain \\[\\begin{equation*} \\begin{aligned} \\Delta = \\lim_{n \\rightarrow -\\infty} &amp; \\Bigg[ \\int_{2^{n}}^{\\infty} \\frac{\\sin^2\\left(\\tau \\pi f \\right)}{\\left(\\tau \\pi f \\right)^2} \\Phi(f) df - \\frac{1}{2}\\int_{2^{n+1}}^{\\infty} \\frac{\\sin^2\\left(\\tau \\pi u \\right)}{\\left(\\tau \\pi u \\right)^2} \\Phi(f) du \\Bigg]. \\end{aligned} \\end{equation*}\\] Now suppose that \\(\\Phi(f) = 2 \\Phi(2f)\\). In this case, we have \\(\\Phi(f) = 2 \\Phi(u)\\) and therefore we obtain \\[\\begin{equation*} \\begin{aligned} \\Delta &amp;= \\lim_{n \\rightarrow -\\infty} \\int_{2^{n}}^{2^{n+1}} \\frac{\\sin^2\\left(\\tau \\pi f \\right)}{\\left(\\tau \\pi f \\right)^2} \\Phi(f) df = 0. \\end{aligned} \\end{equation*}\\] Remark 4.3 This result demonstrates that the mapping from PSD to Allan variance is not necessarily one-to-one. Greenhall (1998) showed that in the continuous case (i.e. \\(\\tau_j \\in \\mathbb{R}\\)) \\(\\Delta = 0\\) if and only if \\(\\Phi(f) = 2 \\Phi(2f)\\). However, the ``only if’’ part of this results (while conjectured) is unknown in the discrete case. 4.2 Properties of the Allan Variance One reason of explaining the widespread use of the Allan variance for sensor calibration is due to the following additivity property, which is particularly convenient to identify composite stochastic processes (see Definition REF MISSING!). Corollary 4.1 (Additivity of the AV) Consider two (independent) stochastic processes \\((X_t)\\) and \\((Y_t)\\) with respective PSD \\(S_X(f)\\) and \\(S_Y(f)\\). Suppose that we observe the process \\(Z_t = X_t + Y_t\\). Then, we have \\[\\begin{equation*} \\text{AV}_j \\left(Z_t \\right) = \\text{AV}_j \\left(X_t \\right) + \\text{AV}_j \\left(Y_t \\right). \\end{equation*}\\] Proof. The proof of this result is direct from Lemma REF MISSING HERE. Indeed, since \\(S_Z(f) = S_X(f) + S_Y(f)\\), we have \\[\\begin{equation*} \\begin{aligned} \\text{AV}_j \\left(Z_t \\right) &amp;= 4 \\int_0^{\\infty} \\frac{\\sin^4(\\pi f \\tau_j)}{(\\pi f \\tau_j)^2} S_{Z}(f) df\\\\ &amp;= 4 \\int_0^{\\infty} \\frac{\\sin^4(\\pi f \\tau_j)}{(\\pi f \\tau_j)^2} S_{X}(f) df + 4 \\int_0^{\\infty} \\frac{\\sin^4(\\pi f \\tau_j)}{(\\pi f \\tau_j)^2} S_{Y}(f) df\\\\ &amp;= \\text{AV}_j \\left(X_t \\right) + \\text{AV}_j \\left(Y_t \\right). \\end{aligned} \\end{equation*}\\] While Lemma 4.1 is an important results which is very convenient to determine the theoretical AV of a certain stochastic process. However, the applicability of this results is often limited since the integral defined in (REF MISSING HERE) can be intractable. An alternative to Lemma 4.1 has been proposed by Zhang (2008) and is far advantageous from a computational standpoint. Lemma 4.2 (AV connection to ACF) For a stationary process \\((X_t)\\) with variance \\(\\sigma^2_X\\) and ACF \\(\\rho(h)\\) we have \\[\\begin{equation*} \\label{stat.av} \\text{AV}_j \\left(X_t \\right) = \\frac{\\sigma_X^2}{\\tau_j^2} \\bigg(\\tau_j\\left[1-\\rho(\\tau_j)\\right] + \\sum_{i=1}^{\\tau_j-1} i \\left[2 \\rho(\\tau_j-i) - \\rho(i) - \\rho(2\\tau_j-i)\\right]\\bigg). \\end{equation*}\\] The proof of this result is instructive and is presented in Xu et al. (2017). Remark 4.4 Using Lemma 4.2, the exact form of the AV for different stationary processes, such as the general class of ARMA models, can easily be derived. Moreover, Zhang (2008) provided the theoretical AV for non-stationary processes such as the random walk and ARFIMA models for which the AV, as mentioned earlier, represents a better measure of uncertainty compared to other methods. Remark 4.5 Lemma 4.2 was extended to non-stationary processes in Xu et al. (2017). Example 4.1 (Theoretical AV of an MA(1) process) From the autocovariance we obtain \\[\\begin{equation*} \\rho(h) = \\text{corr}\\left(X_t, X_{t-h} \\right) =\\left\\{ \\begin{array}{cl} 1 &amp;\\text{if } h = 0\\\\ \\frac{\\theta}{1 + \\theta^2} &amp;\\text{if } |h| = 1\\\\ 0 &amp;\\text{if } |h| &gt; 1.\\\\ \\end{array} \\right. \\end{equation*}\\] We can now apply the formula given in Lemma 4.2, which leads to \\[\\begin{equation*} \\begin{aligned} \\text{AV}_j \\left(X_t \\right) &amp;= \\frac{\\left(1 + \\theta^2 \\right) \\sigma^2}{\\tau_j^2} \\bigg(\\tau_j + \\sum_{i=1}^{\\tau_j-1} i \\left[2 \\rho(\\tau_j-i) - \\rho(i) - \\rho(2\\tau_j-i)\\right]\\bigg)\\\\ &amp;=\\frac{\\left(1 + \\theta^2 \\right) \\sigma^2}{\\tau_j^2} \\bigg(\\tau_j + 2 \\sum_{i=1}^{\\tau_j-1} i \\rho(\\tau_j-i) -\\sum_{i=1}^{\\tau_j-1} i \\rho(i) - \\sum_{i=1}^{\\tau_j-1} i \\rho(2\\tau_j-i)\\bigg)\\\\ &amp;=\\frac{\\left(1 + \\theta^2 \\right) \\sigma^2}{\\tau_j^2} \\left(\\tau_j + 2 (\\tau_j - 1) \\rho(1) - \\rho(1) \\right)\\\\ &amp;=\\frac{\\left(1 + \\theta^2 \\right) \\sigma^2}{\\tau_j^2} \\bigg(\\tau_j + (2\\tau_j - 3) \\frac{\\theta}{1 + \\theta^2} \\bigg). \\end{aligned} \\end{equation*}\\] \\(\\LARGE{\\bullet}\\) 4.3 Estimation Several estimators of the AV have been introduced in the literature. The most commonly is (probably) the Maximum-Overlapping AV (MOAV) estimator proposed by Percival and Guttorp (1994), which is defined as follows: Definition 4.2 (Maximum-Overlapping AV Estimator) The MOAV is defined as: \\[\\begin{eqnarray} \\label{eq:MOAVNS_est} \\hat{\\phi}_j^2 \\equiv \\widehat{\\text{AV}}_j \\left(X_t \\right) = \\frac{1}{2 \\left(T - 2\\tau_j + 1\\right)} \\sum_{k = 2 \\tau_j}^{T} \\left(\\bar{X}_{k}^{(j)} - \\bar{X}_{k-\\tau_j}^{(j)} \\right)^2. \\end{eqnarray}\\] We will now study the properties of this estimator through the following lemmas. Lemma 4.3 (Consistency) Let \\((X_t)\\) be such that: \\((X_t - X_{t-1})\\) is a (strongly) stationary process, \\((X_t - X_{t-1})^2\\) has absolutely summable covariance structure, \\(\\mathbb{E}\\left[(X_t - X_{t-1})^4\\right] &lt; \\infty\\), Then, we have \\[\\widehat{\\text{AV}}_j \\left(X_t \\right) \\overset{ \\mathcal{P} }{\\longrightarrow} \\text{AV}_j \\left(X_t \\right).\\] Proof. TO BE ADDED References "],
["the-generalized-method-of-wavelet-moments.html", "Chapter 5 The Generalized Method of Wavelet Moments", " Chapter 5 The Generalized Method of Wavelet Moments "],
["extensions.html", "Chapter 6 Extensions", " Chapter 6 Extensions "],
["references.html", "References", " References "]
]
