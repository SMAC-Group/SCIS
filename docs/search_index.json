[
["allan-variance-calibration-techniques.html", "Chapter 4 Allan Variance Calibration Techniques 4.1 Review on MLE-based Methods 4.2 An Introduction of Allan Variance 4.3 Properties of the Allan Variance 4.4 Estimation 4.5 Allan variance based estimation", " Chapter 4 Allan Variance Calibration Techniques In this chapter, we will study the Allan variance (AV) and the corresponding calibration techniques. For this purpose, this chapter is organized in the following order: Review on MLE-based methods; An introduction of Allan variance; Allan variance-based estimation. 4.1 Review on MLE-based Methods In general, inertial sensor stochastic calibration involves the estimation of the parameters of composite stochastic processes. These models are typically difficult to estimate because of their latent features. In the definition below we characterize the class of composite stochastic processes we shall consider here. Definition 4.1 (Composite Stochastic Processes for IMU Calibration) Let \\((X_t)\\) be a sum of latent independent stochastic process such that: \\((X_t)\\) is made of a sum which includes a subset or all processes in the set \\(\\{\\)QN, WN, AR1, RW, DR\\(\\}\\), where processes in the subset \\(\\{\\)QN, WN, RW, DR\\(\\}\\) can be included only up to once and process AR1 can be included \\(k\\) times (\\(0 \\leq k &lt; \\infty\\)). Let \\(\\mathcal{Q}\\) denote an arbitrary compact subset of \\(\\mathbb{R}^+\\). Then, the innovation process for processes WN, RW and AR1 have respective variances \\(\\sigma^2\\), \\(\\gamma^2\\) and \\(\\nu^2\\) such that \\(\\sigma^2, \\gamma^2 \\text{ and } \\nu^2 \\in \\mathcal{Q}\\) and processes QN and DR have \\(Q^2 \\in \\mathcal{Q}\\) and \\(|\\omega|\\in \\mathcal{Q}\\) respectively. There are three main class of estimation techniques that can be used for the estimation of the class of composite stochastic processes for IMU calibration. The methods are the following: Maximum likelihood based methods: Although these methods are optimal in theory, their applicability is extremely limited due to numerical reasons and tends to perform badly in practice. Allan variance-based methods: This class of method is the most popular approach for IMU calibration. However, they typically lead to inconsistent estimators and their finite sample performance is often much lower than GMWM-based technique. GMWM and related methods: In our (very biased) opinion, these techniques are currently the optimal choice for the estimation of the parameters of the class of processes considered in Definition 4.1. As we will see, this method is in fact a formalized version Allan variance-based methods. Maximum likelihood based approaches are generally inappropriate for the estimation of the class of processes considered in Definition 4.1. In this chapter, we shall avoid a technical discussion on likelihood based method and refer the readers to Stebler et al. (2011) and Guerrier, Molinari, and Balamuta (2016) for more details. Instead, we will consider an example to illustrate the numerical issues of this technique. Example 4.1 (MLE-based IMU calibration) Suppose we have a composite stochastic process composed of a WN and an AR1 process, i.e. \\[\\begin{equation*} \\begin{aligned} Y_t &amp;= \\phi_0 Y_{t-1} + Z_t, \\;\\;\\; Z_t \\overset{iid}{\\sim} \\mathcal{N}\\left(0, \\nu^2_0 \\right), \\\\ U_t &amp;\\overset{iid}{\\sim} \\mathcal{N}\\left(0, \\sigma^2_0 \\right), \\;\\;\\;\\; X_t = Y_t + U_t. \\end{aligned} \\end{equation*}\\] Then we want to estimate the parameter \\(\\boldsymbol{\\theta}_0 = \\left[\\phi_0 \\;\\; \\nu^2_0 \\;\\; \\sigma^2_0\\right]^T\\). Since the process is Gaussian, we have \\[\\begin{equation*} \\begin{aligned} \\mathbf{X} \\sim \\mathcal{N} \\left(\\mathbf{0}, \\boldsymbol{\\Sigma}(\\boldsymbol{\\theta}_0)\\right), \\end{aligned} \\end{equation*}\\] where \\(\\mathbf{X} \\equiv [X_1, ..., X_T]^T\\) and \\(\\boldsymbol{\\Sigma}(\\boldsymbol{\\theta}_0) \\equiv \\text{Cov}(\\mathbf{X})\\). Since \\(Y_t\\) and \\(U_t\\) are independent, we have \\[\\begin{equation*} \\boldsymbol{\\Sigma}(\\boldsymbol{\\theta}_0) = \\text{Cov}(\\mathbf{X}) = \\text{Cov}(\\mathbf{Y}) + \\text{Cov}(\\mathbf{U}) = \\frac{\\nu^2_0}{1 - \\phi_0^2} \\left[ \\phi_0^{|i-j|}\\right]_{i,j = 1, ..., T} + \\sigma^2_0 \\mathbf{I}_T, \\end{equation*}\\] where \\(\\mathbf{Y} \\equiv [Y_1, ..., Y_T]^T\\), \\(\\mathbf{U} \\equiv [U_1, ..., U_T]^T\\) and where \\(\\mathbf{I}_T\\) denotes the identity matrix of dimension \\(T\\). Note that the form of \\(\\text{Cov}(\\mathbf{Y})\\) is due to the autocovariance of an AR1 which has been discussed in Chapter 2. So we can now write the log-likelihood function of the model considered here which, up to a constant, can be expressed as \\[\\begin{equation*} \\mathcal{L}\\left(\\boldsymbol{\\theta} | \\mathbf{X} \\right) = - \\log \\left( \\det \\left( \\boldsymbol{\\Sigma}(\\boldsymbol{\\theta}) \\right)\\right) - \\mathbf{X}^T \\boldsymbol{\\Sigma}(\\boldsymbol{\\theta})^{-1} \\mathbf{X}. \\end{equation*}\\] Therefore, we can find the maximum likelihood estimator for \\(\\boldsymbol{\\theta}_0\\): \\[\\begin{equation*} \\hat{\\boldsymbol{\\theta}} = \\underset{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}}{\\text{argmax}} \\; \\mathcal{L}\\left(\\boldsymbol{\\theta} | \\mathbf{X} \\right) = \\underset{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}}{\\text{argmin}} \\; \\text{log} \\left( \\det \\left( \\boldsymbol{\\Sigma}(\\boldsymbol{\\theta}) \\right)\\right) + \\mathbf{X}^T \\boldsymbol{\\Sigma}(\\boldsymbol{\\theta})^{-1} \\mathbf{X}. \\end{equation*}\\] Unfortunately, the applicability of this estimator is essentially impossible when \\(T &gt; 10^5\\) since every evaluation of this the function \\(\\mathcal{L}\\left(\\boldsymbol{\\theta} | \\mathbf{X} \\right)\\) requires to invert a \\(T \\times T\\) matrix, which entails a considerable (and often unrealistic) computational burden. An alternative approach to compute maximum likelihood estimator for \\(\\boldsymbol{\\theta}_0\\) is based on the EM-algorithm of Dempster, Laird, and Rubin (1977). If the process \\((Y_t)\\) (or \\(U_t\\)) was observed, then we could easily estimate the parameters of our model by considering separately the likelihood of both processes. Since the composite process is a state-space model we could use the following approach: \\[\\begin{equation*} \\hat{\\boldsymbol{\\theta}} = \\underset{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}}{\\text{argmax}} \\; \\mathcal{L}\\left(\\boldsymbol{\\theta} | \\mathbf{X}, \\hat{\\mathbf{Y}}(\\boldsymbol{\\theta}) \\right), \\end{equation*}\\] where \\(\\hat{\\mathbf{Y}}(\\boldsymbol{\\theta})\\) denotes the estimation of \\(\\mathbf{Y}\\) (i.e. the states) based on a Kalman filter assuming \\(\\boldsymbol{\\theta}\\) to be the correct parameter vector. Unfortunately, this approach suffers from the same computational limitations as the maximum likelihood estimator approach. 4.2 An Introduction of Allan Variance 4.2.1 Definition of Allan Variance The Allan variance (AV) is a statistical technique originally developed in the mid-1960s to study the stability of precision oscillators (see e.g. Allan 1966). It can provide information on the types and magnitude of various superimposed noise terms (i.e. composite stochastic processes). This method has been adapted to characterize the properties of a variety of devices including inertial sensors (see El-Sheimy, Hou, and Niu 2008). The AV is a measure of variability developed for long term memory processes and can in fact be interpreted as a Haar wavelet coefficient variance (see D. B. Percival and Guttorp 1994). We will discuss this connection further on. Definition 4.2 (Allan Variance) We consider the AV at dyadic scales (\\(\\tau_j\\)) starting from local averages of the process which can be denoted as \\[\\begin{equation*} \\bar{X}_{t}^{(j)} \\equiv \\frac{1}{\\tau_j} \\sum_{i = 1}^{\\tau_j} X_{t - \\tau_j + i}\\, , \\label{mean.noav} \\end{equation*}\\] where \\(\\tau_j \\equiv 2^j, \\; j \\in \\left\\{x \\in \\mathbb{N} \\, : \\; 1 \\leq x &lt; \\log_2 (T) - 1 \\right\\}\\) and therefore determines the number of consecutive observations considered for the average. Then, the AV is defined as \\[\\begin{equation*} \\text{AV}_j \\left(X_t \\right) \\equiv \\frac{1}{2} \\, \\mathbb{E}\\left[ \\left(\\bar{X}_{t}^{(j)} - \\bar{X}_{t-\\tau_j}^{(j)} \\right)^2 \\right]. \\end{equation*}\\] Remark 4.1 (Alternative scale definition) The definition of the AV is actually valid for \\(\\tau_j = \\lfloor2^j\\rfloor\\) with \\(j \\in \\left\\{x \\in \\mathbb{R} \\, : \\; 1 \\leq x &lt; \\log_2 (T) - 1 \\right\\}\\). In some case, it could be used to consider this alternative definition (see e.g. El-Sheimy, Hou, and Niu 2008) but we shall restrict ourselves here to the case where \\(j \\in \\left\\{x \\in \\mathbb{N} \\, : \\; 1 \\leq x &lt; \\log_2 (T) - 1 \\right\\}\\). Remark 4.2 (Notation of the Allan Variance) For notational simplicity, we may sometimes replace \\(\\text{AV}_j \\left(X_t \\right)\\) by simply \\(\\phi_j^2\\) when the dependence of the AV to the process \\((X_t)\\) is evident. As highlighted earlier, the AV is, among others, a widely and commonly used approach in engineering for sensor calibration as it is linked to the properties of the process \\((X_t)\\) as shown in the following lemma (see e.g. D. B. Percival and Walden 2006 for the proof). Lemma 4.1 (AV connection to PSD) For a stationary process \\((X_t)\\) with PSD \\(S_{X}(f)\\) we have \\[\\begin{equation*} \\phi_j^2 \\equiv \\text{AV}_j \\left(X_t \\right) = 4 \\int_0^{\\infty} \\frac{\\sin^4(\\pi f \\tau_j)}{(\\pi f \\tau_j)^2} S_{X}(f) df. \\label{eq:allanvariancePSD_LInk} \\end{equation*}\\] Therefore, this result establishes a direct connection between the AV and PSD. Therefore a natural question is whether the mapping PSD \\(\\mapsto\\) AV is one-to-one. Greenhall (1998) (see Theorem 1) showed that this is actually not the case. This is illustrated in the followsing Section 4.2.2. 4.2.2 Spectral Ambiguity of the AV Consider two (independent) stochastic processes \\((X_t)\\) and \\((Y_t)\\) with respective PSD \\(S_X(f)\\) and \\(S_Y(f)\\). Suppose that \\(S_X(f) \\neq S_Y(f)\\), then the two processes will have the same AV if \\[\\begin{equation*} \\Delta \\equiv \\int_0^{\\infty} \\frac{\\sin^4(\\pi f \\tau_j)}{(\\pi f \\tau_j)^2} \\Phi(f) df = 0, \\end{equation*}\\] where \\(\\Phi(f) \\equiv S_{X}(f) - S_{Y}(f)\\). To show that it is possible that \\(\\Delta = 0\\) when \\(\\Phi(f) \\neq 0\\), we will use the following critical identity: \\[\\begin{equation*} \\sin^4(x) = \\sin^2(x) - \\frac{1}{4} \\sin^2(2x). \\end{equation*}\\] First, we note that \\(\\Delta\\) may be expressed using the above critical identity as follows: \\[\\begin{equation*} \\begin{aligned} \\Delta &amp;= \\int_{0}^{\\infty} \\frac{\\sin^4\\left(\\tau \\pi f \\right)}{\\left(\\tau \\pi f \\right)^2} \\Phi(f) df \\\\ &amp;= \\lim_{n \\rightarrow -\\infty} \\int_{2^{n}}^{\\infty} \\frac{\\sin^2\\left(\\tau \\pi f \\right) - \\frac{1}{4} \\sin^2\\left(2 \\tau \\pi f \\right) }{\\left(\\tau \\pi f \\right)^2} \\Phi(f) df . \\end{aligned} \\end{equation*}\\] Second, by the change of variable \\(u = 2f\\) in the second term, we obtain \\[\\begin{equation*} \\begin{aligned} \\Delta = \\lim_{n \\rightarrow -\\infty} &amp; \\Bigg[ \\int_{2^{n}}^{\\infty} \\frac{\\sin^2\\left(\\tau \\pi f \\right)}{\\left(\\tau \\pi f \\right)^2} \\Phi(f) df - \\frac{1}{2}\\int_{2^{n+1}}^{\\infty} \\frac{\\sin^2\\left(\\tau \\pi u \\right)}{\\left(\\tau \\pi u \\right)^2} \\Phi(f) du \\Bigg]. \\end{aligned} \\end{equation*}\\] Now suppose that \\(\\Phi(f) = 2 \\Phi(2f)\\). In this case, we have \\(\\Phi(f) = 2 \\Phi(u)\\) and therefore we obtain \\[\\begin{equation*} \\begin{aligned} \\Delta &amp;= \\lim_{n \\rightarrow -\\infty} \\int_{2^{n}}^{2^{n+1}} \\frac{\\sin^2\\left(\\tau \\pi f \\right)}{\\left(\\tau \\pi f \\right)^2} \\Phi(f) df = 0. \\end{aligned} \\end{equation*}\\] Remark 4.3 This result demonstrates that the mapping from PSD to Allan variance is not necessarily one-to-one. Greenhall (1998) showed that in the continuous case (i.e. \\(\\tau_j \\in \\mathbb{R}\\)) \\(\\Delta = 0\\) if and only if \\(\\Phi(f) = 2 \\Phi(2f)\\). However, the ``only if’’ part of this results (while conjectured) is unknown in the discrete case. 4.3 Properties of the Allan Variance One reason of explaining the widespread use of the Allan variance for sensor calibration is due to the following additivity property, which is particularly convenient to identify composite stochastic processes (see Definition REF MISSING!). Corollary 4.1 (Additivity of the AV) Consider two (independent) stochastic processes \\((X_t)\\) and \\((Y_t)\\) with respective PSD \\(S_X(f)\\) and \\(S_Y(f)\\). Suppose that we observe the process \\(Z_t = X_t + Y_t\\). Then, we have \\[\\begin{equation*} \\text{AV}_j \\left(Z_t \\right) = \\text{AV}_j \\left(X_t \\right) + \\text{AV}_j \\left(Y_t \\right). \\end{equation*}\\] Proof. The proof of this result is direct from Lemma REF MISSING HERE. Indeed, since \\(S_Z(f) = S_X(f) + S_Y(f)\\), we have \\[\\begin{equation*} \\begin{aligned} \\text{AV}_j \\left(Z_t \\right) &amp;= 4 \\int_0^{\\infty} \\frac{\\sin^4(\\pi f \\tau_j)}{(\\pi f \\tau_j)^2} S_{Z}(f) df\\\\ &amp;= 4 \\int_0^{\\infty} \\frac{\\sin^4(\\pi f \\tau_j)}{(\\pi f \\tau_j)^2} S_{X}(f) df + 4 \\int_0^{\\infty} \\frac{\\sin^4(\\pi f \\tau_j)}{(\\pi f \\tau_j)^2} S_{Y}(f) df\\\\ &amp;= \\text{AV}_j \\left(X_t \\right) + \\text{AV}_j \\left(Y_t \\right). \\end{aligned} \\end{equation*}\\] While Lemma 4.1 is an important results which is very convenient to determine the theoretical AV of a certain stochastic process. However, the applicability of this results is often limited since the integral defined in (REF MISSING HERE) can be intractable. An alternative to Lemma 4.1 has been proposed by N. F. Zhang (2008) and is far advantageous from a computational standpoint. Lemma 4.2 (AV connection to ACF) For a stationary process \\((X_t)\\) with variance \\(\\sigma^2_X\\) and ACF \\(\\rho(h)\\) we have \\[\\begin{equation*} \\label{stat.av} \\text{AV}_j \\left(X_t \\right) = \\frac{\\sigma_X^2}{\\tau_j^2} \\bigg(\\tau_j\\left[1-\\rho(\\tau_j)\\right] + \\sum_{i=1}^{\\tau_j-1} i \\left[2 \\rho(\\tau_j-i) - \\rho(i) - \\rho(2\\tau_j-i)\\right]\\bigg). \\end{equation*}\\] The proof of this result is instructive and is presented in Xu et al. (2017). Remark 4.4 Using Lemma 4.2, the exact form of the AV for different stationary processes, such as the general class of ARMA models, can easily be derived. Moreover, N. F. Zhang (2008) provided the theoretical AV for non-stationary processes such as the random walk and ARFIMA models for which the AV, as mentioned earlier, represents a better measure of uncertainty compared to other methods. Remark 4.5 Lemma 4.2 was extended to non-stationary processes in Xu et al. (2017). Example 4.2 (Theoretical AV of an MA(1) process) From the autocovariance we obtain \\[\\begin{equation*} \\rho(h) = \\text{corr}\\left(X_t, X_{t-h} \\right) =\\left\\{ \\begin{array}{cl} 1 &amp;\\text{if } h = 0\\\\ \\frac{\\theta}{1 + \\theta^2} &amp;\\text{if } |h| = 1\\\\ 0 &amp;\\text{if } |h| &gt; 1.\\\\ \\end{array} \\right. \\end{equation*}\\] We can now apply the formula given in Lemma 4.2, which leads to \\[\\begin{equation*} \\begin{aligned} \\text{AV}_j \\left(X_t \\right) &amp;= \\frac{\\left(1 + \\theta^2 \\right) \\sigma^2}{\\tau_j^2} \\bigg(\\tau_j + \\sum_{i=1}^{\\tau_j-1} i \\left[2 \\rho(\\tau_j-i) - \\rho(i) - \\rho(2\\tau_j-i)\\right]\\bigg)\\\\ &amp;=\\frac{\\left(1 + \\theta^2 \\right) \\sigma^2}{\\tau_j^2} \\bigg(\\tau_j + 2 \\sum_{i=1}^{\\tau_j-1} i \\rho(\\tau_j-i) -\\sum_{i=1}^{\\tau_j-1} i \\rho(i) - \\sum_{i=1}^{\\tau_j-1} i \\rho(2\\tau_j-i)\\bigg)\\\\ &amp;=\\frac{\\left(1 + \\theta^2 \\right) \\sigma^2}{\\tau_j^2} \\left(\\tau_j + 2 (\\tau_j - 1) \\rho(1) - \\rho(1) \\right)\\\\ &amp;=\\frac{\\left(1 + \\theta^2 \\right) \\sigma^2}{\\tau_j^2} \\bigg(\\tau_j + (2\\tau_j - 3) \\frac{\\theta}{1 + \\theta^2} \\bigg). \\end{aligned} \\end{equation*}\\] \\(\\LARGE{\\bullet}\\) 4.4 Estimation Several estimators of the AV have been introduced in the literature. The most commonly is (probably) the Maximum-Overlapping AV (MOAV) estimator proposed by D. B. Percival and Guttorp (1994), which is defined as follows: Definition 4.3 (Maximum-Overlapping AV Estimator) The MOAV is defined as: \\[\\begin{eqnarray} \\label{eq:MOAVNS_est} \\hat{\\phi}_j^2 \\equiv \\widehat{\\text{AV}}_j \\left(X_t \\right) = \\frac{1}{2 \\left(T - 2\\tau_j + 1\\right)} \\sum_{k = 2 \\tau_j}^{T} \\left(\\bar{X}_{k}^{(j)} - \\bar{X}_{k-\\tau_j}^{(j)} \\right)^2. \\end{eqnarray}\\] We will now study the properties of this estimator through the following lemmas. 4.4.1 Consistency Lemma 4.3 (Consistency) Let \\((X_t)\\) be such that: \\((X_t - X_{t-1})\\) is a (strongly) stationary process, \\((X_t - X_{t-1})^2\\) has absolutely summable covariance structure, \\(\\mathbb{E}\\left[(X_t - X_{t-1})^4\\right] &lt; \\infty\\), Then, we have \\[\\widehat{\\text{AV}}_j \\left(X_t \\right) \\overset{ \\mathcal{P} }{\\longrightarrow} \\text{AV}_j \\left(X_t \\right).\\] Proof. TO BE ADDED Remark 4.6 (Connection to Wavelet Variance) This result is closely related by the results of D. P. Percival (1995) on the wavelet variance. We shall explore the connection between the AV and wavelet variance in the next section. 4.4.2 Asymptotic Normality Compare to consistency, the asymptotic normality requires stronger conditions given in the following lemma. Lemma 4.4 (Asymptotic normality) Let \\((X_t)\\) be such that: \\((X_t - X_{t-1})\\) is a (strongly) stationary process. \\((X_t - X_{t-1})\\) is strong mixing process with mixing coefficient \\(\\alpha(n)\\) such that \\(\\sum_{n=1}^{\\infty} \\alpha(n)^{\\frac{\\delta}{2+\\delta}} &lt; \\infty\\) for some \\(\\delta &gt; 0\\). \\(\\mathbb{E}\\left[\\left(X_t - X_{t-1}\\right)^{4+\\delta}\\right] &lt; \\infty\\) for some \\(\\delta &gt; 0\\). Then, under these conditions we have that \\[\\sqrt{T}\\left(\\widehat{\\text{AV}}_j \\left(X_t \\right) - \\text{AV}_j \\left(X_t \\right) \\right) \\overset{ \\mathcal{D} }{\\longrightarrow} \\mathcal{N}(0, \\sigma^2_T/T),\\] where \\(\\sigma^2_T \\equiv \\sum_{h = -\\infty}^{\\infty}\\text{cov}\\left( \\left(\\bar{X}_{t}^{(j)} - \\bar{X}_{t-\\tau_j}^{(j)} \\right)^2, \\left(\\bar{X}_{t+h}^{(j)} - \\bar{X}_{t+h-\\tau_j}^{(j)} \\right)^2 \\right)\\). Proof. TO BE ADDED 4.4.3 Confidence Interval of the MOAV Estimator Based on the asymptotic normality results (Lemma ), we can construct the \\(1-\\alpha\\) confidence intervals for \\(\\widehat{\\text{AV}}_j \\left(X_t \\right)\\) as % \\[\\begin{equation*} \\text{CI}\\left(\\text{AV}_j \\left(X_t \\right)\\right) = \\left[ \\widehat{\\text{AV}}_j \\left(X_t \\right) \\pm z_{1 - \\frac{\\alpha}{2}} \\frac{\\sigma_{T}}{T} \\right], \\end{equation*}\\] % where \\(z_{1 - \\frac{\\alpha}{2}} \\equiv \\boldsymbol{\\Phi}^{-1}\\left( 1- \\frac{\\alpha}{2} \\right)\\) is the \\((1- \\frac{\\alpha}{2})\\) quantile of a standard normal distribution.\\ However, the so called ``Long-Run Variance’’ \\(\\sigma^2_{T}\\) is usually unknown. Many methods have been proposed to consistently estimate it under mild conditions (see e.g. Newey and West 1986). Remark 4.7 Gaussian-based confidence intervals are often problematic with the AV as the lower limit of CI can very well be negative. Next, we will discuss an alternative method to construct the CI for such statistic. 4.5 Allan variance based estimation 4.5.1 Allan Variance log-log Representation As illustrated in Lemmas 4.1 and 4.2 the AV depends on the properties of the stochastic process \\((X_t)\\). We will see that ``log-log’’ representation of the AV is often useful for the identify various processes that may compose \\((X_t)\\). For example, let’s suppose that \\(X_t\\) is a white noise process. We showed in REF MISSING HERE that the theoretical AV of such process is given \\[\\begin{equation*} \\phi_j^2 \\equiv \\text{AV}_j(X_t) = \\frac{\\sigma^2}{\\tau_j}. \\end{equation*}\\] Therefore, we have that the Allan Deviation or AD (i.e. \\(\\sqrt{\\text{AV}_j(X_t)}\\) or \\(\\phi_j\\)) is such that \\[\\begin{equation} \\log\\left( \\phi_j \\right) = \\log \\left(\\sqrt{\\frac{\\sigma^2}{\\tau_j}}\\right) = \\log \\left(\\sigma\\right) - \\frac{1}{2} \\log (\\tau_j). \\label{eq:av:wn} \\end{equation}\\] Thus, the log of the AD is linear in \\(\\tau_j\\) with a slope of \\(-1/2\\) and with intercept \\(\\log (\\sigma)\\). Let us start by considering a simple simulated example. 4.5.2 Allan Deviation of a WN process Simulation based on a white noise process with \\(\\sigma^2 = 10^2\\) and \\(T = 10^5\\). # Load packages library(av) # Package for Allan Variance functions library(simts) # Package for time series simulations # Simulate white noise Xt = gen_gts(WN(sigma2 = 1), n = 10^5) # Compute allan variance av = avar(Xt) # Allan Variance log-log Representation plot(av) Figure 4.1: ADD A NICE CAPTION References "]
]
