[
["timeseries.html", "Chapter 2 Introduction to Time Series Analysis 2.1 Time Series 2.2 Latent Time Series Processes and Composite Stochastic Process 2.3 Dependence within Time Series 2.4 Stationarity", " Chapter 2 Introduction to Time Series Analysis In this chapter, we will provide an introduction to time series analysis. This chapter is organized with the following outline: Definition and descriptive analysis of time series; Latent time series processes and composite stochastic process; Dependence within time series; Concept of stationarity; Fundamental representation of time series. 2.1 Time Series Definition 2.1 (Time Series) A time series is a stochastic process (i.e. a sequence of random variables (r.v.)), defined on a common probability space denoted as \\((X_t)_{t = 1,...,T}\\) (i.e. {\\(X_1\\), \\(X_2\\), …., \\(X_T\\)}). Note that the time \\(t\\) is not continuous and belongs to discrete index sets. Therefore, we implicitly assume that \\(t\\) is not random, i.e. the time at which each observation is measured is known, and the time between two consecutive observations is constant. When recording values of a time series over an extended period of time, it is usually difficult to discern any trend or pattern of the time series by simply looking at the values. However, when these data points are displayed on a plot with time on x-axis and \\(X_t\\) on y-axis, some features of the time series jump out. So it is often useful to understand a time series process by performing a descriptive analysis, especially when we have data of small or moderate size. When we perform descriptive analysis, we usually check the following in the time series data/graph: Trends Seasonal (e.g. business cycles) Non-seasonal (e.g. impact of economic indicators on stock returns) Local fluctuation (e.g. vibrations observed before, during and after an earthquake) Changes in the statistical properties Mean (e.g. economic crisis) Variance (e.g. earnings) States (e.g. bear/bull in finance) Model deviations (e.g. outliers) Example 2.1 (Johnson and Johnson Quarterly Earnings) A traditional example of a time series is the quarterly earnings of the company Johnson and Johson. In the graph below, we present these earnings between 1960 and 1980. # Load data data(jj, package = &quot;astsa&quot;) # Construct gts object jj = gts(jj, start = 1960, freq = 4) # Plot time series plot(jj, main = &quot;Johnson and Johnson Quarterly Earnings&quot;, ylab = &quot;Quarterly Earnings per Share ($)&quot;) Figure 2.1: Johnson and Johnson Quarterly Earnings As we can see from the graph, the data contains a non-linear increasing trend as well as a yearly seasonal component. In addition, we can notice that the variability of the data seems to increase with time. By plotting the time series graph for this data, we can discern some important features of the data, which is helpful for us to conduct further analysis such as selecting suitable models for this data. Example 2.2 (Monthly Precipitation Data) Now we consider another data set coming from the domain of hydrology. The data records monthly precipitation (in mm) over a certain period of time (1907 to 1972) and is interesting for scientists in order to study water cycles. The data are presented in the graph below: # Load data data(hydro, package = &quot;simts&quot;) # Construct gts object hydro = gts(hydro, start = 1907, freq = 12) # Plot data plot(hydro, main = &quot;Monthly Precipitation Data&quot;, ylab = &quot;Mean Monthly Precipitation (mm)&quot;) Figure 2.2: Monthly Precipitation Data From the above time series graph, we can observe some extreme observations (i.e. outliers) in this data more easily than if we simply look at all the values of this data. Example 2.3 (Inertial Sensor Data) Now we consider the data coming from the calibration procedure of an Inertial Measurement Unit (IMU). The signals coming from an IMU are usually measured at high frequencies over a long time and are often characterized by linear trends and numerous underlying stochastic processes. We present the time series graph of some data from an IMU below. # Load data data(imu6, package = &quot;imudata&quot;) # Construct gts object imu = gts(imu6[,1], freq = 100*60*60) # Plot data plot(imu, main = &quot;Inertial Sensor Data&quot;, ylab = expression(paste(&quot;Error &quot;, (rad/s^2)))) Figure 2.3: Inertial Sensor Data As we can see from the graph, although a linear trend and other processes are present in this data, it is practically impossible to discern any feature of the data based on the time series graph. In general, the descriptive analysis in classical time series analysis is not appropriate for the analysis of inertial sensors as these data are usually very large in order to perform parameter estimation. 2.2 Latent Time Series Processes and Composite Stochastic Process We first introduce some latent time series processes that are commonly used, especially in the calibration procedure of inertial sensors. Definition 2.2 (Gaussian White Noise) The Gaussian White Noise (WN) process with parameter \\(\\sigma^2 \\in \\mathbb{R}^+\\) is defined as \\[\\begin{equation*} X_t \\overset{iid}{\\sim} \\mathcal{N}\\left(0, \\sigma^2 \\right) \\end{equation*}\\] where “iid” stands for “independent and identically distributed”. Definition 2.3 (Quantization Noise) The Quantization Noise (QN) process with parameter \\(Q^2 \\in \\mathbb{R}^+\\) is a process with Power Spectral Density (PSD) of the form \\[\\begin{equation*} S_{X}(f) = 4 Q^2 \\sin^2 \\left( \\frac{\\pi f}{\\Delta t} \\right) \\Delta t, \\;\\; f &lt; \\frac{\\Delta t}{2}. \\end{equation*}\\] Definition 2.4 (Drift) The Drift (DR) process with parameter \\(\\omega \\in \\Omega\\), where \\(\\Omega\\) is either \\(\\mathbb{R}^+\\) or \\(\\mathbb{R}^-\\), is defined as \\[\\begin{equation*} X_t = \\omega t. \\end{equation*}\\] Definition 2.5 (Random Walk) The Random Walk (RW) process with parameter \\(\\gamma^2 \\in \\mathbb{R}^+\\) is defined as \\[\\begin{equation*} X_t = X_{t-1} + \\epsilon_t \\;\\; \\text{where}\\;\\; \\epsilon_t \\overset{iid}{\\sim} \\mathcal{N}\\left(0, \\gamma^2 \\right)\\;\\; \\text{and}\\;\\; X_0 = 0. \\end{equation*}\\] Definition 2.6 (Auto-Regressive) The Auto-Regressive Process of Order 1 (AR1) process with parameter \\(\\phi \\in (-1, +1)\\) and \\(\\upsilon^2 \\in \\mathbb{R}^+\\) is defined as \\[\\begin{equation*} X_t = \\phi X_{t-1} + Z_t, \\;\\;\\; Z_t \\overset{iid}{\\sim} \\mathcal{N}(0,\\upsilon^2). \\end{equation*}\\] Definition 2.7 (Gauss Markov) The Gauss Markov Process of Order 1 (GM) process with parameter \\(\\beta \\in \\mathbb{R}\\) and \\(\\sigma_G^2 \\in \\mathbb{R}^+\\) is defined as \\[\\begin{equation*} X_t = \\exp(-\\beta \\Delta_t) X_{t-1} + Z_t, \\;\\;\\; Z_t \\overset{iid}{\\sim} \\mathcal{N}(0,\\sigma^2_{G}(1-\\exp(-2\\beta\\Delta t))) \\end{equation*}\\] where \\(\\Delta t\\) denotes the time between \\(X_t\\) and \\(X_{t-1}\\). Remark 2.1 (GM and AR1) A GM process is a one-to-one reparametrization of an AR1 process. In the following, we will only discuss AR1 processes but all results remain valid for GM processes. With the above defined latent time series processes, we introduce the composite stochastic process, which is widely used in the estimation procedure of inertial sensor stochastic calibration. Definition 2.8 (Composite Stochastic Processes) A composite stochastic process is a sum of latent processes. We implicitly assume that these latent processes are independent. Example 2.4 (2*AR1 + WN) The composite stochastic process of “2*AR1 + WN&quot; is given as \\[\\begin{align} Y_t &amp;= \\phi_1 Y_{t-1} + Z_t, \\;\\;\\; Z_t \\overset{iid}{\\sim} \\mathcal{N}(0,\\upsilon_1^2),\\\\ W_t &amp;= \\phi_2 W_{t-1} + U_t, \\;\\;\\; U_t \\overset{iid}{\\sim} \\mathcal{N}(0,\\upsilon_2^2),\\\\ Q_t &amp;\\overset{iid}{\\sim} \\mathcal{N}(0,\\sigma^2),\\\\ X_t &amp;= Y_t + W_t + Q_t, \\end{align}\\] where \\(Y_t\\), \\(W_t\\) and \\(Q_t\\) are independent and only \\((X_t)\\) is observed. 2.3 Dependence within Time Series One of the main purpose of time series analysis is to make predictions. That is, if \\((X_t)_{t=1,\\ldots,T}\\) is an identically distributed but not independent sequence, what is the best predictor for \\({X}_{T+h}\\) for \\(h &gt; 0\\) (i.e. an estimator of \\(\\mathbb{E}[X_{T+h}| X_T,...]\\))? In order to answer this question, we need to understand the dependence between \\(X_{1},\\ldots,X_{T}\\). Before we start to consider the dependence within time series, let us first take a review on independence. Definition 2.9 (Independence of Events) Two events \\(A\\) and \\(B\\) are independent if \\[\\begin{align*} \\mathbb{P}(A \\cap B) = \\mathbb{P}(A)\\mathbb{P}(B). \\end{align*}\\] In general, \\(A_{1},\\ldots,A_{n}\\) are independent if \\[\\begin{align*} \\mathbb{P}(B_1 \\ldots B_n) = \\mathbb{P}(B_1) \\ldots \\mathbb{P}(B_n) \\;\\; \\text{for all} \\;\\; B_i = A_i \\;\\; \\text{or} \\;\\; S, \\;\\; i=1,\\ldots,n. \\end{align*}\\] Definition 2.10 (Independence of Random Variables) Two random variables \\(X\\) and \\(Y\\) with Cumulative Distribution Functions (CDF) \\(F_X(x)\\) and \\(F_Y(y)\\) respectively are independent if and only if their joint CDF \\(F_{X,Y}(x,y)\\) is such that \\[\\begin{align*} F_{X,Y}(x,y) = F_{X}(x) F_{Y}(y). \\end{align*}\\] In general, random variables \\(X_1, \\ldots, X_n\\) with CDF \\(F_{X_1}(x_1), \\ldots, F_{X_n}(x_n)\\) respectively are independent if and only if their joint CDF \\(F_{X_1, \\ldots, X_n}(x_1, \\ldots, x_n)\\) is such that \\[\\begin{align*} F_{X_1,\\ldots,X_n}(x_1,\\ldots,x_n) = F_{X_1}(x_1) \\ldots F_{X_n}(x_n). \\end{align*}\\] Definition 2.11 (iid sequence) The sequence \\(X_{1},X_{2},\\ldots,X_{T}\\) is said to be independent and identically distributed (i.e. iid) if and only if \\[\\begin{align*} \\mathbb{P}(X_{i}&lt;x) = \\mathbb{P}(X_{j}&lt;x) \\;\\; \\forall x \\in \\mathbb{R}, \\forall i,j \\in \\{1,\\ldots,T\\} \\end{align*}\\] and \\[\\begin{align*} \\mathbb{P}(X_{1}&lt;x_{1},X_{2}&lt;x_{2},\\ldots,X_{T}&lt;x_{T})=\\mathbb{P}(X_{1}&lt;x_1) \\ldots \\mathbb{P}(X_{T}&lt;x_T) \\;\\; \\forall T\\geq2, x_1, \\ldots, x_T \\in \\mathbb{R}. \\end{align*}\\] Now we start to consider the dependence, specifically the linear dependence, within time series. Notice that it is difficult to consider the dependence between \\(T\\) random variables at a time. So we need to consider only two random variables at a time. Definition 2.12 (AutoCovariance) AutoCovariance (ACV) denoted as \\(\\gamma_X(t, t+h)\\) is defined as \\[\\begin{align*} \\gamma_X(t, t+h) = \\text{Cov}(X_{t},X_{t+h})= \\mathbb{E}(X_{t}X_{t+h})-\\mathbb{E}(X_{t})\\mathbb{E}(X_{t+h}) \\end{align*}\\] where \\[\\begin{align*} \\mathbb{E}(X_{t}) = \\int_{-\\infty}^{\\infty}x f(x) dx \\;\\; \\text{and} \\;\\; \\mathbb{E}(X_{t},X_{t+h}) = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}x_{1}x_{2} f(x_{1},x_{2}) dx_{1}dx_{2} \\end{align*}\\] where \\(f(x_{1},x_{2})\\) denotes the joint density of \\(X_{t}\\) and \\(X_{t+h}\\). Remark 2.2 (Properties of ACV) ACV is symmetric, i.e. \\(\\gamma_X(t, t+h) = \\gamma_X(t+h, t)\\) as \\(\\text{Cov}(X_{t},X_{t+h}) = \\text{Cov}(X_{t+h},X_{t})\\). Under stationarity (will be discussed very soon), \\(\\gamma_X(h) = \\gamma_X(-h)\\), i.e. ACV is an even function. Variance of the process \\(\\text{Var}(X_t) = \\gamma_X(t, t) \\geq 0\\). Under stationarity, \\(\\text{Var}(X_t) = \\gamma_X(0)\\) and \\(\\mid \\gamma_X(h) \\mid \\leq \\gamma_X(0)\\) by Cauchy-Schwarz inequality. Scale dependent: ACV \\(\\gamma_X(t, t+h)\\) is scale dependent like any covariance. So \\(\\gamma_X(t, t+h) \\in \\mathbb{R}\\). If \\(\\mid \\gamma_X(t, t+h) \\mid\\) is “close” to 0, then they are “less (linearly) dependent”. If \\(\\mid \\gamma_X(t, t+h) \\mid\\) is “far” from 0, \\(X_{t}\\) and \\(X_{t+h}\\) are “more (linearly) dependent”. However in general, it is difficult to assess what “close” and “far” from zero mean. In general, \\(\\gamma_X(t, t+h)=0\\) does not imply \\(X_{t}\\) and \\(X_{t+h}\\) are independent. However, if \\(X_{t}\\) and \\(X_{t+h}\\) are joint normally distributed, then \\(\\gamma_X(t, t+h)=0\\) implies that \\(X_{t}\\) and \\(X_{t+h}\\) are independent. Another measure of linear dependence which is related to the ACV is the AutoCorrelation. This is arguably the most commonly used metric in time series analysis. Definition 2.13 (AutoCorrelation) AutoCorrelation (ACF) denoted as \\(\\rho_X(t, t+h)\\) is defined as \\[\\begin{align*} \\rho_X(t,t+h) = \\text{Corr}(X_{t},X_{t+h}) = \\frac{\\text{Cov}(X_{t},X_{t+h})}{\\sqrt{\\text{Var}(X_{t})} \\sqrt{\\text{Var}(X_{t+h})}}. \\end{align*}\\] Remark 2.3 (Properties of ACF) \\(\\mid \\rho_X(t,t+h) \\mid \\leq 1\\) and \\(\\mid \\rho_X(t,t) \\mid = 1\\). ACF is symmetric, i.e. \\(\\rho_X(t, t+h) = \\rho_X(t+h, t)\\) as \\(\\text{Corr}(X_{t},X_{t+h}) = \\text{Corr}(X_{t+h},X_{t})\\). Under stationarity, \\(\\gamma_X(h) = \\gamma_X(-h)\\), i.e. ACF is an even function. Scale invariant: ACF \\(\\rho_X(t,t+h)\\) is scale free like any correlation. Moreover, if \\(\\rho_X(t,t+h)\\) is “close” to \\(\\pm 1\\), then this implies that there is “strong” (linear) dependence between \\(X_{t}\\) and \\(X_{t+h}\\). We can simplify the notations \\(\\gamma_X(t, t+h)\\) and \\(\\rho_X(t, t+h)\\) to be \\(\\gamma(t, t+h)\\) and \\(\\rho(t, t+h)\\) when there is no ambiguity (i.e. only one time series is considered). Notice that both ACV and ACF are appropriate to measure linear dependence only. Besides linear dependence, other forms of dependence such as monotonic or nonlinear dependence also exist. However, both ACV and ACF are less helpful to measure these dependence as they might have ACV and ACF to be zero. Here is an example: Figure 2.4: Different forms of dependence and their ACF values It is worth noting that correlation does NOT imply causation. For example, if \\(\\rho(t, t+h) \\neq 0\\), it does not imply that \\(X_t \\to X_{t+h}\\) is causal. Actually, real causation doesn’t exist in statistics but there exists approximated metric to measure this concept such as Granger causality (see Granger (1969)). This idea is clearly illustrated in the image below: Figure 2.5: Correlation does NOT imply causation. 2.4 Stationarity In this section we are going to introduce the concept of stationarity, one of the most important characteristics of time series data. First let us consider an example of non-stationary process. Example 2.5 (Non-Stationary Process) \\[\\begin{equation*} X_t \\sim \\mathcal{N} \\left(0, Y_t^2\\right) \\;\\; \\text{where $Y_t$ is unobserved and such that} \\;\\; Y_t \\overset{iid}{\\sim} \\mathcal{N} \\left(0, 1\\right). \\end{equation*}\\] In this case, it is clear that the estimation of \\(\\text{Var}(X_t)\\) is difficult since only \\(X_t\\) is useful for the estimation. So in fact, \\(X_t^2\\) is our best guess for \\(\\text{Var}(X_t)\\). On the other hand, let us consider an example of stationary process where averaging becomes meaningful for such process. Example 2.6 (Stationary Process) \\[\\begin{equation*} X_t = \\theta W_{t-1} + W_t \\;\\;\\; \\text{where} \\;\\;\\; W_t \\stackrel{iid}{\\sim} \\mathcal{N} \\left(0, 1\\right). \\end{equation*}\\] In this case, we can guess that a natural estimator of \\(\\text{Var}(X_t)\\) can be \\(\\hat{\\sigma}^2 = \\frac{1}{T} \\sum_{i = 1}^T X_i^2\\). That is, now averages are meaningful for such process. There exist two forms of stationarity, which are defined below: Definition 2.14 (Strong Stationarity) The time series \\(X_{t}\\) is strongly stationary if the joint probability distribution is invariant under a shift in time, i.e. \\[\\begin{equation*} \\mathbb{P}(X_{t}\\leq x_{0},\\ldots,X_{t+k}\\leq x_{k}) = \\mathbb{P}(X_{t+h}\\leq x_{0} ,\\ldots,X_{t+h+k}\\leq x_{k}) \\end{equation*}\\] for any time shift \\(h\\) and any \\(x_{0}, x_{1},x_{2},\\cdots,x_{k}\\) belong to the domain of \\(X_t,\\cdots,X_{t+k}\\) and \\(X_{t+h},\\cdots,X_{t+h+k}\\). Definition 2.15 (Weak Stationarity) The time series \\((X_{t})_{t \\in \\mathbb{N}}\\) is weakly stationary if the mean and autocovariance are finite and invariant under a shift in time, i.e. \\[\\begin{equation*} \\begin{aligned} \\mathbb{E}\\left[X_t\\right] &amp;= \\mu &lt; \\infty,\\\\ \\mathbb{E}\\left[X_t^2\\right] &amp;= \\mu_2 &lt; \\infty,\\\\ \\text{Cov}(X_{t},X_{t+h})&amp;= \\text{Cov}(X_{t + k},X_{t+h + k}) = \\gamma( h ). \\end{aligned} \\end{equation*}\\] for any time shift \\(h\\). For convenience, we use the abbreviation “stationary” to indicate “weakly stationary” by default. The stationarity of \\(X_{t}\\) is important because it provides a framework in which averaging makes sense. The concept of averaging is essentially meaningless unless properties like mean and covariance are either fixed or evolve in a known manner. Remark 2.4 (Implication on the ACV and ACF) If a process is weakly stationary or strongly stationary and \\(\\text{Cov}(X_{t},X_{t+h})\\) exists for all \\(h \\in \\mathbb{Z}\\), then we have both ACV and ACF only depend on the lag between observations, i.e. \\[\\begin{equation*} \\begin{aligned} \\gamma(t, t+h) &amp;= \\text{Cov}(X_{t},X_{t+h})= \\text{Cov}(X_{t + k},X_{t+h + k}) = \\gamma(t+k, t+h+k) = \\gamma(h),\\\\ \\rho(t, t+h) &amp;= \\text{Corr}(X_{t},X_{t+h})= \\text{Corr}(X_{t + k},X_{t+h + k}) = \\rho(t+k, t+h+k) = \\rho(h). \\end{aligned} \\end{equation*}\\] Remark 2.5 (Relation between Strong and Weak Stationarity) In general, neither type of stationarity implies the other one. However, If \\(X_{t}\\) is Normal (Gaussian) with \\(\\sigma^2 = \\text{Var} (X_{t}) &lt; \\infty\\), then weak stationarity implies strong stationarity. If \\(X_{t}\\) is strongly stationary, \\(\\mathbb{E}(X_t) &lt; \\infty\\) and \\(\\mathbb{E}(X_t^2) &lt; \\infty\\), then \\(X_{t}\\) is weakly stationary. Example 2.7 (Strong Stationarity does NOT imply Weak Stationarity) Consider a Gaussian random walk process \\(X_t\\) where initial value \\(X_0 = 0\\). Since the model is “fixed in time”, this process is strongly stationary. However, we have \\[\\begin{equation*} \\text{Var}\\left(X_t\\right) = \\text{Var}\\left(\\sum_{i = 1}^t Z_i\\right) = \\sum_{i = 1}^t \\gamma^2 = t \\gamma^2 \\end{equation*}\\] and therefore \\(\\mathbb{E}[X^2_t]\\) is not fixed and finite, i.e. \\(X_t\\) is not weakly stationary. Example 2.8 (Weak Stationarity does NOT imply Strong Stationarity) Let \\(X_t \\overset{iid}{\\sim} \\exp(1)\\) (i.e. exponential distribution with \\(\\lambda = 1\\)) and \\(Y_t \\overset{iid}{\\sim} \\mathcal{N}(1,1)\\). Then, let \\[\\begin{equation*} Z_t = \\left\\{ \\begin{array}{cl} X_t &amp;\\text{if } t \\in \\left\\{2k | k \\in \\mathbb{N}\\right\\}\\\\ Y_t &amp;\\text{if } t \\in \\left\\{2k + 1 | k \\in \\mathbb{N}\\right\\}, \\end{array} \\right. \\end{equation*}\\] we have \\(Z_t\\) is weakly stationary but not strongly stationary. Remark 2.6 (Stationarity of Latent Time Series Processes) (Weakly) Stationary: WN, QN, AR1 (Weakly) Non-Stationary: DR, RW Proof (AR1 is weakly stationary). Consider an AR1 process defined as: \\[\\begin{equation*} X_t = \\phi X_{t-1} + Z_t, \\;\\;\\; Z_t \\overset{iid}{\\sim} \\mathcal{N}(0,\\nu^2), \\end{equation*}\\] where \\(\\mid \\phi \\mid &lt; 1\\) and \\(\\nu^2 &lt; \\infty\\). Then we have \\[\\begin{aligned} {X_t} &amp;= {\\phi }{X_{t - 1}} + {Z_t} = \\phi \\left[ {\\phi {X_{t - 2}} + {Z_{t - 1}}} \\right] + {Z_t} = {\\phi ^2}{X_{t - 2}} + \\phi {Z_{t - 1}} + {Z_t} \\\\ &amp; \\; \\vdots \\\\ &amp;= {\\phi ^k}{X_{t-k}} + \\sum\\limits_{j = 0}^{k - 1} {{\\phi ^j}{Z_{t - j}}} . \\end{aligned} \\] By taking the limit in \\(k\\) (which is perfectly valid as we assume \\(t \\in \\mathbb{Z}\\)), we obtain \\[\\begin{equation*} \\begin{aligned} X_t = \\mathop {\\lim }\\limits_{k \\to \\infty} \\; {X_t} = \\sum\\limits_{j = 0}^{\\infty} {{\\phi ^j}{Z_{t - j}}}. \\end{aligned} \\end{equation*}\\] So we have \\[\\begin{equation*} \\begin{aligned} \\mathbb{E}\\left[X_t\\right] &amp;= \\sum\\limits_{j = 0}^{\\infty} {{\\phi ^j}{\\mathbb{E} [Z_{t - j}]}} = 0, \\\\ \\text{Var}\\left(X_t\\right) &amp;= \\text{Var}\\left(\\sum\\limits_{j = 0}^{\\infty} {{\\phi ^j}{Z_{t - j}}}\\right) = \\sum\\limits_{j = 0}^{\\infty} {\\phi^{2j}} \\text{Var}\\left(Z_{t-j}\\right) = \\nu^2 \\sum\\limits_{j = 0}^{\\infty} {\\phi^{2j}} = \\frac{\\nu^2}{1-\\phi^2} &lt; \\infty. \\end{aligned} \\end{equation*}\\] Moreover, assuming for notational simplicity that \\(h &gt; 1\\), we obtain \\[\\begin{equation*} \\begin{aligned} \\text{Cov}\\left(X_t, X_{t+h}\\right) &amp;= \\phi \\text{Cov}\\left(X_t, X_{t+h-1}\\right) = \\phi^2 \\text{Cov}\\left(X_t, X_{t+h-2}\\right) = \\ldots = \\phi^h \\text{Cov}(X_t, X_t). \\end{aligned} \\end{equation*}\\] In general, when \\(h \\in \\mathbb{Z}\\) we obtain \\[\\begin{equation*} \\begin{aligned} \\text{Cov}\\left(X_t, X_{t+h}\\right) &amp; = \\phi^{|h|} \\text{Cov}(X_t, X_t) = \\phi^{|h|} \\frac{\\nu^2}{1-\\phi^2}, \\end{aligned} \\end{equation*}\\] which is a function of the lag \\(h\\) only. Therefore, this AR1 process is weakly stationary. References "]
]
