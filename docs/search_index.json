[
["timeseries.html", "Chapter 2 Introduction to Time Series Analysis 2.1 Time Series 2.2 Latent Time Series Processes and Composite Stochastic Process 2.3 Dependence within Time Series", " Chapter 2 Introduction to Time Series Analysis In this chapter, we will provide an introduction to time series analysis. This chapter is organized with the following outline: Definition and descriptive analysis of time series; Latent time series processes and composite stochastic process; Dependence within time series; Concept of stationarity; Fundamental representation of time series. 2.1 Time Series Definition 2.1 (Time Series) A time series is a stochastic process (i.e. a sequence of random variables (r.v.)), defined on a common probability space denoted as \\((X_t)_{t = 1,...,T}\\) (i.e. {\\(X_1\\), \\(X_2\\), …., \\(X_T\\)}). Note that the time \\(t\\) is not continuous and belongs to discrete index sets. Therefore, we implicitly assume that \\(t\\) is not random, i.e. the time at which each observation is measured is known, and the time between two consecutive observations is constant. When recording values of a time series over an extended period of time, it is usually difficult to discern any trend or pattern of the time series by simply looking at the values. However, when these data points are displayed on a plot with time on x-axis and \\(X_t\\) on y-axis, some features of the time series jump out. So it is often useful to understand a time series process by performing a descriptive analysis, especially when we have data of small or moderate size. When we perform descriptive analysis, we usually check the following in the time series data/graph: Trends Seasonal (e.g. business cycles) Non-seasonal (e.g. impact of economic indicators on stock returns) Local fluctuation (e.g. vibrations observed before, during and after an earthquake) Changes in the statistical properties Mean (e.g. economic crisis) Variance (e.g. earnings) States (e.g. bear/bull in finance) Model deviations (e.g. outliers) Example 2.1 (Johnson and Johnson Quarterly Earnings) A traditional example of a time series is the quarterly earnings of the company Johnson and Johson. In the graph below, we present these earnings between 1960 and 1980. # Load data data(jj, package = &quot;astsa&quot;) # Construct gts object jj = gts(jj, start = 1960, freq = 4) # Plot time series plot(jj, main = &quot;Johnson and Johnson Quarterly Earnings&quot;, ylab = &quot;Quarterly Earnings per Share ($)&quot;) Figure 2.1: Johnson and Johnson Quarterly Earnings As we can see from the graph, the data contains a non-linear increasing trend as well as a yearly seasonal component. In addition, we can notice that the variability of the data seems to increase with time. By plotting the time series graph for this data, we can discern some important features of the data, which is helpful for us to conduct further analysis such as selecting suitable models for this data. Example 2.2 (Monthly Precipitation Data) Now we consider another data set coming from the domain of hydrology. The data records monthly precipitation (in mm) over a certain period of time (1907 to 1972) and is interesting for scientists in order to study water cycles. The data are presented in the graph below: # Load data data(hydro, package = &quot;simts&quot;) # Construct gts object hydro = gts(hydro, start = 1907, freq = 12) # Plot data plot(hydro, main = &quot;Monthly Precipitation Data&quot;, ylab = &quot;Mean Monthly Precipitation (mm)&quot;) Figure 2.2: Monthly Precipitation Data From the above time series graph, we can observe some extreme observations (i.e. outliers) in this data more easily than if we simply look at all the values of this data. Example 2.3 (Inertial Sensor Data) Now we consider the data coming from the calibration procedure of an Inertial Measurement Unit (IMU). The signals coming from an IMU are usually measured at high frequencies over a long time and are often characterized by linear trends and numerous underlying stochastic processes. We present the time series graph of some data from an IMU below. # Load data data(imu6, package = &quot;imudata&quot;) # Construct gts object imu = gts(imu6[,1], freq = 100*60*60) # Plot data plot(imu, main = &quot;Inertial Sensor Data&quot;, ylab = expression(paste(&quot;Error &quot;, (rad/s^2)))) Figure 2.3: Inertial Sensor Data As we can see from the graph, although a linear trend and other processes are present in this data, it is practically impossible to discern any feature of the data based on the time series graph. In general, the descriptive analysis in classical time series analysis is not appropriate for the analysis of inertial sensors as these data are usually very large in order to perform parameter estimation. 2.2 Latent Time Series Processes and Composite Stochastic Process We first introduce some latent time series processes that are commonly used, especially in the calibration procedure of inertial sensors. Definition 2.2 (Gaussian White Noise) The Gaussian White Noise (WN) process with parameter \\(\\sigma^2 \\in \\mathbb{R}^+\\) is defined as \\[\\begin{equation*} X_t \\overset{iid}{\\sim} \\mathcal{N}\\left(0, \\sigma^2 \\right) \\end{equation*}\\] where “iid” stands for “independent and identically distributed”. Definition 2.3 (Quantization Noise) The Quantization Noise (QN) process with parameter \\(Q^2 \\in \\mathbb{R}^+\\) is a process with Power Spectral Density (PSD) of the form \\[\\begin{equation*} S_{X}(f) = 4 Q^2 \\sin^2 \\left( \\frac{\\pi f}{\\Delta t} \\right) \\Delta t, \\;\\; f &lt; \\frac{\\Delta t}{2}. \\end{equation*}\\] Definition 2.4 (Drift) The Drift (DR) process with parameter \\(\\omega \\in \\Omega\\), where \\(\\Omega\\) is either \\(\\mathbb{R}^+\\) or \\(\\mathbb{R}^-\\), is defined as \\[\\begin{equation*} X_t = \\omega t. \\end{equation*}\\] Definition 2.5 (Random Walk) The Random Walk (RW) process with parameter \\(\\gamma^2 \\in \\mathbb{R}^+\\) is defined as \\[\\begin{equation*} X_t = X_{t-1} + \\epsilon_t \\;\\; \\text{where}\\;\\; \\epsilon_t \\overset{iid}{\\sim} \\mathcal{N}\\left(0, \\gamma^2 \\right)\\;\\; \\text{and}\\;\\; X_0 = 0. \\end{equation*}\\] Definition 2.6 (Auto-Regressive) The Auto-Regressive Process of Order 1 (AR1) process with parameter \\(\\phi \\in (-1, +1)\\) and \\(\\upsilon^2 \\in \\mathbb{R}^+\\) is defined as \\[\\begin{equation*} X_t = \\phi X_{t-1} + Z_t, \\;\\;\\; Z_t \\overset{iid}{\\sim} \\mathcal{N}(0,\\upsilon^2). \\end{equation*}\\] Definition 2.7 (Gauss Markov) The Gauss Markov Process of Order 1 (GM) process with parameter \\(\\beta \\in \\mathbb{R}\\) and \\(\\sigma_G^2 \\in \\mathbb{R}^+\\) is defined as \\[\\begin{equation*} X_t = \\exp(-\\beta \\Delta_t) X_{t-1} + Z_t, \\;\\;\\; Z_t \\overset{iid}{\\sim} \\mathcal{N}(0,\\sigma^2_{G}(1-\\exp(-2\\beta\\Delta t))) \\end{equation*}\\] where \\(\\Delta t\\) denotes the time between \\(X_t\\) and \\(X_{t-1}\\). Remark 2.1 (GM and AR1) A GM process is a one-to-one reparametrization of an AR1 process. In the following, we will only discuss AR1 processes but all results remain valid for GM processes. With the above defined latent time series processes, we introduce the composite stochastic process, which is widely used in the estimation procedure of inertial sensor stochastic calibration. Definition 2.8 (Composite Stochastic Processes) A composite stochastic process is a sum of latent processes. We implicitly assume that these latent processes are independent. Example 2.4 (2*AR1 + WN) The composite stochastic process of “2*AR1 + WN&quot; is given as \\[\\begin{align} Y_t &amp;= \\phi_1 Y_{t-1} + Z_t, \\;\\;\\; Z_t \\overset{iid}{\\sim} \\mathcal{N}(0,\\upsilon_1^2),\\\\ W_t &amp;= \\phi_2 W_{t-1} + U_t, \\;\\;\\; U_t \\overset{iid}{\\sim} \\mathcal{N}(0,\\upsilon_2^2),\\\\ Q_t &amp;\\overset{iid}{\\sim} \\mathcal{N}(0,\\sigma^2),\\\\ X_t &amp;= Y_t + W_t + Q_t, \\end{align}\\] where \\(Y_t\\), \\(W_t\\) and \\(Q_t\\) are independent and only \\((X_t)\\) is observed. 2.3 Dependence within Time Series One of the main purpose of time series analysis is to make predictions. That is, if \\((X_t)_{t=1,\\ldots,T}\\) is an identically distributed but not independent sequence, what is the best predictor for \\({X}_{T+h}\\) for \\(h &gt; 0\\) (i.e. an estimator of \\(\\mathbb{E}[X_{T+h}| X_T,...]\\))? In order to answer this question, we first need to understand the dependence between \\(X_{1},\\ldots,X_{T}\\). Before we start to consider the dependence within time series, let us first take a review on independence. Definition 2.9 (Independence of Events) Two events \\(A\\) and \\(B\\) are independent if \\[\\begin{align*} \\mathbb{P}(A \\cap B) = \\mathbb{P}(A)\\mathbb{P}(B). \\end{align*}\\] In general, \\(A_{1},\\ldots,A_{n}\\) are independent if \\[\\begin{align*} \\mathbb{P}(B_1 \\ldots B_n) = \\mathbb{P}(B_1) \\ldots \\mathbb{P}(B_n) \\;\\; \\text{for all} \\;\\; B_i = A_i \\;\\; \\text{or} \\;\\; S, \\;\\; i=1,\\ldots,n. \\end{align*}\\] Definition 2.10 (Independence of Two Random Variables) Two random variables \\(X\\) and \\(Y\\) with Cumulative Distribution Functions (CDF) \\(F_X(x)\\) and \\(F_Y(y)\\) respectively are independent if and only if their joint CDF \\(F_{X,Y}(x,y)\\) is such that \\[\\begin{align*} F_{X,Y}(x,y) = F_{X}(x) F_{Y}(y). \\end{align*}\\] In general, random variables \\(X_1, \\ldots, X_n\\) with CDF \\(F_{X_1}(x_1), \\ldots, F_{X_n}(x_n)\\) respectively are independent if and only if their joint CDF \\(F_{X_1, \\ldots, X_n}(x_1, \\ldots, x_n)\\) is such that \\[\\begin{align*} F_{X_1,\\ldots,X_n}(x_1,\\ldots,x_n) = F_{X_1}(x_1) \\ldots F_{X_n}(x_n). \\end{align*}\\] Definition 2.11 (iid sequence) The sequence \\(X_{1},X_{2},\\ldots,X_{T}\\) is said to be independent and identically distributed (i.e. iid) if and only if \\[\\begin{align*} \\mathbb{P}(X_{i}&lt;x) = \\mathbb{P}(X_{j}&lt;x) \\;\\; \\forall x \\in \\mathbb{R}, \\forall i,j \\in \\{1,\\ldots,T\\} \\end{align*}\\] and \\[\\begin{align*} \\mathbb{P}(X_{1}&lt;x_{1},X_{2}&lt;x_{2},\\ldots,X_{T}&lt;x_{T})=\\mathbb{P}(X_{1}&lt;x_1) \\ldots \\mathbb{P}(X_{T}&lt;x_T) \\;\\; \\forall T\\geq2, x_1, \\ldots, x_T \\in \\mathbb{R}. \\end{align*}\\] Now we start to consider the dependence, specifically the linear dependence, within time series. Notice that it is difficult to consider the dependence between \\(T\\) random variables at a time. So we need to consider only two random variables at a time. Definition 2.12 (AutoCovariance) AutoCovariance (ACV) denoted as \\(\\gamma_X(t, t+h)\\) is defined as \\[\\begin{align*} \\gamma_X(t, t+h) = \\text{Cov}(X_{t},X_{t+h})= \\mathbb{E}(X_{t}X_{t+h})-\\mathbb{E}(X_{t})\\mathbb{E}(X_{t+h}) \\end{align*}\\] where \\[\\begin{align*} \\mathbb{E}(X_{t}) = \\int_{-\\infty}^{\\infty}x f(x) dx \\;\\; \\text{and} \\;\\; \\mathbb{E}(X_{t},X_{t+h}) = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}x_{1}x_{2} f(x_{1},x_{2}) dx_{1}dx_{2} \\end{align*}\\] where \\(f(x_{1},x_{2})\\) denotes the joint density of \\(X_{t}\\) and \\(X_{t+h}\\). Remark 2.2 (Properties of ACV) 1. ACV is symmetric, i.e. \\(\\gamma_X(t, t+h) = \\gamma_X(t+h, t)\\) as \\(\\text{Cov}(X_{t},X_{t+h}) = \\text{Cov}(X_{t+h},X_{t})\\). Variance of the process \\(\\text{Var}(X_t) = \\gamma_X(t, t)\\). Scale dependent: ACV \\(\\gamma_X(t, t+h)\\) is scale dependent like any covariance. So \\(\\gamma_X(t, t+h) \\in \\mathbb{R}\\). If \\(\\mid \\gamma_X(t, t+h) \\mid\\) is “close” to 0, then they are “less (linearly) dependent”. If \\(\\mid \\gamma_X(t, t+h) \\mid\\) is “far” from 0, \\(X_{t}\\) and \\(X_{t+h}\\) are “more (linearly) dependent”. However in general, it is difficult to assess what “close” and “far” from zero means. In general, \\(\\gamma_X(t, t+h)=0\\) does not imply \\(X_{t}\\) and \\(X_{t+h}\\) are independent. However, if \\(X_{t}\\) and \\(X_{t+h}\\) are joint normally distributed, then \\(\\gamma_X(t, t+h)=0\\) implies that \\(X_{t}\\) and \\(X_{t+h}\\) are independent. Another measure of linear dependence which is related to the ACV is the AutoCorrelation. This is arguably the most commonly used metric in time series analysis. Definition 2.13 (AutoCorrelation) AutoCorrelation (ACF) denoted as \\(\\rho_X(t, t+h)\\) is defined as \\[\\begin{align*} \\rho_X(t,t+h) = \\text{Corr}(X_{t},X_{t+h}) = \\frac{\\text{Cov}(X_{t},X_{t+h})}{\\sqrt{\\text{Var}(X_{t})} \\sqrt{\\text{Var}(X_{t+h})}}. \\end{align*}\\] Remark 2.3 (Properties of ACF) 1. \\(\\mid rho_X(t,t+h) \\mid leq 1\\). 2. Scale invariant: ACF \\(\\rho_X(t,t+h)\\) is scale free like any correlation. Moreover, if \\(\\rho_X(t,t+h)\\) is “close” to \\(\\pm 1\\), then this implies that there is “strong” (linear) dependence between \\(X_{t}\\) and \\(X_{t+h}\\). We can simplify the notations \\(\\gamma_X(t, t+h)\\) and \\(\\rho_X(t, t+h)\\) to be \\(\\gamma(t, t+h)\\) and \\(\\rho(t, t+h)\\) when there is no ambiguity (i.e. only one time series is considered). Notice that both ACV and ACF are appropriate to measure linear dependence only. Besides linear dependence, other forms of dependence such as monotonic or nonlinear dependence also exist. However, both ACV and ACF are less helpful to measure these dependence as they might have ACV and ACF to be zero. "]
]
