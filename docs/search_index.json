[
<<<<<<< HEAD
["the-generalized-method-of-wavelet-moments.html", "Chapter 5 The Generalized Method of Wavelet Moments", " Chapter 5 The Generalized Method of Wavelet Moments A new framework for inertial sensor calibration. It is based on the Generalized Method of Wavelet Moments (GMWM) of Guerrier et al. (2013), which is a new statistical approach to estimate the parameters of (complex) time series models. The GMWM is able to estimate efficiently time series models which are commonly used to describe the errors of inertial sensors. This calibration approach provides considerable improvements (in terms of navigation performance) compared to existing methods. This methodology is robust (potentially applicable for FDI purposes) and is able to automatically select a suitable model (or rank models). The GMWM estimator is a GMM type estimator based on a quantity called the Wavelet Variance (WV). This quantity is computed by taking the variance of the Wavelet Coefficient wich are defined as Definition 5.1 (Wavelet Coefficient) In a similar way to the AV, we can define the Wavelet Variance (WV) at dyadic scales (\\(\\tau_j\\)) for \\(j \\in \\left\\{x \\in \\mathbb{N} \\, : \\; 1 \\leq x &lt; \\log_2 (T) - 1 \\right\\}\\). To do so, we first need to define the wavelet filters \\(h_{j,l}\\) as ``weights’’ having the following properties \\[\\begin{equation*} \\sum_{l=0}^{L_j-1} h_{j,l}=0, \\,\\, \\sum_{l=0}^{L_1-1} h_{1,l}^2 =\\frac{1}{2} \\, \\mathrm{and } \\, \\sum_{l=-\\infty}^{\\infty} h_{1,l} h_{1,l+2m} = 0 , \\label{def:wvfilter} \\end{equation*}\\] where \\(m \\in \\mathbb{N}^+\\), \\(L_j = (2^j-1)(L_1-1)+1\\) is the length of the filter at level \\(j\\) and \\(L_1\\) is the length of the first level filter \\(h_{1,l}\\).\\[0.2cm] Then, the wavelet coefficients \\(W_{j,t}\\) are defined as \\[\\begin{equation*} W_{j,t} = \\sum_{l=0}^{L_j-1}h_{j,l} X_{t-l}. \\end{equation*}\\] There exist many type of Wavelet Filter. In this book, we will focus on the Maximum Overlap Discrete Wavelet Transform (MODWT), which is the most usefull one. Figure 5.1: MODWT Wavelet Coefficient References "]
=======
["index.html", "An Introduction to Inertial Sensors Stochastic Calibration Chapter 1 Introduction", " An Introduction to Inertial Sensors Stochastic Calibration Stéphane Guerrier, Roberto Molinari, Yuming Zhang, Haotian Xu, Gaetan Bakalli, Ahmed Radi and Mucyo Karemera 2018-07-17 Chapter 1 Introduction TO DO (Introduction to the text) "],
["timeseries.html", "Chapter 2 Introduction to Time Series Analysis 2.1 Time Series 2.2 Dependence within Time Series 2.3 Stationarity 2.4 Linear Processes 2.5 Basic Time Series Models 2.6 Fundamental Representations of Time Series 2.7 Estimation Problems with Time Series", " Chapter 2 Introduction to Time Series Analysis In this chapter we give an introduction to time series analysis. For this purpose, it is organized in the following order: Definition and descriptive analysis of time series; Dependence within time series (and fundamental representations); Stationarity of time series; Basic time series models; Linear processes; Latent (or composite) stochastic processes; Estimation problems with time series. 2.1 Time Series Definition 2.1 (Time Series) A time series is a stochastic process (i.e. a sequence of random variables) defined on a common probability space. Let us denote this time-indexed stochastic process (time series) as \\((X_t)_{t = 1,...,T}\\), i.e. (\\(X_1\\), \\(X_2\\), …., \\(X_T\\)), where the time \\(t\\) belongs to the discrete index set. Therefore, we implicitly assume that \\(t\\) is non-random, i.e. the time at which each observation is measured is known, and the time between two consecutive observations is constant (i.e. sampling occurs at regular intervals). As for any data analysis procedure, the first step consists in representing the data in such a way as to highlight any important features or information that should be taken into account for the following statistical analysis. For time series, a typical first step is representing the observations over time (where the latter is represented on the x-axis and values of \\(X_t\\) on the y-axis). This can be considered as the first step for the descriptive analysis of a time series, especially when their length is moderate. With this in mind, when performing a descriptive analysis of a time series it is customary to check the following aspects: Trends Seasonal (e.g. business cycles) Non-seasonal (e.g. impact of economic indicators on stock returns) Local fluctuations (e.g. vibrations observed before, during and after an earthquake) Changes in the statistical properties Mean (e.g. economic crisis) Variance (e.g. earnings) States (e.g. bear/bull in finance) Model deviations (e.g. outliers) In order to give an idea of what the above characteristics imply for a time series, the following examples provide practical insight for some of them. Example 2.1 (Johnson and Johnson Quarterly Earnings) A first example of a time series is the quarterly earnings of the company Johnson and Johnson. In the graph below, we present these earnings between 1960 and 1980. # Load simts package library(simts) # Load data data(jj, package = &quot;astsa&quot;) # Construct gts object jj = gts(jj, start = 1960, freq = 4) # Plot time series plot(jj, main = &quot;Johnson and Johnson Quarterly Earnings&quot;, xlab = &quot;Time (year)&quot;, ylab = &quot;Quarterly Earnings per Share ($)&quot;) Figure 2.1: Johnson and Johnson Quarterly Earnings As we can see from the plot, the data contains a non-linear increasing trend as well as a seasonal component highlighted by the almost regularly-spaced peaks and valleys along time. In addition, we can notice that the variability of the data seems to increase with time (the seasonal variations appear to be larger towards the end of the plot). Hence, this simple visual representation can deliver important insight as to the behaviour of the time series and consequently determine the steps to take for further analysis (e.g. consider a non-linear model to explain the trend and consider approaches to model changing variance). \\(\\LARGE{\\bullet}\\) Example 2.2 (Monthly Precipitation Data) Let us consider another data set coming from the domain of hydrology. The data records monthly precipitation (in mm) over a certain period of time (1907 to 1972) and is interesting for hydrologists for the purpose of studying water cycles. The data are presented in the plot below: # Load data data(hydro, package = &quot;simts&quot;) # Construct gts object hydro = gts(hydro, start = 1907, freq = 12) # Plot time series plot(hydro, main = &quot;Monthly Precipitation Data&quot;, xlab = &quot;Time (year)&quot;, ylab = &quot;Mean Monthly Precipitation (mm)&quot;) Figure 2.2: Monthly Precipitation Data The time series plot above differs considerably from the previous one since the values of the time series, being always non-negative, remain almost always between 0 and 1 (no apparent trend) and randomly shows some larger observations that go beyond the value of 2 (or even 3). The latter appear to be extreme observations which could qualify as outliers, i.e. observations that are not representative of the true underlying model that generates the time series and can considerably affect the statistical analysis if not dealt with. \\(\\LARGE{\\bullet}\\) Example 2.3 (Inertial Sensor Data) Another example is provided by the data coming from the calibration procedure of an Inertial Measurement Unit (IMU). The signals (or time series) coming from an IMU are usually measured at high frequencies over a long time and are often characterized by linear trends and numerous underlying stochastic processes. The plot below represents the time series of an error signal coming from a gyroscope belonging to an IMU. # Load data data(imu6, package = &quot;imudata&quot;) # Construct gts object imu = gts(imu6[,1], freq = 100*60*60) # Plot time series plot(imu, main = &quot;Inertial Sensor Data&quot;, ylab = expression(paste(&quot;Angular Velocity &quot;, (rad/s))), xlab = &quot;Time (h)&quot;) Figure 2.3: Inertial Sensor Data As we can see from the plot, there wouldn’t appear to be any linear trend, seasonality or increased variation in the time series. Indeed, from this plot it is difficult to detect any particular characteristic of this time series although a linear trend and other processes are present in this data. Therefore, especially when the length of the time series is considerable, this representation of time series can only give insight into few aspects (if none) and consequently is not suitable, on its own, to adequately inform the subsequent statistical analysis. \\(\\LARGE{\\bullet}\\) In order to deliver a more appropriate (or more complete) representation of a time series, it is important to study the concept of dependence since (in a linear vision of time) past observations have an influence on present and (possibly) future ones. Hence, the next section gives an overview of this concept. 2.2 Dependence within Time Series As mentioned above, it is straightforward to assume that observations measured through time are dependent on each other (in that observations at time \\(t\\) have some form of impact on observations at time \\(t+1\\) or beyond). Due to this characteristic, one of the main interests in time series is prediction where, if \\((X_t)_{t=1,\\ldots,T}\\) is an identically distributed but not independent sequence, we often want to know the value of \\({X}_{T+h}\\) for \\(h &gt; 0\\) (i.e. an estimator of \\(\\mathbb{E}[X_{T+h}| X_T,...]\\)). In order to tackle this challenge, we first need to understand the dependence between \\(X_{1},\\ldots,X_{T}\\) and, even before this, we have to formally define what independence is. Definition 2.2 (Independence of Events) Two events \\(A\\) and \\(B\\) are independent if \\[\\begin{align*} \\mathbb{P}(A \\cap B) = \\mathbb{P}(A)\\mathbb{P}(B), \\end{align*}\\] with \\(\\mathbb{P}(A)\\) denoting the probability of event \\(A\\) occuring and \\(\\mathbb{P}(A \\cap B)\\) denoting the joint probability (i.e. the probability that events \\(A\\) and \\(B\\) occur jointly). In general, \\(A_{1},\\ldots,A_{n}\\) are independent if \\[\\begin{align*} \\mathbb{P}(A_1 \\ldots A_n) = \\mathbb{P}(A_1) \\ldots \\mathbb{P}(A_n) \\;\\; \\forall \\; A_i \\in S, \\;\\; i=1,\\ldots,n \\end{align*}\\] where \\(S\\) is the sample space. Definition 2.3 (Independence of Random Variables) Two random variables \\(X\\) and \\(Y\\) with Cumulative Distribution Functions (CDF) \\(F_X(x)\\) and \\(F_Y(y)\\), respectively, are independent if and only if their joint CDF \\(F_{X,Y}(x,y)\\) is such that \\[\\begin{align*} F_{X,Y}(x,y) = F_{X}(x) F_{Y}(y). \\end{align*}\\] In general, random variables \\(X_1, \\ldots, X_n\\) with CDF \\(F_{X_1}(x_1), \\ldots, F_{X_n}(x_n)\\) are respectively independent if and only if their joint CDF \\(F_{X_1, \\ldots, X_n}(x_1, \\ldots, x_n)\\) is such that \\[\\begin{align*} F_{X_1,\\ldots,X_n}(x_1,\\ldots,x_n) = F_{X_1}(x_1) \\ldots F_{X_n}(x_n). \\end{align*}\\] Definition 2.4 (iid sequence) The sequence \\(X_{1},X_{2},\\ldots,X_{T}\\) is said to be independent and identically distributed (i.e. iid) if and only if \\[\\begin{align*} \\mathbb{P}(X_{i}&lt;x) = \\mathbb{P}(X_{j}&lt;x) \\;\\; \\forall x \\in \\mathbb{R}, \\forall i,j \\in \\{1,\\ldots,T\\}, \\end{align*}\\] and \\[\\begin{align*} \\mathbb{P}(X_{1}&lt;x_{1},X_{2}&lt;x_{2},\\ldots,X_{T}&lt;x_{T})=\\mathbb{P}(X_{1}&lt;x_1) \\ldots \\mathbb{P}(X_{T}&lt;x_T) \\;\\; \\forall T\\geq2, x_1, \\ldots, x_T \\in \\mathbb{R}. \\end{align*}\\] The basic idea behind the above definitions of independence is the fact that the probability of an event regarding variable \\(X_i\\) remains unaltered no matter what occurs for variable \\(X_j\\) (for \\(i \\neq j\\)). From this definition, we can now start exploring the concept of dependence, starting from linear dependence within a time series. For this purpose, below we define a quantity called AutoCovariance (ACV). Definition 2.5 (AutoCovariance) Autocovariance denoted as \\(\\gamma_X(t, t+h)\\) is defined as \\[\\begin{align*} \\gamma_X(t, t+h) = \\text{Cov}(X_{t},X_{t+h})= \\mathbb{E}(X_{t}X_{t+h})-\\mathbb{E}(X_{t})\\mathbb{E}(X_{t+h}), \\end{align*}\\] where \\(\\text{Cov}(\\cdot)\\) denotes the covariance and \\[\\begin{align*} \\mathbb{E}(X_{t}) = \\int_{-\\infty}^{\\infty}x \\, f(x) \\, dx \\;\\; \\text{and} \\;\\; \\mathbb{E}(X_{t},X_{t+h}) = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}x_{t}\\,x_{t+h}\\, f(x_{t},x_{t+h}) \\, dx_{t}\\,dx_{t+h}, \\end{align*}\\] where \\(f(x)\\) denotes the density of \\(X_t\\) and \\(f(x_{t},x_{t+h})\\) denotes the joint density of \\(X_{t}\\) and \\(X_{t+h}\\). Notice that, when two variable are independent, \\(\\mathbb{E}(X_{t}X_{t+h}) = \\mathbb{E}(X_{t})\\mathbb{E}(X_{t+h})\\) and hence \\(\\gamma_X(t, t+h) = 0\\). In a nutshell, the ACV measures the degree to which the mean-behaviour of a variable (e.g. \\(X_{t+h}\\)) changes when another one changes (e.g. \\(X_t\\)) and hence, to what extent they are linearly dependent. Remark 2.1 (Properties of ACV) ACV is symmetric, i.e. \\(\\gamma_X(t, t+h) = \\gamma_X(t+h, t)\\) as \\(\\text{Cov}(X_{t},X_{t+h}) = \\text{Cov}(X_{t+h},X_{t})\\). Under stationarity (will be discussed very soon, see Section 2.3 for more details), \\(\\gamma_X(h) = \\gamma_X(-h)\\), i.e. ACV is an even function. Variance of the process \\(\\text{Var}(X_t) = \\gamma_X(t, t) \\geq 0\\). Under stationarity, \\(\\text{Var}(X_t) = \\gamma_X(0)\\) and \\(\\mid \\gamma_X(h) \\mid \\leq \\gamma_X(0)\\) by Cauchy-Schwarz inequality. Scale dependent: ACV \\(\\gamma_X(t, t+h)\\) is scale dependent like any covariance. So \\(\\gamma_X(t, t+h) \\in \\mathbb{R}\\). If \\(\\mid \\gamma_X(t, t+h) \\mid\\) is “close” to 0, then \\(X_{t}\\) and \\(X_{t+h}\\) are “less (linearly) dependent”. If \\(\\mid \\gamma_X(t, t+h) \\mid\\) is “far” from 0, then \\(X_{t}\\) and \\(X_{t+h}\\) are “more (linearly) dependent”. However in general, it is difficult to assess what “close” and “far” from zero mean. In general, \\(\\gamma_X(t, t+h)=0\\) does not imply \\(X_{t}\\) and \\(X_{t+h}\\) are independent. However, if \\(X_{t}\\) and \\(X_{t+h}\\) are joint normally distributed, then \\(\\gamma_X(t, t+h)=0\\) implies that \\(X_{t}\\) and \\(X_{t+h}\\) are independent. However the ACV is a quantity whose bounds are not known a priori and it can consequently be impossible to determine whether the degree of linear dependence is large or not. For this reason another measure of linear dependence can be used which is related to the ACV. Indeed, the AutoCorrelation Function (ACF) is a commonly used metric in time series analysis and is defined below. Definition 2.6 (AutoCorrelation) AutoCorrelation (ACF) denoted as \\(\\rho_X(t, t+h)\\) is defined as \\[\\begin{align*} \\rho_X(t,t+h) = \\text{Corr}(X_{t},X_{t+h}) = \\frac{\\text{Cov}(X_{t},X_{t+h})}{\\sqrt{\\text{Var}(X_{t})} \\sqrt{\\text{Var}(X_{t+h})}}, \\end{align*}\\] where \\(\\text{Var}(\\cdot)\\) denotes the variance. Remark 2.2 (Properties of ACF) \\(\\mid \\rho_X(t,t+h) \\mid \\leq 1\\) and \\(\\mid \\rho_X(t,t) \\mid = 1\\). ACF is symmetric, i.e. \\(\\rho_X(t, t+h) = \\rho_X(t+h, t)\\) as \\(\\text{Corr}(X_{t},X_{t+h}) = \\text{Corr}(X_{t+h},X_{t})\\). Under stationarity, \\(\\gamma_X(h) = \\gamma_X(-h)\\), i.e. ACF is an even function. Scale invariant: ACF \\(\\rho_X(t,t+h)\\) is scale free like any correlation. Moreover, if \\(\\rho_X(t,t+h)\\) is “close” to \\(\\pm 1\\), then this implies that there is “strong” (linear) dependence between \\(X_{t}\\) and \\(X_{t+h}\\). Since the ACV is bounded by the product of the standard deviations of the two variables, we have that the ACF is bounded between 1 and -1. Therefore it is possible to better interpret the linear dependence where an ACF of zero indicates an absence of linear dependence while an ACF close to 1 or -1 indicates respectively a strong positive or negative linear dependence. As underlined up to now, both ACV and ACF are appropriate to measure linear dependence only. Therefore if the ACV and ACF are both zero, this does not imply that there isn’t dependence between the variables. Indeed, aside from linear dependence, other forms of dependence such as monotonic or nonlinear dependence also exist which the ACV or ACF don’t measure directly. The figure below shows scatterplots (one variable on the x-axis and another on the y-axis) along with the values of the ACF: Figure 2.4: Different forms of dependence and their ACF values As can be seen, the first row in the figure shows a consistent behaviour of the ACF value since the absolute value goes closer to 1 the more the points go closer to forming a line. However, the following rows show plots where there is clearly a strong relation (dependence) between the variables but the ACF doesn’t detect this characteristic since this relation is not linear. Finally, it is worth noting that correlation does NOT imply causation. For example, if \\(\\rho(t, t+h) \\neq 0\\), this does not imply that \\(X_t \\to X_{t+h}\\) is causal. More specifically, the presence (or absence) of causality cannot be detected through statistical tools although some approximated metrics have been proposed to measure this concept (e.g Granger causality, see Granger 1969). For the following sections we will simplify the notations \\(\\gamma_X(t, t+h)\\) and \\(\\rho_X(t, t+h)\\) to be \\(\\gamma(h)\\) and \\(\\rho(h)\\) when there is no ambiguity (i.e. only one time series is considered and the ACF only depends on the time-lag \\(h\\)). 2.3 Stationarity In this section we are going to introduce the concept of stationarity, one of the most important characteristics of time series data. First let us consider an example of non-stationary processes. Example 2.4 (Non-Stationary Process) \\[\\begin{equation*} X_t \\sim \\mathcal{N} \\left(0, Y_t^2\\right) \\;\\; \\text{where $Y_t$ is unobserved and such that} \\;\\; Y_t \\overset{iid}{\\sim} \\mathcal{N} \\left(0, 1\\right). \\end{equation*}\\] In this case, it is clear that the estimation of \\(\\text{Var}(X_t)\\) is difficult since only \\(X_t\\) is useful for the estimation. So in fact, \\(X_t^2\\) is our best guess for \\(\\text{Var}(X_t)\\). On the other hand, let us consider an example of stationary processes where averaging becomes meaningful for such process. Example 2.5 (Stationary Process) \\[\\begin{equation*} X_t = \\theta W_{t-1} + W_t \\;\\;\\; \\text{where} \\;\\;\\; W_t \\stackrel{iid}{\\sim} \\mathcal{N} \\left(0, 1\\right). \\end{equation*}\\] In this case, we can guess that a natural estimator of \\(\\text{Var}(X_t)\\) can be \\(\\hat{\\sigma}^2 = \\frac{1}{T} \\sum_{i = 1}^T X_i^2\\). That is, now averages are meaningful for such process. We formalize the above idea by introducing the concept of stationarity. There exist two forms of stationarity, which are defined below: Definition 2.7 (Strong Stationarity) The time series \\(X_{t}\\) is strongly stationary if the joint probability distribution is invariant under a shift in time, i.e. \\[\\begin{equation*} \\mathbb{P}(X_{t}\\leq x_{0},\\ldots,X_{t+k}\\leq x_{k}) = \\mathbb{P}(X_{t+h}\\leq x_{0} ,\\ldots,X_{t+h+k}\\leq x_{k}) \\end{equation*}\\] for any time shift \\(h\\) and any \\(x_{0}, x_{1},x_{2},\\cdots,x_{k}\\) belong to the domain of \\(X_t,\\cdots,X_{t+k}\\) and \\(X_{t+h},\\cdots,X_{t+h+k}\\). Definition 2.8 (Weak Stationarity) The time series \\((X_{t})_{t \\in \\mathbb{N}}\\) is weakly stationary if the mean and autocovariance are finite and invariant under a shift in time, i.e. \\[\\begin{equation*} \\begin{aligned} \\mathbb{E}\\left[X_t\\right] &amp;= \\mu &lt; \\infty,\\\\ \\mathbb{E}\\left[X_t^2\\right] &amp;= \\mu_2 &lt; \\infty,\\\\ \\text{Cov}(X_{t},X_{t+h})&amp;= \\text{Cov}(X_{t + k},X_{t+h + k}) = \\gamma( h ). \\end{aligned} \\end{equation*}\\] for any time shift \\(h\\). For convenience, we use the abbreviation “stationary” to indicate “weakly stationary” by default. The stationarity of \\(X_{t}\\) is important because it provides a framework in which averaging makes sense. The concept of averaging is essentially meaningless unless properties like mean and covariance are either fixed or evolve in a known manner. Remark 2.3 (Implication on the ACV and ACF) If a process is weakly stationary or strongly stationary and \\(\\text{Cov}(X_{t},X_{t+h})\\) exists for all \\(h \\in \\mathbb{Z}\\), then we have both ACV and ACF only depend on the lag between observations, i.e. \\[\\begin{equation*} \\begin{aligned} \\gamma(t, t+h) &amp;= \\text{Cov}(X_{t},X_{t+h})= \\text{Cov}(X_{t + k},X_{t+h + k}) = \\gamma(t+k, t+h+k) = \\gamma(h),\\\\ \\rho(t, t+h) &amp;= \\text{Corr}(X_{t},X_{t+h})= \\text{Corr}(X_{t + k},X_{t+h + k}) = \\rho(t+k, t+h+k) = \\rho(h). \\end{aligned} \\end{equation*}\\] Remark 2.4 (Relation between Strong and Weak Stationarity) In general, neither type of stationarity implies the other one. However, If \\(X_{t}\\) is Normal (Gaussian) with \\(\\sigma^2 = \\text{Var} (X_{t}) &lt; \\infty\\), then weak stationarity implies strong stationarity. If \\(X_{t}\\) is strongly stationary, \\(\\mathbb{E}(X_t) &lt; \\infty\\) and \\(\\mathbb{E}(X_t^2) &lt; \\infty\\), then \\(X_{t}\\) is weakly stationary. Example 2.6 (Strong Stationarity does NOT imply Weak Stationarity) An iid Cauchy process is strongly but not weakly stationary as the mean of the process does not exist. Example 2.7 (Weak Stationarity does NOT imply Strong Stationarity) Let \\(X_t \\overset{iid}{\\sim} \\exp(1)\\) (i.e. exponential distribution with \\(\\lambda = 1\\)) and \\(Y_t \\overset{iid}{\\sim} \\mathcal{N}(1,1)\\). Then, let \\[\\begin{equation*} Z_t = \\left\\{ \\begin{array}{cl} X_t &amp;\\text{if } t \\in \\left\\{2k | k \\in \\mathbb{N}\\right\\}\\\\ Y_t &amp;\\text{if } t \\in \\left\\{2k + 1 | k \\in \\mathbb{N}\\right\\}, \\end{array} \\right. \\end{equation*}\\] we have \\(Z_t\\) is weakly stationary but not strongly stationary. Remark 2.5 (Stationarity of Latent Time Series Processes) (Weakly) Stationary: WN, QN, AR1 (Weakly) Non-Stationary: DR, RW Proof (AR1 is weakly stationary). Consider an AR1 process defined as: \\[\\begin{equation*} X_t = \\phi X_{t-1} + Z_t, \\;\\;\\; Z_t \\overset{iid}{\\sim} \\mathcal{N}(0,\\nu^2), \\end{equation*}\\] where \\(\\mid \\phi \\mid &lt; 1\\) and \\(\\nu^2 &lt; \\infty\\). Then we have \\[\\begin{aligned} {X_t} &amp;= {\\phi }{X_{t - 1}} + {Z_t} = \\phi \\left[ {\\phi {X_{t - 2}} + {Z_{t - 1}}} \\right] + {Z_t} = {\\phi ^2}{X_{t - 2}} + \\phi {Z_{t - 1}} + {Z_t} \\\\ &amp; \\; \\vdots \\\\ &amp;= {\\phi ^k}{X_{t-k}} + \\sum\\limits_{j = 0}^{k - 1} {{\\phi ^j}{Z_{t - j}}} . \\end{aligned} \\] By taking the limit in \\(k\\) (which is perfectly valid as we assume \\(t \\in \\mathbb{Z}\\)), we obtain \\[\\begin{equation*} \\begin{aligned} X_t = \\mathop {\\lim }\\limits_{k \\to \\infty} \\; {X_t} = \\sum\\limits_{j = 0}^{\\infty} {{\\phi ^j}{Z_{t - j}}}. \\end{aligned} \\end{equation*}\\] So we have \\[\\begin{equation*} \\begin{aligned} \\mathbb{E}\\left[X_t\\right] &amp;= \\sum\\limits_{j = 0}^{\\infty} {{\\phi ^j}{\\mathbb{E} [Z_{t - j}]}} = 0, \\\\ \\text{Var}\\left(X_t\\right) &amp;= \\text{Var}\\left(\\sum\\limits_{j = 0}^{\\infty} {{\\phi ^j}{Z_{t - j}}}\\right) = \\sum\\limits_{j = 0}^{\\infty} {\\phi^{2j}} \\text{Var}\\left(Z_{t-j}\\right) = \\nu^2 \\sum\\limits_{j = 0}^{\\infty} {\\phi^{2j}} = \\frac{\\nu^2}{1-\\phi^2} &lt; \\infty. \\end{aligned} \\end{equation*}\\] Moreover, assuming for notational simplicity that \\(h &gt; 1\\), we obtain \\[\\begin{equation*} \\begin{aligned} \\text{Cov}\\left(X_t, X_{t+h}\\right) &amp;= \\phi \\text{Cov}\\left(X_t, X_{t+h-1}\\right) = \\phi^2 \\text{Cov}\\left(X_t, X_{t+h-2}\\right) = \\ldots = \\phi^h \\text{Cov}(X_t, X_t). \\end{aligned} \\end{equation*}\\] In general, when \\(h \\in \\mathbb{Z}\\) we obtain \\[\\begin{equation*} \\begin{aligned} \\text{Cov}\\left(X_t, X_{t+h}\\right) &amp; = \\phi^{|h|} \\text{Cov}(X_t, X_t) = \\phi^{|h|} \\frac{\\nu^2}{1-\\phi^2}, \\end{aligned} \\end{equation*}\\] which is a function of the lag \\(h\\) only. Therefore, this AR1 process is weakly stationary. 2.4 Linear Processes In this section we introduce the concept of linear processes. As a matter of fact, considered stationary models can all, so far, be represented as linear processes. Definition 2.9 (Linear Processes) A stochastic process \\((X_t)\\) is said to be a linear process if it can be expressed as a linear combination of an iid Gaussian sequence (i.e. white noise process), i.e.: \\[{X_t} = \\mu + \\sum\\limits_{j = - \\infty }^\\infty {{\\psi _j}{W_{t - j}}} \\] where \\(W_t \\overset{iid}{\\sim} \\mathcal{N}(0, \\sigma^2)\\) and \\(\\sum\\limits_{j = - \\infty }^\\infty {\\left| {{\\psi _j}} \\right|} &lt; \\infty\\). Notice that the condition \\(\\sum\\limits_{j = - \\infty }^\\infty {\\left| {{\\psi _j}} \\right|} &lt; \\infty\\) is required in the definition of linear processes in order to ensure that the series has a limit and is related to the absolutely summable covariance structure, which is defined below. Definition 2.10 (Absolutely Summable Covariance Structure) A process \\((X_t)\\) is said to have an absolutely summable covariance structure if \\(\\sum\\limits_{h = - \\infty }^\\infty {\\left| \\gamma_X(h) \\right|} &lt; \\infty\\). Remark 2.6 (Properties of Linear Processes) All linear processes are stationary since \\[\\begin{aligned} \\mathbb{E}[X_t] &amp;= \\mu, \\\\ \\gamma(h) &amp;= \\sigma^2\\sum\\limits_{j = - \\infty }^\\infty {{\\psi _j}{\\psi _{j+h}}}. \\end{aligned}\\] All linear processes have absolutely summable covariance structures. Proof (ACV of Linear Processes). \\[\\begin{align*} \\gamma(h) &amp;= \\text{Cov}(X_t, X_{t+h}) = \\text{Cov}(\\mu+\\sum_{j=-\\infty}^{\\infty} \\psi_j W_{t-j}, \\mu+\\sum_{j=-\\infty}^{\\infty} \\psi_j W_{t+h-j})\\\\ &amp;= \\text{Cov}(\\sum_{j=-\\infty}^{\\infty} \\psi_j W_{t-j}, \\sum_{j=-\\infty}^{\\infty} \\psi_j W_{t-(j-h)}) \\\\ &amp;= \\text{Cov}(\\sum_{j=-\\infty}^{\\infty} \\psi_j W_{t-j}, \\sum_{j=-\\infty}^{\\infty} \\psi_{j+h} W_{t-j}) \\\\ &amp;= \\sum_{j=-\\infty}^{\\infty} \\psi_j \\psi_{j+h} \\text{Cov}(W_{t-j}, W_{t-j}) \\\\ &amp;= \\sigma^2\\sum\\limits_{j = - \\infty }^\\infty {{\\psi _j}{\\psi _{j+h}}}. \\end{align*}\\] Proof (All linear processes have absolutely summable covariance structures.). \\[\\begin{align*} \\sum_{h=-\\infty}^{\\infty} \\mid \\gamma(h) \\mid &amp;= \\sum_{h=-\\infty}^{\\infty} \\sigma^2 \\mid \\sum_{j=-\\infty}^{\\infty} \\psi_j \\psi_{j+h} \\mid \\\\ &amp;\\leq \\sigma^2 \\sum_{h=-\\infty}^{\\infty} \\sum_{j=-\\infty}^{\\infty} \\mid \\psi_j \\psi_{j+h} \\mid \\\\ &amp;= \\sigma^2 \\sum_{j=-\\infty}^{\\infty} \\sum_{h=-\\infty}^{\\infty} \\mid \\psi_j \\mid \\cdot \\mid \\psi_{j+h} \\mid \\\\ &amp;= \\sigma^2 \\sum_{j=-\\infty}^{\\infty} \\mid \\psi_j \\mid \\sum_{h=-\\infty}^{\\infty} \\mid \\psi_{j+h} \\mid \\\\ &amp;= \\sigma^2 \\big( \\sum_{j=-\\infty}^{\\infty} \\mid \\psi_j \\mid \\big)^2 &lt; \\infty \\end{align*}\\] So with the assumption that \\(\\sum_{j=-\\infty}^{\\infty} \\mid \\psi_j \\mid &lt; \\infty\\), we obtain that all linear processes have absolutely summable covariance structures. Notice that here we have shown that the condition \\(\\sum\\limits_{j = - \\infty }^\\infty {\\left| {{\\psi _j}} \\right|} &lt; \\infty\\) is actually stronger than \\(\\sum\\limits_{h = - \\infty }^\\infty {\\left| \\gamma(h) \\right|} &lt; \\infty\\). Example 2.8 (AR1 is a linear process) When we prove above that AR1 is weakly stationary, we have shown that for an AR1 process \\(X_t = \\phi X_{t-1} + Z_t, \\;\\;\\; Z_t \\overset{iid}{\\sim} \\mathcal{N}(0,\\nu^2)\\), it can be represented as \\[\\begin{align*} X_t = \\sum\\limits_{j = 0}^{\\infty} {{\\phi ^j}{Z_{t - j}}}. \\end{align*}\\] Therefore, AR1 is a linear process. 2.5 Basic Time Series Models We first introduce some latent time series models that are commonly used, especially in the calibration procedure of inertial sensors. Definition 2.11 (Gaussian White Noise) The Gaussian White Noise (WN) process with parameter \\(\\sigma^2 \\in \\mathbb{R}^+\\) is defined as \\[\\begin{equation*} X_t \\overset{iid}{\\sim} \\mathcal{N}\\left(0, \\sigma^2 \\right) \\end{equation*}\\] where “iid” stands for “independent and identically distributed”. Definition 2.12 (Quantization Noise) The Quantization Noise (QN) process with parameter \\(Q^2 \\in \\mathbb{R}^+\\) is a process with Power Spectral Density (PSD) of the form \\[\\begin{equation*} S_{X}(f) = 4 Q^2 \\sin^2 \\left( \\frac{\\pi f}{\\Delta t} \\right) \\Delta t, \\;\\; f &lt; \\frac{\\Delta t}{2}. \\end{equation*}\\] Definition 2.13 (Drift) The Drift (DR) process with parameter \\(\\omega \\in \\Omega\\), where \\(\\Omega\\) is either \\(\\mathbb{R}^+\\) or \\(\\mathbb{R}^-\\), is defined as \\[\\begin{equation*} X_t = \\omega t. \\end{equation*}\\] Definition 2.14 (Random Walk) The Random Walk (RW) process with parameter \\(\\gamma^2 \\in \\mathbb{R}^+\\) is defined as \\[\\begin{equation*} X_t = X_{t-1} + \\epsilon_t \\;\\; \\text{where}\\;\\; \\epsilon_t \\overset{iid}{\\sim} \\mathcal{N}\\left(0, \\gamma^2 \\right)\\;\\; \\text{and}\\;\\; X_0 = 0. \\end{equation*}\\] Definition 2.15 (Auto-Regressive) The Auto-Regressive process of Order 1 (AR1) with parameter \\(\\phi \\in (-1, +1)\\) and \\(\\upsilon^2 \\in \\mathbb{R}^+\\) is defined as \\[\\begin{equation*} X_t = \\phi X_{t-1} + Z_t, \\;\\;\\; Z_t \\overset{iid}{\\sim} \\mathcal{N}(0,\\upsilon^2). \\end{equation*}\\] Definition 2.16 (Gauss Markov) The Gauss Markov process of Order 1 (GM) with parameter \\(\\beta \\in \\mathbb{R}\\) and \\(\\sigma_G^2 \\in \\mathbb{R}^+\\) is defined as \\[\\begin{equation*} X_t = \\exp(-\\beta \\Delta_t) X_{t-1} + Z_t, \\;\\;\\; Z_t \\overset{iid}{\\sim} \\mathcal{N}(0,\\sigma^2_{G}(1-\\exp(-2\\beta\\Delta t))) \\end{equation*}\\] where \\(\\Delta t\\) denotes the time between \\(X_t\\) and \\(X_{t-1}\\). Remark 2.7 (GM and AR1) A GM process is a one-to-one reparametrization of an AR1 process. In the following, we will only discuss AR1 processes but all results remain valid for GM processes. With the above defined latent time series processes, we introduce the composite stochastic process, which is widely used in the estimation procedure of inertial sensor stochastic calibration. Definition 2.17 (Composite Stochastic Process) A composite stochastic process is a sum of latent processes. We implicitly assume that these latent processes are independent. Example 2.9 (2*AR1 + WN) The composite stochastic process of “2*AR1 + WN&quot; is given as \\[\\begin{align} Y_t &amp;= \\phi_1 Y_{t-1} + Z_t, \\;\\;\\; Z_t \\overset{iid}{\\sim} \\mathcal{N}(0,\\upsilon_1^2),\\\\ W_t &amp;= \\phi_2 W_{t-1} + U_t, \\;\\;\\; U_t \\overset{iid}{\\sim} \\mathcal{N}(0,\\upsilon_2^2),\\\\ Q_t &amp;\\overset{iid}{\\sim} \\mathcal{N}(0,\\sigma^2),\\\\ X_t &amp;= Y_t + W_t + Q_t, \\end{align}\\] where \\(Y_t\\), \\(W_t\\) and \\(Q_t\\) are independent and only \\(X_t\\) is observed. 2.6 Fundamental Representations of Time Series We conclude this chapter by summarizing the fundamental representations of time series. If two processes have the same fundamental representations, then these two processes are the same. There are two most commonly used fundamental representations of time series, i.e. ACV and ACF; Power Spectral Density (PSD). Remark 2.8 (ACV and ACF as fundamental representation) If we consider a zero mean normally distributed process, it is clear that its joint distribution is fully characterized by the autocovariances \\(\\mathbb{E}[X_t X_{t+h}]\\) since the joint probability density only depends on these covariances. Once we know the autocovariances we know everything there is to know about the process and therefore: if two processes have the same autocovariance function, then they are the same process. Remark 2.9 (PSD as fundamental representation) The PSD is defined as \\[\\begin{equation*} S_X(f) = \\int_{- \\infty}^{\\infty} \\gamma_{X}(h)\\,e^{-ifh}dh, \\end{equation*}\\] where \\(f\\) is a frequency. Hence, the PSD is a Fourier Transform (FT) of the autocovariance function which describes the variance of a time series over frequencies (with respect to lags \\(h\\)). Given that the definition of the PSD, as for the autocovariance function, once we know the PSD we know everything there is to know about the process and therefore: if two processes have the same PSD, then they are the same process. 2.7 Estimation Problems with Time Series Estimation in the context of time series is not as straightforward as in the iid case. For example, let us consider the easiest case of estimation: the sample mean of a stationary time series. Example 2.10 (Estimation with Sample Mean) Let \\((X_t)\\) be a stationary time series, so we have that \\(\\mathbb{E}[X_t] = \\mu\\) and the value of \\(\\mu\\) can be estimated by the sample mean, i.e. \\[\\begin{equation*} \\bar{X} = \\frac{1}{T} \\sum_{t = 1}^T X_t. \\end{equation*}\\] Using the properties of a stationary process, we obtain \\[\\begin{equation*} \\text{Var} \\left(\\bar{X}\\right) = \\frac{\\gamma(0)}{T} \\sum_{h = -T}^{T} \\left(1 - \\frac{|h|}{T}\\right) \\rho(h) \\end{equation*}\\] since \\[\\begin{align*} \\text{Var}(\\bar{X}) &amp;= \\frac{1}{T^2} \\text{Var}(\\sum_{t=1}^T X_t) = \\frac{1}{T^2} (\\sum_{t=1}^T \\text{Var}(X_t) + 2\\underset{1 \\leq t&lt;s\\leq T}{\\sum\\sum} \\text{Cov}(X_t, X_s)) \\\\ &amp;= \\frac{1}{T^2} (T\\gamma(0) + 2(T-1)\\gamma(1) + 2(T-2)\\gamma(2) + \\ldots + 2\\gamma(T-1)) \\\\ &amp;= \\frac{\\gamma(0)}{T^2} (T+ 2(T-1)\\rho(1) + \\ldots + 2\\rho(T-1)) \\\\ &amp;= \\frac{\\gamma(0)}{T} + \\frac{2\\gamma(0)}{T} \\sum_{h=1}^{T-1} (1-\\frac{h}{T})\\rho(h) \\\\ &amp;= \\frac{\\gamma(0)}{T} \\sum_{h = -(T-1)}^{T-1} \\left(1 - \\frac{|h|}{T}\\right) \\rho(h) \\\\ &amp;= \\frac{\\gamma(0)}{T} \\sum_{h = -T}^{T} \\left(1 - \\frac{|h|}{T}\\right) \\rho(h). \\end{align*}\\] Example 2.11 (Estimation with Sample Mean in AR1) As in the previous example, let us consider a stationary AR1 process, i.e. \\[\\begin{equation*} X_t = \\phi X_{t-1} + Z_t, \\;\\;\\; \\text{where} \\;\\;\\; |\\phi| &lt; 1 \\;\\;\\; \\text{and} \\;\\;\\; Z_t \\overset{iid}{\\sim} \\mathcal{N} \\left(0, \\nu^2\\right). \\end{equation*}\\] We have obtained before that in AR1, \\(\\gamma(h) = \\phi^h \\sigma^2 \\left(1 - \\phi^2\\right)^{-1}\\). Therefore, we obtain (after some computations): \\[\\begin{equation*} \\text{Var} \\left( {\\bar X} \\right) = \\frac{\\nu^2 \\left( T - 2\\phi - T \\phi^2 + 2 \\phi^{T + 1}\\right)}{T^2\\left(1-\\phi^2\\right)\\left(1-\\phi\\right)^2}. \\end{equation*}\\] Unfortunately, deriving such an exact formula is often difficult when considering more complex models. Therefore, asymptotic approximations are often employed to simplify the calculation. For example, in this AR1 case we have \\[\\begin{equation*} \\lim_{T \\to \\infty } \\; T \\text{Var} \\left( {\\bar X} \\right) = \\frac{\\nu^2}{\\left(1-\\phi\\right)^2}, \\end{equation*}\\] providing the following approximate formula \\[\\begin{equation*} \\text{Var} \\left( {\\bar X} \\right) \\approx \\frac{\\nu^2}{T \\left(1-\\phi\\right)^2}. \\end{equation*}\\] Alternatively, simulation methods can also be employed. For example, one could compute \\(\\text{Var} \\left( {\\bar X} \\right)\\) as follows: Step 1: Simulate under the assumed model, i.e. \\(X_t^* \\sim F_{\\theta_0}\\), where \\(F_{\\theta_0}\\) denotes the true model (in this case an AR1 process). Step 2: Compute \\({\\bar X^*}\\) (i.e. average based on \\((X_t^*)\\)). Step 3: Repeat Steps 1 and 2 \\(B\\) times. Step 4: Compute the empirical variance \\({\\bar X^*}\\) (based on \\(B\\) independent replications). The above procedure is known as Monte-Carlo method (in this case it is actually a Monte-Carlo integral) and is closely related to the concept of parametric bootstrap (see Efron and Tibshirani (1994)) which is a very popular tool in statistics. Now we define the classical estimators of \\(\\gamma(h)\\) and \\(\\rho(h)\\) for AutoCovariance and AutoCorrelation functions. Definition 2.18 (Sample AutoCovariance Function) The sample autocovariance function is defined as \\[\\begin{equation*} \\hat{\\gamma}(h) = \\frac{1}{T} \\sum_{t = 1}^{T-h} \\left(X_{t} - \\bar{X}\\right) \\left(X_{t+h} - \\bar{X}\\right) \\end{equation*}\\] with \\(\\hat{\\gamma}(h) = \\hat{\\gamma}(-h)\\) for \\(h = 0, 1, ..., k\\), where \\(k\\) is a fixed integer. Definition 2.19 (Sample AutoCorrelation Function) The sample autocorrelation function is defined as \\[\\begin{equation*} \\hat{\\rho}(h) = \\frac{\\hat{\\gamma}(h)}{\\hat{\\gamma}(0)} \\end{equation*}\\] with \\(\\hat{\\rho}(h) = \\hat{\\rho}(-h)\\) for \\(h = 0, 1, ..., k\\), where \\(k\\) is a fixed integer. We will discuss more about the properties of these estimators in the next chapter. References "],
["properties-of-statistical-estimators.html", "Chapter 3 Properties of Statistical Estimators 3.1 Extremum Estimators 3.2 Consistency 3.3 Asymptotic Normality", " Chapter 3 Properties of Statistical Estimators In this chapter, we will provide a review of the properties of statistical estimators. This chapter is organized with the following outline: Extremum estimators; Consistency; Asymptotic normality. 3.1 Extremum Estimators In this section, we will introduce a commonly used class of estimators, extremum estimators, and some examples of it. Definition 3.1 (Extremum Estimators) Many estimators have a common structure, which is often useful to study their asymptotic properties. One structure or framework is the class of estimators that maximize some objective function, referred to as extremum estimators, which can can be defined as follows: \\[\\begin{equation*} \\hat{\\boldsymbol{\\theta}} \\equiv \\underset{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}}{\\text{argmax}} \\; \\hat{Q}_n(\\boldsymbol{\\theta}) \\end{equation*}\\] where \\(\\boldsymbol{\\theta}\\) and \\(\\boldsymbol{\\Theta}\\) denote, respectively, the parameter vector of interest and its set of possible values. The vast majority of statistical estimators can be represented as extremum estimators. For example, least squares, maximum likelihood or (generalized) method of moment estimators can all be represented as extremum estimators. Example 3.1 (Least Squares Estimator as Extremum Estimator) Consider the linear model \\(\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{\\beta}_0 + \\boldsymbol{\\epsilon}\\) where \\(\\boldsymbol{X} \\in \\mathbb{R}^{n \\times p}\\) is a full rank constant matrix, \\(\\boldsymbol{\\beta} \\in {\\mathcal{B}} \\subseteq \\mathbb{R}^p\\) and \\(\\epsilon_i \\overset{iid}{\\sim} \\mathcal{N}(0, \\sigma_\\epsilon^2)\\). Let \\(\\hat{\\boldsymbol{\\beta}}\\) denote the Least Squares Estimator (LSE) of \\(\\boldsymbol{\\beta}_0\\), i.e. \\[\\begin{equation*} \\hat{\\boldsymbol{\\beta}} = \\left(\\boldsymbol{X}^T \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^T \\boldsymbol{y}. \\end{equation*}\\] This LSE is an extremum estimator since it can be expressed as \\[\\begin{equation*} \\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\beta} \\in \\mathcal{B}}{\\text{argmax}} \\; -||\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta} ||_2^2. \\end{equation*}\\] Example 3.2 (Maximum Likelihood Estimator as Extremum Estimator) Let \\(Z_1, \\ldots, Z_n\\) be an iid sample with pdf \\(f(z|\\boldsymbol{\\theta}_0)\\). The Maximum Likelihood Estimator (MLE) is given by \\[\\begin{equation*} \\hat{\\boldsymbol{\\theta}} = \\underset{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}}{\\text{argmax}} \\; \\frac{1}{n} \\sum_{i = 1}^{n} \\text{log}\\left[f\\left(z_i| \\boldsymbol{\\theta}\\right)\\right]. \\end{equation*}\\] Here instead of the actual log-likelihood, we are actually using a normalized log-likelihood, which has no impact on the estimator but the normalized form is more convenient to use when we let \\(n \\to \\infty\\). Therefore, the MLE can be seen as an extremum estimator with \\[\\begin{equation*} \\hat{Q}_n(\\boldsymbol{\\theta}) = \\frac{1}{n} \\sum_{i = 1}^{n} \\text{log}\\left[f\\left(z_i| \\boldsymbol{\\theta}\\right)\\right]. \\end{equation*}\\] Definition 3.2 (Generalized Method of Moments) Let \\(Z_1, \\ldots, Z_n\\) be an iid sample with pdf \\(f(z|\\boldsymbol{\\theta}_0)\\). Suppose that there is a moment function vector \\(\\boldsymbol{g}(z | \\boldsymbol{\\theta})\\) such that \\(\\mathbb{E}[\\boldsymbol{g} (z | \\boldsymbol{\\theta}_0)] = 0\\). Then the Generalized Method of Moments (GMM) estimator of \\(\\boldsymbol{\\theta}_0\\) is defined as \\[\\begin{equation*} \\hat{\\boldsymbol{\\theta}} = \\underset{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}}{\\text{argmax}} \\; - \\left[\\frac{1}{n} \\sum_{i = 1}^n \\boldsymbol{g}(z_i | \\boldsymbol{\\theta}) \\right]^T \\widehat{\\boldsymbol{W}} \\left[\\frac{1}{n} \\sum_{i = 1}^n \\boldsymbol{g}(z_i | \\boldsymbol{\\theta}) \\right], \\end{equation*}\\] where \\(\\widehat{\\boldsymbol{W}}\\) is an positive definite matrix of appropriate dimension. Alternatively (but equivalently), we can define GMM estimator as \\[\\begin{equation*} \\hat{\\boldsymbol{\\theta}} = \\underset{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}}{\\text{argmax}} \\; - || \\hat{\\boldsymbol{\\mu}} - \\boldsymbol{\\mu}(\\boldsymbol{\\theta}) ||_{\\widehat{\\boldsymbol{W}}}^2, \\end{equation*}\\] where \\(|| \\boldsymbol{x} ||_{\\boldsymbol{A}}^2 = \\boldsymbol{x}^T \\boldsymbol{A} \\boldsymbol{x}\\), and where \\(\\hat{\\boldsymbol{\\mu}}\\) and \\(\\boldsymbol{\\mu}(\\boldsymbol{\\theta})\\) denote, respectively, the empirical and model based moments. We will use this form of definition of GMM estimator more often. Example 3.3 (A Simple GMM Estimator as Extremum Estimator) Let \\(Z_i \\overset{iid}{\\sim} \\mathcal{N}(\\mu_0, \\sigma^2_0)\\) and \\(\\boldsymbol{\\theta}_0 = (\\mu_0, \\sigma_0^2)^T\\). Suppose we wish to estimate \\(\\boldsymbol{\\theta}_0\\) by matching the first three empirical moments with their theoretical counterparts. In this case, a reasonable moment function or condition defining a GMM estimator is given by: \\[\\begin{equation*} \\boldsymbol{g} (Z | \\boldsymbol{\\theta}) = \\begin{bmatrix} Z - \\mu\\\\ Z^2 - \\left(\\mu^2 + \\sigma^2\\right)\\\\ Z^3 - \\left(\\mu^3 + 3 \\mu \\sigma^2\\right) \\end{bmatrix}. \\end{equation*}\\] Notice that \\(\\frac{1}{n} \\sum_{i=1}^n \\boldsymbol{g} (Z_i | \\boldsymbol{\\theta}) = \\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta})\\), where \\(\\hat{\\boldsymbol{\\gamma}}\\) and \\(\\boldsymbol{\\gamma}(\\boldsymbol{\\theta})\\) denote, respectively, the empirical and model based moments, i.e. \\[\\begin{equation*} \\hat{\\boldsymbol{\\gamma}} = \\frac{1}{n} \\sum_{i = 1}^n \\begin{bmatrix} Z_i\\\\ Z_i^2\\\\ Z_i^3\\\\ \\end{bmatrix}, \\;\\;\\;\\; \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) = \\begin{bmatrix} \\mu\\\\ \\mu^2 + \\sigma^2\\\\ \\mu^3 + 3 \\mu \\sigma^2 \\end{bmatrix}. \\end{equation*}\\] Therefore we can write the GMM estimator of \\(\\boldsymbol{\\theta}_0\\) as \\[\\begin{equation*} \\hat{\\boldsymbol{\\theta}} = \\underset{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}}{\\text{argmin}} \\; || \\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma} (\\boldsymbol{\\theta}) ||_{\\widehat{\\boldsymbol{W}}}^2 = \\underset{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}}{\\text{argmax}} \\; - || \\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma} (\\boldsymbol{\\theta}) ||_{\\widehat{\\boldsymbol{W}}}^2 \\end{equation*}\\] 3.2 Consistency In this section, we will introduce one of the most important properties of statistical estimators, consistency. And we will discuss in details about the conditions for consistency of the extremum estimators. Definition 3.3 (Consistency) An estimator \\(\\hat{\\boldsymbol{\\theta}}\\) is said to be consistent or weakly consistent if it converges in probability to \\(\\boldsymbol{\\theta}_0\\), i.e. \\[\\begin{equation*} \\underset{n \\to \\infty}{\\text{lim}} \\mathbb{P} (||\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0||_2 \\geq \\epsilon) = 0, \\end{equation*}\\] for all \\(\\epsilon &gt;0\\). In Layman’s term, consistency simply means that if \\(n\\) is large enough, then \\(\\hat{\\boldsymbol{\\theta}}\\) will be arbitrarily close to \\(\\boldsymbol{\\theta}_0\\) (i.e. inside of an hypersphere of radius \\(\\epsilon\\) centered at \\(\\boldsymbol{\\theta}_0\\)). This also means the procedure (i.e. our estimator \\(\\hat{\\boldsymbol{\\theta}}\\)) based on unlimited data will be able to identify the underlying truth (i.e. \\(\\boldsymbol{\\theta}_0\\)). Figure 3.1: Interpretation of consistency To show the consistency of estimators, we often make use of the following two important results: Theorem 3.1 (Weak Law of Large Number) Suppose \\(X_i\\) are iid random variables with finite mean \\(\\mu\\) (i.e. \\(\\mathbb{E}[X_i] = \\mu\\)) and finite variance. Let \\(\\bar{X}_n = \\frac{1}{n} \\sum_{i = 1}^n X_i\\), then \\(\\bar{X}_n \\overset{\\mathcal{P}}{\\mapsto} \\mu\\). Theorem 3.2 (Continuous Mapping Theorem) If \\(Y_n \\overset{\\mathcal{P}}{\\mapsto} \\mu\\) and \\(g(\\cdot)\\) is a continuous function, then \\(g(Y_n) \\overset{\\mathcal{P}}{\\mapsto} g(\\mu)\\). Here we present a simple example to prove the consistency of an estimator with the above two results. Example 3.4 (Consistency in Exponential Distribution) Suppose we have an iid sample from exponential distribution, i.e. \\(X_i \\overset{iid}{\\sim} \\text{exp}(\\lambda_0),\\; \\lambda_0 \\in \\mathbb{R}^+, \\; i = 1,..., n\\). So assuming \\(X \\geq 0\\), the density of \\(X\\) is given by \\[\\begin{equation*} f(x|\\lambda) = \\lambda \\text{exp}\\left( - \\lambda x \\right). \\end{equation*}\\] In this example, we want to show that the MLE for \\(\\lambda_0\\) is a consistent estimator of \\(\\lambda_0\\). First, we want to find the MLE for \\(\\lambda_0\\). The normalized log-likelihood function is given by \\[\\begin{equation*} \\mathcal{L}(\\lambda | X_1, ..., X_n) = \\text{log}(\\lambda) - \\lambda \\bar{X}_n. \\end{equation*}\\] By solving \\[\\begin{equation*} \\frac{\\partial}{\\partial \\lambda} \\; \\mathcal{L}(\\lambda | X_1, \\ldots, X_n) = \\frac{1}{\\lambda} - \\bar{X}_n = 0, \\end{equation*}\\] we obtain \\(\\hat{\\lambda} = \\frac{1}{\\bar{X}_n}\\). We verify that \\[\\begin{equation*} \\frac{\\partial^2}{\\partial \\lambda^2} \\; \\mathcal{L}(\\lambda | X_1, \\ldots, X_n) = -\\frac{1}{\\lambda^2} &lt; 0, \\end{equation*}\\] which implies that \\(\\hat{\\lambda}\\) is the maxima of \\(\\mathcal{L}(\\lambda | X_1, \\ldots, X_n)\\). Therefore, the MLE for \\(\\lambda_0\\) is \\(\\hat{\\lambda} = \\frac{1}{\\bar{X}_n}\\). By Weak Law of Large Number, we have \\(\\bar{X}_n \\overset{\\mathcal{P}}{\\mapsto} \\mu\\), where is given by \\[\\begin{equation*} \\mu = \\mathbb{E}[X_i] = \\int_{0}^{\\infty} x \\lambda_0 \\text{exp}\\left( - \\lambda_0 x \\right) dx = \\frac{1}{\\lambda_0}. \\end{equation*}\\] And also since the function \\(g(x) = 1/x\\) is continuous in \\(\\mathbb{R}^+\\), we obtain by the Continuous Mapping Theorem that \\(\\hat{\\lambda} \\overset{\\mathcal{P}}{\\mapsto} \\lambda_0\\), which concludes that the MLE for \\(\\lambda_0\\) is a consistent estimator of \\(\\lambda_0\\) in exponential distribution. 3.2.1 Consistency of Extremum Estimators When considering real-life problems, the above approach based on Weak Law of Large Number and Continuous Mapping Theorem is in general not flexible enough. Therefore, we often rely on the results as following. Theorem 3.3 (Consistency of Extremum Estimators) If there is a function \\({Q}_0 (\\boldsymbol{\\theta})\\) such that: C1. \\({Q}_0 (\\boldsymbol{\\theta})\\) is uniquely maximized in \\(\\boldsymbol{\\theta}_0\\), C2. \\(\\boldsymbol{\\Theta}\\) is compact, C3. \\({Q}_0 (\\boldsymbol{\\theta})\\) is continuous in \\(\\boldsymbol{\\theta}\\), C4. \\(\\hat{Q}_n (\\boldsymbol{\\theta})\\) converges uniformly in probability to \\(Q_0 (\\boldsymbol{\\theta})\\), then we have \\(\\hat{\\boldsymbol{\\theta}} \\overset{\\mathcal{P}}{\\mapsto} \\boldsymbol{\\theta}_0\\). Definition 3.4 (Compactness) We say \\(\\boldsymbol{\\Theta}\\) is compact if every open cover of \\(\\boldsymbol{\\Theta}\\) contains a finite subcover. If \\(\\boldsymbol{\\Theta}\\) is compact, then \\(\\boldsymbol{\\Theta}\\) is closed (i.e. containing all its limit points) and bounded (i.e. all its points are within some finite distance of each other). Definition 3.5 (Uniform Convergence in Probability) \\(\\hat{Q}_n (\\boldsymbol{\\theta})\\) is said to converges uniformly in probability to \\(Q_0 (\\boldsymbol{\\theta})\\) if \\(\\underset{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}}{\\text{sup}} \\; |\\hat{Q}_n (\\boldsymbol{\\theta}) - {Q}_0 (\\boldsymbol{\\theta})| \\overset{\\mathcal{P}}{\\mapsto} 0\\). Theorem 3.3 is an important result as it provides a general approach to prove the consistency of the class of extremum estimators. Notice that in this theorem: Condition (C1) is substantive and there are well-known examples where it fails. We will discuss further on how this assumption can (in some cases) be verified in practice. Condition (C2) is also substantive as it requires that there exist some known bounds on the parameters. In practice, this assumption is often neglected although it is in most cases unrealistic to assume it. Condition (C3) and (C4) are often referred to as “standard regularity conditions”. They are typically satisfied. The verification of these conditions will be discussed further later in this section. Proof (Consistency of Extremum Estimators). Let \\(\\mathcal{G}\\) be an \\(\\epsilon\\)-neighborhood centered at \\(\\boldsymbol{\\theta}_0\\) for some \\(\\epsilon &gt; 0\\), i.e. \\(\\mathcal{G} = \\{ \\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}: || \\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0 ||_2 &lt; \\epsilon\\}\\) for some \\(\\epsilon &gt; 0\\). We want to show \\(\\hat{\\boldsymbol{\\theta}} \\overset{\\mathcal{P}}{\\mapsto} \\boldsymbol{\\theta}_0\\), which is equivalent to show \\[\\begin{equation*} \\underset{n \\to \\infty}{\\text{lim}} \\mathbb{P}(||\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0 ||_2 \\geq \\epsilon) = \\underset{n \\to \\infty}{\\text{lim}} \\mathbb{P}(\\hat{\\boldsymbol{\\theta}} \\notin \\mathcal{G}) = 0 \\end{equation*}\\] Define \\(\\gamma = Q_0(\\boldsymbol{\\theta}_0) - \\underset{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta} \\setminus \\mathcal{G}}{\\text{sup}} Q_0({\\boldsymbol{\\theta}}) &gt; 0 \\;\\;\\) by condition C1. So \\(\\hat{\\boldsymbol{\\theta}} \\notin \\mathcal{G}\\) implies that \\(Q_0(\\hat{\\boldsymbol{\\theta}}) \\leq \\underset{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta} \\setminus \\mathcal{G}}{\\text{sup}} Q_0({\\boldsymbol{\\theta}}) = Q_0(\\boldsymbol{\\theta}_0) - \\gamma\\). Let \\(\\mathcal{A} = \\{ Q_0(\\hat{\\boldsymbol{\\theta}}) \\leq Q_0(\\boldsymbol{\\theta}_0) - \\gamma \\}\\). So \\(\\underset{n \\to \\infty}{\\text{lim}}\\mathbb{P}(\\hat{\\boldsymbol{\\theta}} \\notin \\mathcal{G}) \\leq \\underset{n \\to \\infty}{\\text{lim}}\\mathbb{P}(\\mathcal{A})\\). Now we define the following events: \\[\\begin{align*} \\mathcal{B} &amp;= \\{|\\hat{Q_n}(\\hat{\\boldsymbol{\\theta}}) - Q_0(\\hat{\\boldsymbol{\\theta}})| &gt; \\gamma/3 \\}, \\\\ \\mathcal{C} &amp;= \\{|\\hat{Q_n}(\\boldsymbol{\\theta}_0) - Q_0(\\boldsymbol{\\theta}_0)| &gt; \\gamma/3 \\}, \\\\ \\mathcal{D} &amp;= \\{\\underset{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}}{\\text{sup}} |\\hat{Q_n}(\\boldsymbol{\\theta}) - Q_0(\\boldsymbol{\\theta})| &gt;\\gamma/3 \\}. \\end{align*}\\] So we have \\[\\begin{align*} \\mathbb{P} (\\mathcal{A}) &amp;\\leq \\mathbb{P} (\\mathcal{A} \\cup (\\mathcal{B} \\cup \\mathcal{C})) + \\mathbb{P}(\\mathcal{B} \\cup \\mathcal{C}) \\\\ &amp;= \\mathbb{P}(\\mathcal{A} \\cap \\mathcal{B}^c \\cap \\mathcal{C}^c) + \\mathbb{P}(\\mathcal{B} \\cup \\mathcal{C}) \\\\ &amp;= \\mathbb{P}(\\emptyset) + \\mathbb{P}(\\mathcal{B} \\cup \\mathcal{C}) \\\\ &amp;= \\mathbb{P}(\\mathcal{B} \\cup \\mathcal{C}). \\end{align*}\\] Notice that \\(\\mathcal{A} \\cap \\mathcal{B}^c \\cap \\mathcal{C}^c = \\emptyset\\) because if \\(\\mathcal{A}\\), \\(\\mathcal{B}^c\\), and \\(\\mathcal{C}^c\\) hold simultaneously, then \\[\\begin{align*} \\hat{Q_n}(\\hat{\\boldsymbol{\\theta}}) &amp;= \\hat{Q_n}(\\hat{\\boldsymbol{\\theta}}) - Q_0(\\hat{\\boldsymbol{\\theta}}) + Q_0(\\hat{\\boldsymbol{\\theta}}) \\\\ &amp;\\leq |\\hat{Q_n}(\\hat{\\boldsymbol{\\theta}}) - Q_0(\\hat{\\boldsymbol{\\theta}})| + Q_0(\\hat{\\boldsymbol{\\theta}}) \\\\ &amp;\\leq \\gamma/3 + Q_0(\\hat{\\boldsymbol{\\theta}}) \\;\\;\\;\\;\\;\\; \\text{since} \\;\\; \\mathcal{B}^c \\;\\; \\text{holds}\\\\ &amp;\\leq Q_0({\\boldsymbol{\\theta}}_0) - 2\\gamma/3 \\;\\;\\;\\;\\;\\; \\text{since} \\;\\; \\mathcal{A} \\;\\; \\text{holds}\\\\ &amp;\\leq Q_0({\\boldsymbol{\\theta}}_0) - \\hat{Q_n}(\\hat{\\boldsymbol{\\theta}}_0) + \\hat{Q_n}(\\hat{\\boldsymbol{\\theta}}_0) - 2\\gamma/3 \\\\ &amp;\\leq |Q_0({\\boldsymbol{\\theta}}_0) - \\hat{Q_n}({\\boldsymbol{\\theta}}_0)| + \\hat{Q_n}({\\boldsymbol{\\theta}}_0) - 2\\gamma/3 \\\\ &amp;\\leq \\hat{Q_n}({\\boldsymbol{\\theta}}_0) - \\gamma/3 \\;\\;\\;\\;\\;\\; \\text{since} \\;\\; \\mathcal{C}^c \\;\\; \\text{holds}\\\\ &amp;&lt; \\hat{Q_n}({\\boldsymbol{\\theta}}_0) \\end{align*}\\] which contradicts that \\(\\hat{\\boldsymbol{\\theta}} = \\underset{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}}{\\text{argmax}} \\hat{Q_n}({\\boldsymbol{\\theta}})\\). So \\[\\begin{align*} \\mathbb{P}(\\mathcal{A}) &amp;= \\mathbb{P}(\\mathcal{B} \\cup \\mathcal{C}) \\\\ &amp;= \\mathbb{P}(|\\hat{Q_n}(\\hat{\\boldsymbol{\\theta}}) - Q_0(\\hat{\\boldsymbol{\\theta}})| &gt; \\gamma/3 \\;\\; \\text{or} \\;\\; |\\hat{Q_n}(\\boldsymbol{\\theta}_0) - Q_0(\\boldsymbol{\\theta}_0)| &gt; \\gamma/3) \\\\ &amp;= \\mathbb{P} (\\underset{\\boldsymbol{\\theta} \\in \\{\\hat{\\boldsymbol{\\theta}}, \\boldsymbol{\\theta}_0\\} }{\\text{sup}} |\\hat{Q_n}(\\boldsymbol{\\theta}) - Q_0(\\boldsymbol{\\theta})| &gt; \\gamma/3) \\\\ &amp; \\leq \\mathbb{P} (\\underset{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta} }{\\text{sup}} |\\hat{Q_n}(\\boldsymbol{\\theta}) - Q_0(\\boldsymbol{\\theta})| &gt; \\gamma/3) \\\\ &amp;= \\mathbb{P}(\\mathcal{D}). \\end{align*}\\] So by condition C4, we have \\[\\begin{equation*} \\underset{n \\to \\infty}{\\text{lim}}\\mathbb{P}(\\hat{\\boldsymbol{\\theta}} \\notin \\mathcal{G}) \\leq \\underset{n \\to \\infty}{\\text{lim}} \\mathbb{P}(\\mathcal{A}) \\leq \\underset{n \\to \\infty}{\\text{lim}}\\mathbb{P}(\\mathcal{D}) = 0. \\end{equation*}\\] Therefore, \\(\\underset{n \\to \\infty}{\\text{lim}}\\mathbb{P}(\\hat{\\boldsymbol{\\theta}} \\notin \\mathcal{G}) = 0\\). 3.2.2 Verification of Condition C1 In general, the verification of Condition C1 is difficult and is often assumed in the statistical literature. Here we present two results based on Newey and McFadden (1994) and Komunjer (2012) respectively, which allow us to verify the condition C1 for GMM-type estimators. A discussion on the verification of this condition for other estimators can for example be found in Chapter 7 of Baltagi (2008). Lemma 3.1 (GMM Identification) If \\(\\boldsymbol{W} &gt; 0\\) (i.e. positive definite), where \\(\\boldsymbol{W}\\) is such that \\(\\widehat{\\boldsymbol{W}} \\overset{\\mathcal{P}}{\\mapsto} \\boldsymbol{W}\\), and for \\(\\boldsymbol{g}_0(\\boldsymbol{\\theta}) = \\mathbb{E}[\\boldsymbol{g}(z|\\boldsymbol{\\theta})]\\), we have \\(\\boldsymbol{g}_0(\\boldsymbol{\\theta}_0)=0\\) and \\(\\boldsymbol{g}_0(\\boldsymbol{\\theta}) \\neq 0\\) if \\(\\boldsymbol{\\theta} \\neq \\boldsymbol{\\theta}_0\\), then \\(Q_0(\\boldsymbol{\\theta}) = -\\boldsymbol{g}_0(\\boldsymbol{\\theta})^T \\boldsymbol{W} \\boldsymbol{g}_0(\\boldsymbol{\\theta})\\) has a unique maximum at \\(\\boldsymbol{\\theta}_0\\). Proof (GMM Identification). Since \\(\\boldsymbol{W} &gt;0\\) and there exists a unique \\(\\boldsymbol{\\theta}_0 \\in \\boldsymbol{\\Theta}\\) such that \\(\\boldsymbol{g}_0(\\boldsymbol{\\theta}) = 0\\), we can say that for all \\(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}\\), \\[\\begin{equation*} Q_0(\\boldsymbol{\\theta}) = -\\boldsymbol{g}_0(\\boldsymbol{\\theta})^T \\boldsymbol{W} \\boldsymbol{g}_0(\\boldsymbol{\\theta}) \\leq -\\boldsymbol{g}_0(\\boldsymbol{\\theta}_0) \\boldsymbol{W} \\boldsymbol{g}_0(\\boldsymbol{\\theta}_0) = Q_0(\\boldsymbol{\\theta_0}). \\end{equation*}\\] Notice that if we write a GMM estimator as in Example 3.3, then the condition \\(\\boldsymbol{g}_0(\\boldsymbol{\\theta}) = 0\\) if and only if \\(\\boldsymbol{\\theta} = \\boldsymbol{\\theta}_0\\) can be replaced by \\(\\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) = \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}_0)\\) if and only if \\(\\boldsymbol{\\theta} = \\boldsymbol{\\theta}_0\\). In order to verify the condition in Lemma 3.1 that \\(\\boldsymbol{g}_0(\\boldsymbol{\\theta}) = 0\\) if and only if \\(\\boldsymbol{\\theta} = \\boldsymbol{\\theta}_0\\) (or alternatively \\(\\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) = \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}_0)\\) if and only if \\(\\boldsymbol{\\theta} = \\boldsymbol{\\theta}_0\\)), the following theorem based on Komunjer (2012) provides us a way. Theorem 3.4 (Homomorphism) Let \\(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta} \\subset \\mathbb{R}^p\\). Let \\(\\boldsymbol{g}^*(\\boldsymbol{\\theta})\\) denote a subset of \\(p\\) elements of \\(\\boldsymbol{g}_0 (\\boldsymbol{\\theta}) \\in \\mathbb{R}^q, \\; q \\geq p\\) such that: \\(\\boldsymbol{g}^*(\\boldsymbol{\\theta})\\) is in \\(\\mathcal{C}^2\\) (i.e. \\(\\boldsymbol{g}^*(\\boldsymbol{\\theta})\\) can be differentiated twice); For every \\(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}\\), \\(J(\\boldsymbol{\\theta})\\) is non-negative (or alternatively non-positive), where \\(J(\\boldsymbol{\\theta}) \\equiv \\text{det} (\\frac{\\partial}{\\partial \\boldsymbol{\\theta}^T}\\, \\boldsymbol{g}^*(\\boldsymbol{\\theta}) )\\); \\(||\\boldsymbol{g}^*(\\boldsymbol{\\theta})|| \\to \\infty\\) whenever \\(|| \\boldsymbol{\\theta} || \\to \\infty\\); For every \\(s \\in \\mathbb{R}^p\\) the equation \\(\\boldsymbol{g}^*(\\boldsymbol{\\theta}) = s\\) has countably many (possibly zero) solutions in \\(\\boldsymbol{\\Theta}\\); then \\(\\boldsymbol{g}^*(\\boldsymbol{\\theta})\\) is a homeomorphism (i.e. \\(\\boldsymbol{g}^*(\\boldsymbol{\\theta})\\) is continuous and one-to-one). A direct consequence of Lemma 3.1 and Theorem 3.4 is that any GMM estimator with \\(\\boldsymbol{W} &gt; 0\\) satisfying the conditions of Theorem 3.4 verifies Condition C1 of Theorem 3.3. If one can show that \\(\\boldsymbol{g}^c(\\boldsymbol{\\theta})\\) is in \\(\\mathcal{C}\\) (where \\(\\boldsymbol{g}^c(\\boldsymbol{\\theta})\\) denotes the element of \\(\\boldsymbol{g}_0(\\boldsymbol{\\theta})\\) that are not in \\(\\boldsymbol{g}^*(\\boldsymbol{\\theta})\\)) then Condition C3 of Theorem 3.3 is also verified. When considering a GMM estimator of the form used in Example 3.3, one can simply verify the conditions of Theorem 3.3 with \\(\\boldsymbol{g}(\\boldsymbol{\\theta}) = \\boldsymbol{\\gamma} (\\boldsymbol{\\theta})\\). For Theorem 3.4, in Komunjer (2012) it is actually assumed that \\(\\boldsymbol{\\theta} \\in \\mathbb{R}^p\\) while we assume that \\(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}\\). We use this simplification to avoid an overly technical treatment of this topic. In fact, we assume here that there exists a one-to-one function \\(h(\\cdot)\\) such that \\(h \\, : \\,\\mathbb{R}^p \\mapsto \\boldsymbol{\\Theta}\\). This condition is typically verified in practice. Example 3.5 (An Example to Verify Condition C1) In this example we revisit Example 3.3. Let us say that \\(\\widehat{\\boldsymbol{W}}\\) is such that \\(\\widehat{\\boldsymbol{W}} \\overset{\\mathcal{P}}{\\mapsto} \\boldsymbol{W} &gt; 0\\). We have shown that \\[\\begin{equation*} \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) = \\begin{bmatrix} \\mu\\\\ \\mu^2 + \\sigma^2\\\\ \\mu^3 + 3 \\mu \\sigma^2 \\end{bmatrix}. \\end{equation*}\\] So we define that \\[\\begin{equation*} \\boldsymbol{g}^*(\\boldsymbol{\\theta}) = \\begin{bmatrix} \\mu\\\\ \\mu^2 + \\sigma^2\\\\ \\end{bmatrix} \\;\\;\\;\\;\\; \\text{and} \\;\\;\\;\\;\\; g^c(\\boldsymbol{\\theta}) = \\begin{bmatrix} \\mu^3 + 3 \\mu \\sigma^2\\\\ \\end{bmatrix}. \\end{equation*}\\] Since the elements of \\(\\boldsymbol{g}^*(\\boldsymbol{\\theta})\\) are polynomial in \\(\\boldsymbol{\\Theta}\\), the condition \\(\\boldsymbol{g}^*(\\boldsymbol{\\theta}) \\in \\mathcal{C}^2\\) is trivially satisfied. Next, since \\[\\begin{equation*} \\frac{\\partial}{\\partial \\boldsymbol{\\theta}^T}\\, \\boldsymbol{g}^*(\\boldsymbol{\\theta}) = \\begin{bmatrix} 1 &amp; 0\\\\ 2\\mu &amp; 1 \\end{bmatrix}, \\end{equation*}\\] we have \\(J(\\boldsymbol{\\theta}) \\equiv \\text{det} (\\frac{\\partial}{\\partial \\boldsymbol{\\theta}^T}\\, \\boldsymbol{g}^*(\\boldsymbol{\\theta}) ) = 1\\). Finally, the last two conditions of Theorem 3.4 are trivially satisfied since \\(||\\boldsymbol{g}^*(\\boldsymbol{\\theta})||\\) can only diverge if \\(|| \\boldsymbol{\\theta} ||\\) diverges and since \\(\\boldsymbol{g}^*(\\boldsymbol{\\theta}) = s\\) has (one) countably solutions in \\(\\boldsymbol{\\Theta}\\). Therefore, it follows from Lemma 3.1 and Theorem 3.4 that the function \\(Q_0(\\boldsymbol{\\theta}) = - || \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}_0) - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) ||^2_{\\boldsymbol{W}}\\) is uniquely maximized in \\(\\boldsymbol{\\theta}_0\\). 3.2.3 Verification of Condition C4 In general, the verification of Condition C4 requires pointwise convergence and Lipschitz continuity as specified in the following theorem, which is a slightly adapted version of the Arzela-Ascoli Theorem. Theorem 3.5 (modified Arzela-Ascoli Theorem) Suppose \\(\\boldsymbol{\\Theta}\\) is compact. If for every \\(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}\\), we have \\(\\hat{Q}_n (\\boldsymbol{\\theta}) \\overset{\\mathcal{P}}{\\mapsto} Q_0 (\\boldsymbol{\\theta})\\), \\(\\hat{Q}_n (\\boldsymbol{\\theta})\\) is almost surely Lipschitz continuous, i.e. \\(\\underset{\\boldsymbol{\\theta}_1, \\boldsymbol{\\theta}_2 \\in \\boldsymbol{\\Theta}}{\\text{sup}} |\\hat{Q}_n (\\boldsymbol{\\theta}_1) - \\hat{Q}_n (\\boldsymbol{\\theta}_2)| \\leq H ||\\boldsymbol{\\theta}_1 - \\boldsymbol{\\theta}_2||\\) where \\(H\\) is bounded almost surely, then \\(\\hat{Q}_n (\\boldsymbol{\\theta})\\) converges uniformly in probability to \\(Q_0 (\\boldsymbol{\\theta})\\). Here we will not discuss how to prove that \\(\\hat{Q}_n (\\boldsymbol{\\theta})\\) is almost surely Lipschitz continuous and assume it for simplicity (more discussion on this topic can for example be found in Newey and McFadden (1994)). Nevertheless, it worth mentioning that this condition is almost always satisfied in practice and is therefore reasonable to assume for simplicity. Example 3.6 (An Example to Verify Condition C4) In this example we revisit Example 3.3. Since we have \\(Z_i \\overset{iid}{\\sim} \\mathcal{N}(\\mu_0, \\sigma^2_0)\\), we let \\[\\begin{equation*} \\boldsymbol{X}_i \\equiv \\begin{bmatrix} Z_i\\\\ Z_i^2\\\\ Z_i^3 \\end{bmatrix}. \\end{equation*}\\] So by the Weak Law of Large Number (Theorem 3.1), we have \\[\\begin{equation*} \\hat{\\boldsymbol{\\gamma}} = \\frac{1}{n} \\sum_{i = 1}^n \\boldsymbol{X}_i \\overset{\\mathcal{P}}{\\mapsto} \\mathbb{E}[\\boldsymbol{X}_i] = \\boldsymbol{\\gamma}({\\boldsymbol{\\theta}_0}) \\equiv \\boldsymbol{\\gamma}_0. \\end{equation*}\\] Define \\[\\begin{equation*} \\hat{Q}_n (\\boldsymbol{\\theta}) = -||\\hat{\\boldsymbol{\\gamma}}- \\boldsymbol{\\gamma} (\\boldsymbol{\\theta}) ||_{\\widehat{\\boldsymbol{W}}}^2 \\end{equation*}\\] where \\(\\widehat{\\boldsymbol{W}} \\overset{\\mathcal{P}}{\\mapsto} \\boldsymbol{W}\\), and also define \\[\\begin{equation*} Q_0(\\boldsymbol{\\theta}) = -||\\boldsymbol{\\gamma}(\\boldsymbol{\\theta}_0) - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta})||_\\boldsymbol{W}^2. \\end{equation*}\\] In order to show that \\(\\hat{Q}_n (\\boldsymbol{\\theta}) \\overset{\\mathcal{P}}{\\mapsto} Q_0(\\boldsymbol{\\theta})\\) for all \\(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}\\), we want to show that \\(|\\hat{Q}_n (\\boldsymbol{\\theta}) - Q_0(\\boldsymbol{\\theta})| \\overset{\\mathcal{P}}{\\mapsto} 0\\). \\[\\begin{equation*} \\begin{aligned} \\hat{Q}_n (\\boldsymbol{\\theta}) - {Q}_0 (\\boldsymbol{\\theta}) &amp;\\leq |\\hat{Q}_n (\\boldsymbol{\\theta}) - {Q}_0 (\\boldsymbol{\\theta})| = \\left|\\underbrace{ ||\\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma} (\\boldsymbol{\\theta}) ||_{\\widehat{\\boldsymbol{W}}}^2 - ||\\gamma_0 - \\boldsymbol{\\gamma} (\\boldsymbol{\\theta}) ||_\\boldsymbol{W}^2}_\\boldsymbol{A}\\right|. \\end{aligned} \\end{equation*}\\] Without loss of generality, we assume that \\(\\boldsymbol{W}^* = \\widehat{\\boldsymbol{W}} - \\boldsymbol{W}\\) and that \\(\\boldsymbol{W}\\) is symmetric. \\[\\begin{align*} \\boldsymbol{A} &amp;= \\hat{\\boldsymbol{\\gamma}}^T \\widehat{\\boldsymbol{W}}\\hat{\\boldsymbol{\\gamma}} + \\boldsymbol{\\gamma}(\\boldsymbol{\\theta})^T \\widehat{\\boldsymbol{W}} \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) - 2\\hat{\\boldsymbol{\\gamma}}^T\\widehat{\\boldsymbol{W}}\\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) - \\boldsymbol{\\gamma}_0^T \\boldsymbol{W} \\boldsymbol{\\gamma}_0 - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta})^T \\boldsymbol{W} \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) + 2\\boldsymbol{\\gamma}_0^T \\boldsymbol{W} \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) \\\\ &amp;= ||\\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma}_0||_{\\widehat{\\boldsymbol{W}}}^2 - \\boldsymbol{\\gamma}_0^T \\widehat{\\boldsymbol{W}} \\boldsymbol{\\gamma}_0 + 2\\widehat{\\boldsymbol{\\gamma}}^T \\widehat{\\boldsymbol{W}} \\boldsymbol{\\gamma}_0 \\\\ &amp;+ \\boldsymbol{\\gamma}(\\boldsymbol{\\theta})^T \\widehat{\\boldsymbol{W}} \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) - 2\\hat{\\boldsymbol{\\gamma}}^T\\widehat{\\boldsymbol{W}}\\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) - \\boldsymbol{\\gamma}_0^T \\boldsymbol{W} \\boldsymbol{\\gamma}_0 - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta})^T \\boldsymbol{W} \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) + 2\\boldsymbol{\\gamma}_0^T \\boldsymbol{W} \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) \\\\ &amp;= ||\\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma}_0||_{\\widehat{\\boldsymbol{W}}}^2 + ||\\boldsymbol{\\gamma}_0 - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta})||_{\\boldsymbol{W}^*}^2 \\\\ &amp;- 2\\boldsymbol{\\gamma}_0^T\\widehat{\\boldsymbol{W}}\\boldsymbol{\\gamma}_0 - 2\\hat{\\boldsymbol{\\gamma}}^T \\widehat{\\boldsymbol{W}} \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) + 2\\boldsymbol{\\gamma}_0^T \\widehat{\\boldsymbol{W}} \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) + 2\\hat{\\boldsymbol{\\gamma}}^T \\widehat{\\boldsymbol{W}} \\boldsymbol{\\gamma}_0 \\\\ &amp;= ||\\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma}_0||_{\\widehat{\\boldsymbol{W}}}^2 + ||\\boldsymbol{\\gamma}_0 - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta})||_{\\boldsymbol{W}^*}^2 + 2(\\boldsymbol{\\gamma}_0 - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}))^T \\widehat{\\boldsymbol{W}} (\\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma}_0). \\end{align*}\\] So by triangular inequality, we have \\[\\begin{align*} |\\hat{Q}_n (\\boldsymbol{\\theta}) - {Q}_0 (\\boldsymbol{\\theta})| &amp;\\leq \\left| \\underbrace{\\|\\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}_0) \\|_{\\widehat{\\boldsymbol{W}}}^2}_{\\equiv a_1} \\right| + \\left| \\underbrace{\\|\\boldsymbol{\\gamma}(\\boldsymbol{\\theta}_0) - \\boldsymbol{\\gamma} (\\boldsymbol{\\theta}) \\|_{{\\boldsymbol{W}}^*}^2}_{\\equiv a_2} \\right| \\\\ &amp;+ \\left| \\underbrace{2 \\left( \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}_0) - \\boldsymbol{\\gamma} (\\boldsymbol{\\theta})\\right)^T \\widehat{\\boldsymbol{W}} \\left( \\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}_0)\\right)}_{\\equiv a_3}\\right|. \\end{align*}\\] Before continuing, suppose that \\(\\boldsymbol{x}\\) is a vector and \\(\\boldsymbol{W}\\) the above defined matrix, we have that \\(\\boldsymbol{x}^T \\boldsymbol{W} \\boldsymbol{x} \\leq \\lambda_1 ||\\boldsymbol{x}||^2\\), where \\(\\lambda_1\\) is the largest eigenvalue of \\(\\boldsymbol{W}\\). Furthermore, let us also define the Frobenius norm of a matrix as \\(||\\boldsymbol{W}|| = (\\sum_i^N \\sum_j^J w_{i,j}^2)^{\\frac{1}{2}}\\) and \\(||\\boldsymbol{W}||^2 = \\sigma_{\\text{max}} \\leq ||\\boldsymbol{W}||\\), where \\(\\sigma_{\\text{max}}\\) is the largest singular value of \\(||\\boldsymbol{W}||\\). Using those properties, we can investigate the terms in the above equation. Considering \\(a_1\\) , we have \\[\\begin{align*} a_1 &amp;\\leq ||\\boldsymbol{\\gamma}_0 - \\boldsymbol{\\gamma} (\\boldsymbol{\\theta}) ||^2 \\lambda_1 \\leq ||\\boldsymbol{\\gamma}_0 - \\boldsymbol{\\gamma} (\\boldsymbol{\\theta}) ||^2 \\|\\boldsymbol{W}^*|| \\\\ &amp;= \\sum^3_{i = 1} \\left(\\boldsymbol{\\gamma}_{0_i} - \\boldsymbol{\\gamma}_i (\\boldsymbol{\\theta})\\right)^2 \\sqrt{\\sum^3_{i = 1} \\sum^3_{j = 1} \\left(\\hat{w}_{i,j} - w_{i,j}\\right)^2} \\\\ &amp;\\leq 3 \\underset{i}{\\text{max}} \\left(\\boldsymbol{\\gamma}_{0_i} - \\boldsymbol{\\gamma}_i (\\boldsymbol{\\theta})\\right)^2 \\cdot 3 \\underset{i}{\\text{max}} \\sqrt{\\left(\\hat{w}_{i,j} - w_{i,j}\\right)^2} \\end{align*}\\] Since \\(\\underset{i}{\\text{max}} \\left(\\boldsymbol{\\gamma}_{0_i} - \\boldsymbol{\\gamma}_i (\\boldsymbol{\\theta})\\right)^2\\) is bounded by conditions C1 to C3, and \\(\\underset{i}{\\text{max}} \\sqrt{\\left(\\hat{w}_{i,j} - w_{i,j}\\right)^2} \\overset{\\mathcal{P}}{\\mapsto} 0\\) by \\(\\widehat{\\boldsymbol{W}} \\overset{\\mathcal{P}}{\\mapsto} \\boldsymbol{W}\\), we show that \\(a_1 \\overset{\\mathcal{P}}{\\mapsto} 0\\). Similarly, we can also show that \\(a_2 \\overset{\\mathcal{P}}{\\mapsto} 0\\) using the fact that he sample moments converge to the population ones. Finally, considering the previous results on \\(a_1\\) and \\(a_2\\), we can see that the last term \\(a_3\\) also tends to 0. Therefore, we can say that for all \\(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}\\), \\[\\begin{equation*} |\\hat{Q}_n (\\boldsymbol{\\theta}) - Q_0(\\boldsymbol{\\theta})| \\overset{\\mathcal{P}}{\\mapsto} 0. \\end{equation*}\\] In general, showing that \\(\\hat{Q}_n (\\boldsymbol{\\theta}) \\overset{\\mathcal{P}}{\\mapsto} Q_0(\\boldsymbol{\\theta})\\) for all \\(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}\\) for every \\(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}\\) is generally done using the Weak Law of Large Number (i.e. Theorem 3.1). However, when \\(X_i\\) which are not iid random variables, Theorem 3.1) cannot be applied. The following theorem (taken from Proposition 7.5 of Hamilton (1994)) generalizes this result for (weak) stationary processes with absolutely summable covariance structure. Theorem 3.6 (Weak Law of Large Number for Dependent Process) Suppose \\((X_t)\\) is a (weak) stationary process with absolutely summable autocovariance structure, then \\[\\begin{equation*} \\bar{X}_T \\overset{\\mathcal{P}}{\\mapsto} \\mathbb{E}[X_t]. \\end{equation*}\\] Proof (Weak Law of Large Number for Dependent Process). Since \\(\\bar{X}_n - \\mu\\) has mean zero, its variance is \\[\\begin{equation*} \\mathbb{E}[(\\bar{X}_n - \\mu)^2] = 1/n\\sum_{h=-\\infty}^{\\infty} \\gamma(h) \\leq 1/n\\sum_{h=-\\infty}^{\\infty} |\\gamma(h)| \\leq C/n. \\end{equation*}\\] By Chebychev’s inequality, for all \\(\\epsilon &gt; 0\\), \\[\\begin{equation*} \\mathbb{P} \\left(| \\bar{X}_n - \\mu | \\geq \\epsilon \\right) \\leq \\frac{\\mathbb{E}[(\\bar{X}_n - \\mu)^2]}{\\epsilon^2} \\leq \\frac{C}{\\epsilon^2n}, \\end{equation*}\\] so that \\[\\begin{equation*} \\underset{n \\to \\infty}{\\text{lim}} \\mathbb{P} \\left(| \\bar{X}_n - \\mu | \\geq \\epsilon \\right) \\to 0, \\end{equation*}\\] which concludes the proof. Example 3.7 (Consistency of Sample Mean) Consider a stationary AR1 process and suppose we want to study whether its sample mean converges in probability to its expected value. This time we consider a non-zero mean AR1, i.e. \\[\\begin{equation*} \\left(X_t - \\mu\\right) = \\phi \\left(X_{t-1} - \\mu\\right) + Z_t, \\end{equation*}\\] where \\(\\mu = \\mathbb{E}[X_t]\\), \\(Z_t \\overset{iid}{\\sim} \\mathcal{N} (0, \\nu^2)\\) and \\(\\nu^2 &lt; \\infty\\). This process can also be written as a linear process: \\[\\begin{equation*} X_i - \\mu = \\sum_{k=0}^{\\infty}\\phi^k Z_{i-k}. \\end{equation*}\\] Since the process is stationary for \\(|\\phi| &lt; 1\\), we have \\[\\begin{equation*} \\gamma_h = \\frac{\\nu^2 \\phi^{|h|}}{1-\\phi^2}, \\end{equation*}\\] for \\(h \\in \\mathbb{Z}\\). Then we have \\[\\begin{equation*} \\sum_{h = -\\infty}^{\\infty} |\\gamma_k| = \\sum_{h = -\\infty}^{\\infty} \\frac{\\nu^2 |\\phi|^{|h|}}{1-\\phi^2} &lt; 2 \\lim_{n\\to\\infty}\\frac{\\nu^2 (1-|\\phi|^{n+1})}{(1-\\phi^2)(1-|\\phi|)} = \\frac{2\\nu^2 }{(1-\\phi^2)(1-|\\phi|)} &lt; \\infty, \\end{equation*}\\] implying that the process has an absolutely summable covariance structure. Therefore, by applying Theorem 3.6, we can verify that \\[\\begin{equation*} \\bar{X}_T = \\frac{1}{T} \\sum_{t = 1}^T \\, X_t \\overset{\\mathcal{P}}{\\mapsto} \\mathbb{E}[X_t] = \\mu. \\end{equation*}\\] 3.2.4 Consistency of Sample AutoCovariance and AutoCorrelation Functions In Chapter 2, we have defined the sample autocovariance andautocorrelation functions. In the following, we will show that these estimators are both consistent. Corollary 3.1 (Consistency of Sample AutoCovariance and AutoCorrelation Functions) Let \\((X_t)\\) be such that \\((X_t)\\) is weakly stationary and \\((X_t^2)\\) has an absolutely summable covariance structure, then for all \\(|h| &lt; \\infty\\), we have \\[\\begin{align*} \\hat{\\gamma}(h) &amp;\\overset{\\mathcal{P}}{\\mapsto} \\gamma(h),\\\\ \\hat{\\rho}(h) &amp;\\overset{\\mathcal{P}}{\\mapsto} \\rho(h). \\end{align*}\\] Proof (Consistency of Sample AutoCovariance and AutoCorrelation Functions). \\((X_t^2)\\) has an absolutely summable covariance structure implies that both \\((X_t)\\) and \\((X_tX_{t-h})\\) for all \\(h \\in \\mathbb{Z}\\) have absolutely summable covariance structures. Under the conditions that \\((X_t)\\) is weakly stationary and has absolutely summable covariance structure, we have \\(\\bar{X}_T \\overset{\\mathcal{P}}{\\mapsto} \\mu\\). Let \\[\\begin{equation*} \\tilde{\\gamma}(h) = \\frac{1}{T} \\sum_{t = h+1}^{T} \\left(X_{t} - \\mu\\right) \\left(X_{t-h} - \\mu\\right). \\end{equation*}\\] Since \\((\\left(X_{t} - \\mu\\right) \\left(X_{t-h} - \\mu\\right))\\) is stationary and has absolutely summable covariance structure, we have \\(\\tilde{\\gamma}(h) \\overset{\\mathcal{P}}{\\mapsto} \\gamma(h)\\). We can also show that \\(\\sqrt{T} \\left(\\tilde{\\gamma}(h) - \\hat{\\gamma}(h) \\right) = o_p(1)\\). Therefore, we obtain \\[\\begin{equation*} \\hat{\\gamma}(h) \\overset{\\mathcal{P}}{\\mapsto} \\gamma(h). \\end{equation*}\\] Similarly, we can show that \\[\\begin{equation*} \\left( \\hat{\\gamma}(0), \\hat{\\gamma}(h) \\right)^T \\overset{\\mathcal{P}}{\\mapsto} \\left( \\gamma(0), \\gamma(h) \\right)^T. \\end{equation*}\\] Then by Theorem 3.2, we have \\[\\begin{equation*} \\hat{\\rho}(h) \\overset{\\mathcal{P}}{\\mapsto} \\rho(h), \\end{equation*}\\] which concludes the proof. 3.3 Asymptotic Normality The Central Limit Theorem (CLT) takes one step further than the Law of Large Number. It identifies the limiting distribution of the (properly scaled) sum of random variables as the normal distribution, which allows us to do the statistical inference (confidence interval and hypothesis testing). The scale will tell us how fast this approximation converges to the normal distribution. 3.3.1 CLT for iid Random Variables Theorem 3.7 (CLT for iid sequences) Suppose \\(X_i\\) are iid random variables with \\(\\mathbb{E}[X_i] = \\mu\\) and \\(\\text{Var}(X_i) = \\sigma^2 &lt; \\infty\\). Let \\(\\bar{X}_n = \\frac{1}{n} \\sum_{i = 1}^n X_i\\), then \\(\\sqrt{n}\\left(\\bar{X}_n - \\mu\\right) \\overset{\\mathcal{D}}{\\mapsto} \\mathcal{N}(0, \\sigma^2)\\). Theorem 3.8 (CLT for iid multivariate sequences) Suppose \\(\\boldsymbol{X}_i\\) are iid random vectors with \\(\\mathbb{E}[\\boldsymbol{X}_i] = \\boldsymbol{\\mu} \\in \\mathbb{R}^d\\) and \\(\\boldsymbol{\\text{Cov}}(\\boldsymbol{X}_i) = \\boldsymbol{\\Sigma} \\in \\mathbb{R}^{d \\times d}\\). Then, we have \\(\\sqrt{n}\\left(\\bar{\\boldsymbol{X}}_n - \\boldsymbol{\\mu}\\right) \\overset{\\mathcal{D}}{\\mapsto} \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{\\Sigma})\\). Theorem 3.9 (Slutsky’s Theorem) Let \\(X_n\\), \\(Y_n\\) and \\(X\\) be random variables. If \\(X_n \\overset{\\mathcal{D}}{\\mapsto} X\\) and \\(Y_n \\overset{\\mathcal{P}}{\\mapsto} c\\) for some constant \\(c\\), then \\(X_n + Y_n \\overset{\\mathcal{D}}{\\mapsto} X+c\\), \\(X_n Y_n \\overset{\\mathcal{D}}{\\mapsto} cX\\), \\(X_n / Y_n \\overset{\\mathcal{D}}{\\mapsto} X/c\\) if \\(c \\neq 0\\). General results on the asymptotic normality of extremum estimators can be found in Newey and McFadden (1994). In this section, we will only restrict our attention to a simple example of GMM estimators as in Example 3.3. We will see that the asymptotic normality of extremum estimators is implied by combining the following results and techniques: Consistency of \\(\\hat{\\boldsymbol{\\theta}}\\), Central Limit Theorem (see Theorem 3.8), Slutsky’s Theorem (see Theorem 3.9), Taylor expansions. Example 3.8 (An Example to Prove Asymptotic Normality of Extremum Estimators) In this example we revisit Example 3.3. So we have \\[\\begin{equation*} \\hat{\\boldsymbol{\\theta}} = \\underset{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}}{\\text{argmax}} \\hat{Q}_n(\\boldsymbol{\\theta}) = \\underset{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}}{\\text{argmax}} -||\\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) ||_\\widehat{\\boldsymbol{W}}^2 \\end{equation*}\\] where \\[\\begin{equation*} \\hat{\\boldsymbol{\\gamma}} = \\frac{1}{n} \\sum_{i = 1}^n \\begin{bmatrix} Z_i\\\\ Z_i^2\\\\ Z_i^3\\\\ \\end{bmatrix}, \\;\\;\\;\\; \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) = \\begin{bmatrix} \\mu\\\\ \\mu^2 + \\sigma^2\\\\ \\mu^3 + 3 \\mu \\sigma^2 \\end{bmatrix}. \\end{equation*}\\] So by CLT (Theorem 3.8), we have \\[\\begin{equation*} \\sqrt{n}\\left(\\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}_0)\\right) = \\frac{1}{\\sqrt{n}} \\sum_{i = 1}^n \\begin{bmatrix} Z_i - \\mu_0\\\\ Z_i^2 - \\mu_0^2 - \\sigma_0^2\\\\ Z_i^3 - \\mu_0^3 - 3 \\mu_0 \\sigma_0^2\\\\ \\end{bmatrix} \\overset{\\mathcal{D}}{\\mapsto} \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{\\Sigma}), \\end{equation*}\\] where \\(\\boldsymbol{\\Sigma} = \\text{Cov}\\left( \\begin{bmatrix} Z_i &amp; Z_i^2&amp; Z_i^3\\\\ \\end{bmatrix}^T \\right)\\). Since \\(\\hat{\\boldsymbol{\\theta}}\\) maximizes \\(\\hat{Q}_n(\\boldsymbol{\\theta})\\), we have \\[\\begin{equation*} \\frac{\\partial \\hat{Q}_n(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} \\bigg\\rvert_{\\boldsymbol{\\theta} = \\hat{\\boldsymbol{\\theta}}} = \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\left(\\left( \\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) \\right) \\right)^T \\widehat{\\boldsymbol{W}} \\left( \\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) \\right) \\bigg\\rvert_{\\boldsymbol{\\theta} = \\hat{\\boldsymbol{\\theta}}} = \\boldsymbol{0}. \\end{equation*}\\] Using Taylor expansion for \\(\\boldsymbol{\\gamma}(\\hat{\\boldsymbol{\\theta}})\\) around the true \\(\\boldsymbol{\\theta}_0\\), we obtain \\[\\begin{equation*} \\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma}(\\hat{\\boldsymbol{\\theta}}) = \\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}_0) + \\frac{\\partial \\left( \\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) \\right)}{\\partial \\boldsymbol{\\theta}}\\bigg\\rvert_{\\boldsymbol{\\theta} = \\boldsymbol{\\theta}_0} \\left( \\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0 \\right) + o_p(1). \\end{equation*}\\] Under certain regularity conditions, we have (using Theorem 3.2) that \\[\\begin{equation*} \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\left( \\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) \\right)\\bigg\\rvert_{\\boldsymbol{\\theta} = \\hat{\\boldsymbol{\\theta}}} = - \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) \\bigg\\rvert_{\\boldsymbol{\\theta} = \\hat{\\boldsymbol{\\theta}}} \\overset{\\mathcal{P}}{\\mapsto} - \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) \\bigg\\rvert_{\\boldsymbol{\\theta} = \\boldsymbol{\\theta}_0}. \\end{equation*}\\] So under certain regularity conditions, using the above equations and Slutsky’s Theorem, we can obtain that \\[\\begin{equation*} \\sqrt{n}\\left( \\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0 \\right) \\overset{\\mathcal{D}}{\\mapsto} \\mathcal{N}\\left(\\boldsymbol{0}, \\boldsymbol{D}^T \\boldsymbol{\\Sigma} \\boldsymbol{D}\\right), \\end{equation*}\\] where \\[\\begin{equation*} \\boldsymbol{D} = \\left[ ||\\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\left( \\boldsymbol{\\gamma}(\\boldsymbol{\\theta})\\right)||_{\\boldsymbol{W}}\\rvert_{\\boldsymbol{\\theta} = \\boldsymbol{\\theta}_0} \\right]^{-1} \\left(\\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\boldsymbol{\\gamma}(\\boldsymbol{\\theta})\\bigg\\rvert_{\\boldsymbol{\\theta} = \\boldsymbol{\\theta}_0} \\right)^T \\boldsymbol{W} \\end{equation*}\\] due to the fact that \\[\\begin{equation*} \\begin{aligned} &amp;\\sqrt{n}\\left( \\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0 \\right) = \\overbrace{-\\left[ \\left(\\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\left( \\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) \\right)\\bigg\\rvert_{\\boldsymbol{\\theta} = \\hat{\\boldsymbol{\\theta}}} \\right)^T \\widehat{\\boldsymbol{W}} \\left( \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\left( \\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) \\right)\\bigg\\rvert_{\\boldsymbol{\\theta} = \\hat{\\boldsymbol{\\theta}}} \\right) \\right]^{-1}}^{ \\overset{p}{\\to} \\left[ ||\\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\left( \\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta})\\right)||_{\\boldsymbol{W}}\\rvert_{\\boldsymbol{\\theta} = \\boldsymbol{\\theta}_0} \\right]^{-1}}\\\\ &amp;\\underbrace{\\left(\\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\left( \\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}) \\right)\\bigg\\rvert_{\\boldsymbol{\\theta} = \\hat{\\boldsymbol{\\theta}}} \\right)^T \\widehat{\\boldsymbol{W}}}_{\\overset{p}{\\to} \\left(\\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\left( \\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta})\\right)\\rvert_{\\boldsymbol{\\theta} = \\boldsymbol{\\theta}_0} \\right)^T \\boldsymbol{W}} \\sqrt{n} \\left( \\hat{\\boldsymbol{\\gamma}} - \\boldsymbol{\\gamma}(\\boldsymbol{\\theta}_0) \\right) + o_p(1). \\end{aligned} \\end{equation*}\\] 3.3.2 CLT for Dependent Processes For a dependent process, the validity of CLT requires the process to be “mixing” or “asymptotically independent”. Suppose two events \\(G\\) and \\(H\\) are independent, then \\[|\\mathbb{P}(G \\cap H) - \\mathbb{P}(G) \\mathbb{P}(H)| = 0.\\] Based on this idea, many dependence measures have been developed. The most commonly used \\(\\alpha\\)-mixing coefficient is one of these dependence measures which can be easily verified for certain stochastic processes. Definition 3.6 (Mixing Coefficients) For a stochastic process \\(\\{ X_i \\}_{i \\in \\mathbb{Z}}\\), we define the strong- or \\(\\alpha\\)-mixing coefficients as \\[\\begin{equation*} \\alpha(t_1, t_2) = \\text{sup}\\{ |\\mathbb{P}(A \\cap B) - \\mathbb{P}(A)\\mathbb{P}(B)|: \\; A\\in\\mathcal{F}_{-\\infty}^{t_1}, B\\in\\mathcal{F}_{t_2}^{\\infty} \\}, \\end{equation*}\\] where \\(\\mathcal{F}_{-\\infty}^{t_1} = \\sigma(X_{-\\infty}, \\dots, X_{t_1})\\) and \\(\\mathcal{F}_{t_2}^{\\infty} = \\sigma(X_{t_2}, \\dots, X_{\\infty})\\) are \\(\\sigma\\)-algebras generated by corresponding random variables. Remark 3.1 (Mixing Coefficients) If the process is stationary, then \\(\\alpha(t_1, t_2) = \\alpha(t_2, t_1) = \\alpha(|t_1-t_2|) \\equiv \\alpha(\\tau)\\). If \\(\\alpha(\\tau) \\to 0\\) as \\(\\tau \\to \\infty\\), then the process is strong-mixing or \\(\\alpha\\)-mixing. Theorem 3.10 (Central Limit Theorem and Alpha-Mixing) Let \\((X_t)\\) be a strictly stationary process with \\(\\mathbb{E}[X_t] =0\\). \\(S_n \\equiv \\sum_{t=1}^{n}X_t\\) is the partial sum process with \\(\\sigma_n^2 \\equiv \\text{Var}(S_n)\\). Suppose \\((X_t)\\) is \\(\\alpha\\)-mixing, and that for \\(\\delta &gt; 0\\), we have \\[\\begin{equation*} \\mathbb{E}\\left[ |X_t|^{2+\\delta} \\right] \\leq \\infty, \\mbox{ and } \\sum_{n = 0}^{\\infty} \\alpha(n)^{\\delta/2 + \\delta} \\leq \\infty. \\end{equation*}\\] Then \\[\\begin{equation*} \\underset{n \\to \\infty}{\\text{lim}}\\frac{\\sigma_n^2}{n} = \\mathbb{E}\\left[ |X_t|^{2} \\right] + 2\\sum_{k=1}^{\\infty}\\mathbb{E}\\left[ X_1X_k \\right] \\equiv \\sigma^2. \\end{equation*}\\] If \\(\\sigma^2 &gt; 0\\), \\((X_t)\\) obeys both the Central Limit Theorem with variance \\(\\sigma^2\\), and the functional Central Limit Theorem. Remark 3.2 (Implication of Alpha-Mixing) If a process \\((X_t)\\) is \\(\\alpha\\)-mixing, then its covariance structure is absolutely summable. 3.3.3 Asymptotic Normality of Sample AutoCovariance and AutoCorrelation Functions If \\((X_t)\\) is a white noise, then \\(\\hat{\\rho}(h)\\) should be equal to 0 if \\(h \\neq 0\\). In practice, this is of course not the case due the estimation error of \\(\\hat{\\rho}(h)\\). The next result gives us a way to assess whether the data comes from a completely random series or whether correlations are statistically significant at some lags. Theorem 3.11 (Distribution of Sample ACF in iid Case) If \\((X_t)\\) is white noise (with finite variance) and \\(h = 1, ..., H\\) where \\(H\\) is fixed but arbitrary, then we have that \\[\\begin{equation*} \\sqrt{T} \\left(\\hat{\\rho}(h) - {\\rho}(h)\\right) \\overset{\\mathcal{D}}{\\mapsto} \\mathcal{N}\\left(0, 1\\right). \\end{equation*}\\] The proof of Theorem 3.11 is straightforward from the CLT and Delta method. It is therefore omitted here but can for example be found in Hamilton (1994). Theorem 3.11 implies that an approximate confidence interval for \\(\\hat{\\rho}(h)\\) (in the iid case) is given by \\[\\text{CI}({\\rho}(h), \\alpha) = \\hat{\\rho}(h) \\pm \\frac{z_{1-\\frac{\\alpha}{2}} }{\\sqrt{T}}\\] for \\(0 &lt; h &lt; k &lt; \\infty\\) and where \\(z_{1- \\frac{\\alpha}{2}} \\equiv \\boldsymbol{\\Phi}^{-1}\\left( 1- \\frac{\\alpha}{2} \\right)\\) is the \\((1- \\frac{\\alpha}{2})\\) quantile of a standard normal distribution. Typically, for \\(\\alpha = 0.05\\) one would consider the following confidence interval: \\[\\text{CI}({\\rho}(h), 0.05) = \\hat{\\rho}(h) \\pm \\frac{2}{\\sqrt{T}}.\\] References "],
["allan-variance-calibration-techniques.html", "Chapter 4 Allan Variance Calibration Techniques 4.1 Spectral Ambiguity of the AV 4.2 Properties of the Allan Variance 4.3 Estimation 4.4 Allan variance based estimation", " Chapter 4 Allan Variance Calibration Techniques The Allan Variance (AV) is a statistical technique originally developed in the mid-1960s to study the stability of precision oscillators (see e.g. Allan 1966). It can provide information on the types and magnitude of various superimposed noise terms (i.e. composite stochastic processes). This method has been adapted to characterize the properties of a variety of devices including inertial sensors (see El-Sheimy, Hou, and Niu 2008). The AV is a measure of variability developed for long term memory processes and can in fact be interpreted as a Haar wavelet coefficient variance (see Percival and Guttorp 1994). We will discuss this connection further on. Definition 4.1 (Allan Variance) We consider the AV at dyadic scales (\\(\\tau_j\\)) starting from local averages of the process which can be denoted as \\[\\begin{equation*} \\bar{X}_{t}^{(j)} \\equiv \\frac{1}{\\tau_j} \\sum_{i = 1}^{\\tau_j} X_{t - \\tau_j + i}\\, , \\label{mean.noav} \\end{equation*}\\] where \\(\\tau_j \\equiv 2^j, \\; j \\in \\left\\{x \\in \\mathbb{N} \\, : \\; 1 \\leq x &lt; \\log_2 (T) - 1 \\right\\}\\) therefore determines the number of consecutive observations considered for the average. Then, the AV is defined as \\[\\begin{equation*} \\text{AV}_j \\left(X_t \\right) \\equiv \\frac{1}{2} \\, \\mathbb{E}\\left[ \\left(\\bar{X}_{t}^{(j)} - \\bar{X}_{t-\\tau_j}^{(j)} \\right)^2 \\right]. \\end{equation*}\\] Remark 4.1 (Alternative scale definition) The definition of the AV is actually valid for \\(\\tau_j = \\lfloor2^j\\rfloor\\) with \\(j \\in \\left\\{x \\in \\mathbb{R} \\, : \\; 1 \\leq x &lt; \\log_2 (T) - 1 \\right\\}\\). In some case, it could use to consider this alternative definition (see e.g. El-Sheimy, Hou, and Niu 2008) but we shall restrict ourself here to the case where \\(j \\in \\left\\{x \\in \\mathbb{N} \\, : \\; 1 \\leq x &lt; \\log_2 (T) - 1 \\right\\}\\). Remark 4.2 (Notation of the Allan Variance) For notational simplicity, we may sometimes replace \\(\\text{AV}_j \\left(X_t \\right)\\) by simply \\(\\phi_j^2\\) when the dependence of the AV to the process \\((X_t)\\) is evident. As highlighted earlier, the AV is, among others, a widely and commonly used approach in engineering for sensor calibration as it is linked to the properties of the process \\((X_t)\\) as shown in the following lemma (see e.g. Percival and Walden 2006 for a proof). Lemma 4.1 (AV connection to PSD) For a stationary process \\((X_t)\\) with PSD \\(S_{X}(f)\\) we have \\[\\begin{equation*} \\phi_j^2 \\equiv \\text{AV}_j \\left(X_t \\right) = 4 \\int_0^{\\infty} \\frac{\\sin^4(\\pi f \\tau_j)}{(\\pi f \\tau_j)^2} S_{X}(f) df. \\label{eq:allanvariancePSD_LInk} \\end{equation*}\\] Therefore, this result establishes a direct connection between the AV and PSD. A natural question is therefore whether the mapping PSD \\(\\mapsto\\) AV is one-to-one. Greenhall (1998) (see Theorem 1) showed that this is actually not the case. This is illustrated in the following section ????? (ADD REF). 4.1 Spectral Ambiguity of the AV Consider two (independent) stochastic processes \\((X_t)\\) and \\((Y_t)\\) with respective PSD \\(S_X(f)\\) and \\(S_Y(f)\\). Suppose that \\(S_X(f) \\neq S_Y(f)\\), then the two processes will have the same AV if \\[\\begin{equation*} \\Delta \\equiv \\int_0^{\\infty} \\frac{\\sin^4(\\pi f \\tau_j)}{(\\pi f \\tau_j)^2} \\Phi(f) df = 0, \\end{equation*}\\] where \\(\\Phi(f) \\equiv S_{X}(f) - S_{Y}(f)\\). To show that it is possible that \\(\\Delta = 0\\) when \\(\\Phi(f) \\neq 0\\), we will use the following critical identity: \\[\\begin{equation} \\label{ident:av:indet} \\sin^4(x) = \\sin^2(x) - \\frac{1}{4} \\sin^2(2x). \\end{equation}\\] First, we note that \\(\\Delta\\) may be expressed using () as follows: \\[\\begin{equation*} \\begin{aligned} \\Delta &amp;= \\int_{0}^{\\infty} \\frac{\\sin^4\\left(\\tau \\pi f \\right)}{\\left(\\tau \\pi f \\right)^2} \\Phi(f) df \\\\ &amp;= \\lim_{n \\rightarrow -\\infty} \\int_{2^{n}}^{\\infty} \\frac{\\sin^2\\left(\\tau \\pi f \\right) - \\frac{1}{4} \\sin^2\\left(2 \\tau \\pi f \\right) }{\\left(\\tau \\pi f \\right)^2} \\Phi(f) df . \\end{aligned} \\end{equation*}\\] Second, by the change of variable \\(u = 2f\\) in the second term we obtain \\[\\begin{equation*} \\begin{aligned} \\Delta = \\lim_{n \\rightarrow -\\infty} &amp; \\Bigg[ \\int_{2^{n}}^{\\infty} \\frac{\\sin^2\\left(\\tau \\pi f \\right)}{\\left(\\tau \\pi f \\right)^2} \\Phi(f) df - \\frac{1}{2}\\int_{2^{n+1}}^{\\infty} \\frac{\\sin^2\\left(\\tau \\pi u \\right)}{\\left(\\tau \\pi u \\right)^2} \\Phi(f) du \\Bigg]. \\end{aligned} \\end{equation*}\\] Now suppose that \\(\\Phi(f) = 2 \\Phi(2f)\\). In this case, we have \\(\\Phi(f) = 2 \\Phi(u)\\) and therefore we obtain \\[\\begin{equation*} \\begin{aligned} \\Delta &amp;= \\lim_{n \\rightarrow -\\infty} \\int_{2^{n}}^{2^{n+1}} \\frac{\\sin^2\\left(\\tau \\pi f \\right)}{\\left(\\tau \\pi f \\right)^2} \\Phi(f) df = 0. \\end{aligned} \\end{equation*}\\] Remark 4.3 This result demonstrates that the mapping from PSD to Allan variance is not necessarily one-to-one. Greenhall (1998) showed that in the continuous case (i.e. \\(\\tau_j \\in \\mathbb{R}\\)) \\(\\Delta = 0\\) if and only if \\(\\Phi(f) = 2 \\Phi(2f)\\). However, the ``only if’’ part of this results (while conjectured) is unknown in the discrete case. 4.2 Properties of the Allan Variance One reason of explaining the widespread use of the Allan variance for sensor calibration is due to the following additivity property, which is particularly convenient to identify composite stochastic processes (see Definition REF MISSING!). Corollary 4.1 (Additivity of the AV) Consider two (independent) stochastic processes \\((X_t)\\) and \\((Y_t)\\) with respective PSD \\(S_X(f)\\) and \\(S_Y(f)\\). Suppose that we observe the process \\(Z_t = X_t + Y_t\\). Then, we have \\[\\begin{equation*} \\text{AV}_j \\left(Z_t \\right) = \\text{AV}_j \\left(X_t \\right) + \\text{AV}_j \\left(Y_t \\right). \\end{equation*}\\] Proof. The proof of this result is direct from Lemma REF MISSING HERE. Indeed, since \\(S_Z(f) = S_X(f) + S_Y(f)\\), we have \\[\\begin{equation*} \\begin{aligned} \\text{AV}_j \\left(Z_t \\right) &amp;= 4 \\int_0^{\\infty} \\frac{\\sin^4(\\pi f \\tau_j)}{(\\pi f \\tau_j)^2} S_{Z}(f) df\\\\ &amp;= 4 \\int_0^{\\infty} \\frac{\\sin^4(\\pi f \\tau_j)}{(\\pi f \\tau_j)^2} S_{X}(f) df + 4 \\int_0^{\\infty} \\frac{\\sin^4(\\pi f \\tau_j)}{(\\pi f \\tau_j)^2} S_{Y}(f) df\\\\ &amp;= \\text{AV}_j \\left(X_t \\right) + \\text{AV}_j \\left(Y_t \\right). \\end{aligned} \\end{equation*}\\] While Lemma 4.1 is an important results which is very convenient to determine the theoretical AV of a certain stochastic process. However, the applicability of this results is often limited since the integral defined in (REF MISSING HERE) can be intractable. An alternative to Lemma 4.1 has been proposed by Zhang (2008) and is far advantageous from a computational standpoint. Lemma 4.2 (AV connection to ACF) For a stationary process \\((X_t)\\) with variance \\(\\sigma^2_X\\) and ACF \\(\\rho(h)\\) we have \\[\\begin{equation*} \\label{stat.av} \\text{AV}_j \\left(X_t \\right) = \\frac{\\sigma_X^2}{\\tau_j^2} \\bigg(\\tau_j\\left[1-\\rho(\\tau_j)\\right] + \\sum_{i=1}^{\\tau_j-1} i \\left[2 \\rho(\\tau_j-i) - \\rho(i) - \\rho(2\\tau_j-i)\\right]\\bigg). \\end{equation*}\\] The proof of this result is instructive and is presented in Xu et al. (2017). Remark 4.4 Using Lemma 4.2, the exact form of the AV for different stationary processes, such as the general class of ARMA models, can easily be derived. Moreover, Zhang (2008) provided the theoretical AV for non-stationary processes such as the random walk and ARFIMA models for which the AV, as mentioned earlier, represents a better measure of uncertainty compared to other methods. Remark 4.5 Lemma 4.2 was extended to non-stationary processes in Xu et al. (2017). Example 4.1 (Theoretical AV of an MA(1) process) From the autocovariance we obtain \\[\\begin{equation*} \\rho(h) = \\text{corr}\\left(X_t, X_{t-h} \\right) =\\left\\{ \\begin{array}{cl} 1 &amp;\\text{if } h = 0\\\\ \\frac{\\theta}{1 + \\theta^2} &amp;\\text{if } |h| = 1\\\\ 0 &amp;\\text{if } |h| &gt; 1.\\\\ \\end{array} \\right. \\end{equation*}\\] We can now apply the formula given in Lemma 4.2, which leads to \\[\\begin{equation*} \\begin{aligned} \\text{AV}_j \\left(X_t \\right) &amp;= \\frac{\\left(1 + \\theta^2 \\right) \\sigma^2}{\\tau_j^2} \\bigg(\\tau_j + \\sum_{i=1}^{\\tau_j-1} i \\left[2 \\rho(\\tau_j-i) - \\rho(i) - \\rho(2\\tau_j-i)\\right]\\bigg)\\\\ &amp;=\\frac{\\left(1 + \\theta^2 \\right) \\sigma^2}{\\tau_j^2} \\bigg(\\tau_j + 2 \\sum_{i=1}^{\\tau_j-1} i \\rho(\\tau_j-i) -\\sum_{i=1}^{\\tau_j-1} i \\rho(i) - \\sum_{i=1}^{\\tau_j-1} i \\rho(2\\tau_j-i)\\bigg)\\\\ &amp;=\\frac{\\left(1 + \\theta^2 \\right) \\sigma^2}{\\tau_j^2} \\left(\\tau_j + 2 (\\tau_j - 1) \\rho(1) - \\rho(1) \\right)\\\\ &amp;=\\frac{\\left(1 + \\theta^2 \\right) \\sigma^2}{\\tau_j^2} \\bigg(\\tau_j + (2\\tau_j - 3) \\frac{\\theta}{1 + \\theta^2} \\bigg). \\end{aligned} \\end{equation*}\\] \\(\\LARGE{\\bullet}\\) 4.3 Estimation Several estimators of the AV have been introduced in the literature. The most commonly is (probably) the Maximum-Overlapping AV (MOAV) estimator proposed by Percival and Guttorp (1994), which is defined as follows: Definition 4.2 (Maximum-Overlapping AV Estimator) The MOAV is defined as: \\[\\begin{eqnarray} \\label{eq:MOAVNS_est} \\hat{\\phi}_j^2 \\equiv \\widehat{\\text{AV}}_j \\left(X_t \\right) = \\frac{1}{2 \\left(T - 2\\tau_j + 1\\right)} \\sum_{k = 2 \\tau_j}^{T} \\left(\\bar{X}_{k}^{(j)} - \\bar{X}_{k-\\tau_j}^{(j)} \\right)^2. \\end{eqnarray}\\] We will now study the properties of this estimator through the following lemmas. 4.3.1 Consistency Lemma 4.3 (Consistency) Let \\((X_t)\\) be such that: \\((X_t - X_{t-1})\\) is a (strongly) stationary process, \\((X_t - X_{t-1})^2\\) has absolutely summable covariance structure, \\(\\mathbb{E}\\left[(X_t - X_{t-1})^4\\right] &lt; \\infty\\), Then, we have \\[\\widehat{\\text{AV}}_j \\left(X_t \\right) \\overset{ \\mathcal{P} }{\\longrightarrow} \\text{AV}_j \\left(X_t \\right).\\] Proof. TO BE ADDED Remark 4.6 (Connection to Wavelet Variance) This result is closely related by the results of Percival (1995) on the wavelet variance. We shall explore the connection between the AV and wavelet variance in the next section. 4.3.2 Asymptotic Normality Compare to consistency, the asymptotic normality requires stronger conditions given in the following lemma. Lemma 4.4 (Asymptotic normality) Let \\((X_t)\\) be such that: \\((X_t - X_{t-1})\\) is a (strongly) stationary process. \\((X_t - X_{t-1})\\) is strong mixing process with mixing coefficient \\(\\alpha(n)\\) such that \\(\\sum_{n=1}^{\\infty} \\alpha(n)^{\\frac{\\delta}{2+\\delta}} &lt; \\infty\\) for some \\(\\delta &gt; 0\\). \\(\\mathbb{E}\\left[\\left(X_t - X_{t-1}\\right)^{4+\\delta}\\right] &lt; \\infty\\) for some \\(\\delta &gt; 0\\). Then, under these conditions we have that \\[\\sqrt{T}\\left(\\widehat{\\text{AV}}_j \\left(X_t \\right) - \\text{AV}_j \\left(X_t \\right) \\right) \\overset{ \\mathcal{D} }{\\longrightarrow} \\mathcal{N}(0, \\sigma^2_T/T),\\] where \\(\\sigma^2_T \\equiv \\sum_{h = -\\infty}^{\\infty}\\text{cov}\\left( \\left(\\bar{X}_{t}^{(j)} - \\bar{X}_{t-\\tau_j}^{(j)} \\right)^2, \\left(\\bar{X}_{t+h}^{(j)} - \\bar{X}_{t+h-\\tau_j}^{(j)} \\right)^2 \\right)\\). Proof. TO BE ADDED 4.3.3 Confidence Interval of the MOAV Estimator Based on the asymptotic normality results (Lemma ), we can construct the \\(1-\\alpha\\) confidence intervals for \\(\\widehat{\\text{AV}}_j \\left(X_t \\right)\\) as % \\[\\begin{equation*} \\text{CI}\\left(\\text{AV}_j \\left(X_t \\right)\\right) = \\left[ \\widehat{\\text{AV}}_j \\left(X_t \\right) \\pm z_{1 - \\frac{\\alpha}{2}} \\frac{\\sigma_{T}}{T} \\right], \\end{equation*}\\] % where \\(z_{1 - \\frac{\\alpha}{2}} \\equiv \\boldsymbol{\\Phi}^{-1}\\left( 1- \\frac{\\alpha}{2} \\right)\\) is the \\((1- \\frac{\\alpha}{2})\\) quantile of a standard normal distribution.\\ However, the so called ``Long-Run Variance’’ \\(\\sigma^2_{T}\\) is usually unknown. Many methods have been proposed to consistently estimate it under mild conditions (see e.g. Newey and West 1986). Remark 4.7 Gaussian-based confidence intervals are often problematic with the AV as the lower limit of CI can very well be negative. Next, we will discuss an alternative method to construct the CI for such statistic. 4.4 Allan variance based estimation 4.4.1 Allan Variance log-log Representation As illustrated in Lemmas 4.1 and 4.2 the AV depends on the properties of the stochastic process \\((X_t)\\). We will see that ``log-log’’ representation of the AV is often useful for the identify various processes that may compose \\((X_t)\\). For example, let’s suppose that \\(X_t\\) is a white noise process. We showed in REF MISSING HERE that the theoretical AV of such process is given \\[\\begin{equation*} \\phi_j^2 \\equiv \\text{AV}_j(X_t) = \\frac{\\sigma^2}{\\tau_j}. \\end{equation*}\\] Therefore, we have that the Allan Deviation or AD (i.e. \\(\\sqrt{\\text{AV}_j(X_t)}\\) or \\(\\phi_j\\)) is such that \\[\\begin{equation} \\log\\left( \\phi_j \\right) = \\log \\left(\\sqrt{\\frac{\\sigma^2}{\\tau_j}}\\right) = \\log \\left(\\sigma\\right) - \\frac{1}{2} \\log (\\tau_j). \\label{eq:av:wn} \\end{equation}\\] Thus, the log of the AD is linear in \\(\\tau_j\\) with a slope of \\(-1/2\\) and with intercept \\(\\log (\\sigma)\\). Let us start by considering a simple simulated example. 4.4.2 Allan Deviation of a WN process Simulation based on a white noise process with \\(\\sigma^2 = 10^2\\) and \\(T = 10^5\\). # Load packages library(av) # Package for Allan Variance functions library(simts) # Package for time series simulations # Simulate white noise Xt = gen_gts(WN(sigma2 = 1), n = 10^5) # Compute allan variance av = avar(Xt) # Allan Variance log-log Representation plot(av) Figure 4.1: ADD A NICE CAPTION References "],
["the-generalized-method-of-wavelet-moments.html", "Chapter 5 The Generalized Method of Wavelet Moments", " Chapter 5 The Generalized Method of Wavelet Moments "],
["extensions.html", "Chapter 6 Extensions", " Chapter 6 Extensions "],
["references.html", "References", " References "],
["introduction-1.html", "Chapter 7 Introduction 7.1 Time Series 7.2 Exploratory Data Analysis for Time Series 7.3 Basic Time Series Models 7.4 Composite Stochastic Processes", " Chapter 7 Introduction “Prévoir consiste à projeter dans l’avenir ce qu’on a perçu dans le passé.” – Henri Bergson After reading this chapter you will be able to: Describe what a time series is. Perform exploratory data analysis on time series data. Evaluate different characteristics of a time series. Classify basic time series models through equations and plots. Manipulate a time series equation using backsubstitution. 7.1 Time Series Generally speaking a time series (or stochastic process) corresponds to set of “repeated” observations of the same variable such as price of a financial asset or temperature in a given location. In terms of notation a time series is often written as \\[\\left(X_1, X_2, ..., X_n \\right) \\;\\;\\; \\text{ or } \\;\\;\\; \\left(X_t\\right)_{t = 1,...,n}.\\] The time index \\(t\\) is contained within either the set of reals, \\(\\mathbb{R}\\), or integers, \\(\\mathbb{N}\\). When \\(t \\in \\mathbb{R}\\), the time series becomes a continuous-time stochastic process such a Brownian motion, a model used to represent the random movement of particles within a suspended liquid or gas, or an ElectroCardioGram (ECG) signal, which corresponds to the palpitations of the heart. However, within this text, we will limit ourselves to the cases where \\(t \\in \\mathbb{N}\\), better known as discrete-time processes. Discrete-time processes are where a variable is measured sequentially at fixed and equally spaced intervals in time. This implies that we will have two assumptions: \\(t\\) is not random, e.g. the time at which each observation is measured is known, and the time between two consecutive observations is constant. Moreover, the term “time series” can also represent a probability model for a set of observations. For example, one of the fundamental probability models used in time series analysis is called a white noise process and is defined as \\[W_t \\mathop \\sim \\limits^{iid} N(0, \\sigma^2).\\] This statement simply means that \\((W_t)\\) is normally distributed and independent over time. This model may appear to be dull but we will soon see it as a crucial component to constructing more complex models. Unlike the white noise process, time series are typically not independent over time. Suppose that the temperature in Champaign is unusually low, then it is reasonable to assume that tomorrow’s temperature will also be low. Indeed, such behavior would suggest the existence of a dependency over time. The time series methods we will discuss in this text consists of parametric models used to characterize (or at least approximate) the joint distribution of \\((X_t)\\). Often, time series models can be decomposed into two components, the first of which is what we call a signal, say \\((Y_t)\\), and the second component is a noise, say \\((W_t)\\), leading to the model \\[X_t = Y_t + W_t.\\] Typically, we have \\(\\mathbb{E}[Y_t] \\neq 0\\) while \\(\\mathbb{E}[W_t] = 0\\) (although we may have \\(\\mathbb{E}[W_t | W_{t-1}, ..., W_1] \\neq 0\\)). Such models impose some parametric structure which represents a convenient and flexible way of studying time series as well as a means to evaluate future values of the series through forecasting. As we will see, predicting future values is one of the main aspects of time series analysis. However, making predictions is often a daunting task or as famously stated by Nils Bohr: “Prediction is very difficult, especially about the future.” There are plenty of examples of predictions that turned out to be completely erroneous. For example, three days before the 1929 crash, Irving Fisher, Professor of Economics at Yale University, famously predicted: “Stock prices have reached what looks like a permanently high plateau”. Another example is given by Thomas Watson, president of IBM, who said in 1943: “I think there is a world market for maybe five computers.” 7.2 Exploratory Data Analysis for Time Series When dealing with relatively small time series (e.g. a few thousands), it is often useful to look at a graph of the original data. A graph can be an informative tool for “detecting” some features of a time series such as trends and the presence of outliers. Indeed, a trend is typically assumed to be present in a time series when the data exhibit some form of long term increase or decrease or combination of increases or decreases. Such trends could be linear or non-linear and represent an important part of the “signal” of a model. Here are a few examples of non-linear trends: Seasonal trends (periodic): These are the cyclical patterns which repeat after a fixed/regular time period. This could be due to business cycles (e.g. bust/recession, recovery). Non-seasonal trends (periodic): These patterns cannot be associated to seasonal variation and can for example be due to an external variable such as, for example, the impact of economic indicators on stock returns. Note that such trends are often hard to detect based on a graphical analysis of the data. “Other” trends: These trends have typically no regular patterns and are over a segment of time, known as a “window”, that change the statistical properties of a time series. A common example of such trends is given by the vibrations observed before, during and after an earthquake. Example 7.1 A traditional example of a time series is the quarterly earnings of the company Johnson and Johson. In the figure below, we present these earnings between 1960 and 1980. # Load data data(jj, package = &quot;astsa&quot;) # Construct gts object jj = gts(jj, start = 1960, freq = 4, unit_ts = &quot;$&quot;, name_ts = &quot;Earnings&quot;, data_name = &quot;Johnson and Johnson Quarterly Earnings&quot;) # Plot time series plot(jj) One trait that the graph makes evident is that the data contains a non-linear increasing trend as well as a yearly seasonal component. In addition, one can note that the variability of the data seems to increase with time. Being able to make such observations provides important information to select suitable models for the data. Moreover, when observing “raw” time series data it is also interesting to evaluate if some of the following phenomena occur: Change in Mean: Does the mean of the process shift over time? Change in Variance: Does the variance of the process evolve with time? Change in State: Does the time series appear to change between “states” having distinct statistical properties? Outliers Does the time series contain some “extreme” observations? (Note that this is typically difficult to assess visually.) Example 7.2 In the figure below, we present an example of displacement recorded during an earthquake as well as an explosion. # TO DO! From the graph, it can be observed that the statistical properties of the time series appear to change over time. For instance, the variance of the time series shifts at around \\(t = 1150\\) for both series. The shift in variance also opens “windows” where there appear to be distinct states. In the case of the explosion data, this is particularly relevant around \\(t = 50, \\cdots, 250\\) and then again from \\(t = 1200, \\cdots, 1500\\). Even within these windows, there are “spikes” that could be considered as outliers most notably around \\(t = 1200\\) in the explosion series. Extreme observations or outliers are commonly observed in real time series data, this is illustrated in the following example. Example 7.3 We consider here a data set coming from the domain of hydrology. The data concerns monthly precipitation (in mm) over a certain period of time (1907 to 1972) and is interesting for scientists in order to study water cycles. The data are presented in the graph below: # Load hydro dataset data(&quot;hydro&quot;) # Simulate based on data hydro = gts(as.vector(hydro), start = 1907, freq = 12, unit_ts = &quot;in.&quot;, name_ts = &quot;Precipitation&quot;, data_name = &quot;Hydrology data&quot;) # Plot hydro plot(hydro) Next, we consider an example coming from high-frequency finance to illustrate the limitations our current framework. Example 7.4 The figure below presents the returns or price innovations (i.e. the changes in price from one observation to the next) for Starbuck’s stock on July 1, 2011 for about 150 seconds (left panel) and about 400 minutes (right panel). # TO DO add Duke&#39;s code here! It can be observed on the left panel that observations are not equally spaced. Indeed, in high-frequency data the intervals between two points are typically not constant and are, even worse, random variables. This implies that the time when a new observation will be available is in general unknown. On the right panel, one can observe that the variability of the data seems to change during the course of the trading day. Such a phenomenon is well known in the finance community since a lot of variation occurs at the start (and the end) of the day while the middle of the day is associated with small changes. Moreover, clear extreme observations can also be noted in this graph at around 11:00. Finally, let us consider the limitations of a direct graphical representation of a time series when the sample size is large. Indeed, due to visual limitations, a direct plotting of the data will probably result in an uninformative aggregation of points between which it is unable to distinguish anything. This is illustrated in the following example. Example 7.5 We consider here the data coming from the calibration procedure of an Inertial Measurement Unit (IMU) which, in general terms, is used to enhance navigation precision or reconstruct three dimensional movements (see e.g. link). These sensors are used in a very wide range of applications such as robotics, virtual reality, vehicle stability control, human and animal motion capture and so forth (see e.g. link). The signals coming from these instruments are measured at high frequencies over a long time and are often characterized by linear trends and numerous underlying stochastic processes. The code below retrieves some data from an IMU and plots it directly: # Load IMU data data(imu6, package = &quot;imudata&quot;) # Construct gst object Xt = gts(imu6[,1], data_name = &quot;Gyroscope data&quot;, unit_time = &quot;hour&quot;, freq = 100*60*60, name_ts = &quot;Angular rate&quot;, unit_ts = bquote(rad^2/s^2)) # Plot time series plot(Xt) Although a linear trend and other processes are present in this signal (time series), it is practically impossible to understand or guess anything from the plot. 7.3 Basic Time Series Models In this section, we introduce some simple time series models. Before doing so it is useful to define \\(\\Omega_t\\) as all the information avaiable up to time \\(t-1\\), i.e. \\[\\Omega_t = \\left(X_{t-1}, X_{t-2}, ..., X_0 \\right).\\] As we will see this compact notation is quite useful. 7.3.1 White noise processes The building block for most time series models is the Gaussian white noise process, which can be defined as \\[{W_t}\\mathop \\sim \\limits^{iid} N\\left( {0,\\sigma _w^2} \\right).\\] This definition implies that: \\(\\mathbb{E}[W_t | \\Omega_t] = 0\\) for all \\(t\\), \\(\\cov\\left(W_t, W_{t-h} \\right) = \\boldsymbol{1}_{h = 0} \\; \\sigma^2\\) for all \\(t, h\\). Therefore, in this process there is an absence of temporal (or serial) dependence and is homoskedastic (i.e. it has a constant variance). White noise can be generalized into two sorts of processes: weak and strong. The process \\((W_t)\\) is a weak white noise if \\(\\mathbb{E}[W_t] = 0\\) for all \\(t\\), \\(\\var\\left(W_t\\right) = \\sigma^2\\) for all \\(t\\), \\(\\cov \\left(W_t, W_{t-h}\\right) = 0\\), for all \\(t\\), and for all \\(h \\neq 0\\). Note that this definition does not imply that \\(W_t\\) and \\(W_{t-h}\\) are independent (for \\(h \\neq 0\\)) but simply uncorrelated. However, the notion of independence is used to define a strong white noise as \\(\\mathbb{E}[W_t] = 0\\) and \\(\\var(W_t) = \\sigma^2 &lt; \\infty\\), for all \\(t\\), \\(F(W_t) = F(W_{t-h})\\), for all \\(t,h\\) (where \\(F(W_t)\\) denotes the distribution of \\(W_t\\)), \\(W_t\\) and \\(W_{t-h}\\) are independent for all \\(t\\) and for all \\(h \\neq 0\\). It is clear from these definitions that if a process is a strong white noise it is also a weak white noise. However, the converse is not true as shown in the following example: Example 7.6 Let \\(Y_t \\mathop \\sim F_{t+2}\\), where \\(F_{t+2}\\) denotes a Student distribution with \\(t+2\\) degrees of freedom. Assuming the sequence \\((Y_1, \\ldots, Y_n)\\) to be independent, we let \\(X_t = \\sqrt{\\frac{t}{t+2}} Y_t\\). Then, the process \\((X_t)\\) is obviously not a strong white noise as the distribution of \\(X_t\\) changes with \\(t\\). However, this process is a weak white noise since we have: \\(\\mathbb{E}[X_t] = \\sqrt{\\frac{t}{t+2}} \\mathbb{E}[Y_t] = 0\\) for all \\(t\\). \\(\\var(X_t) = \\frac{t}{t+2} \\var(Y_t) = \\frac{t}{t+2} \\frac{t+2}{t} = 1\\) for all \\(t\\). \\(\\cov(X_t, X_{t+h}) = 0\\) (by independence), for all \\(t\\), and for all \\(h \\neq 0\\). The code below presents an example of how to simulate a Gaussian white noise process. n = 1000 # process length sigma2 = 1 # process variance Xt = gen_gts(n, WN(sigma2 = sigma2)) plot(Xt) 7.3.2 Random Walk Processes The term random walk was first introduced by Karl Pearson in the early nineteen-hundreds. Regarding white noise, there exist a large range of random walk processes. For example, one of the simplest forms of a random walk process can be explained as follows: suppose that you are walking on campus and your next step can either be to your left, your right, forward or backward (each with equal probability). Two realizations of such processes are represented below: set.seed(5) RW2dimension(steps = 10^2) RW2dimension(steps = 10^4) Such processes inspired Karl Pearson’s famous quote that “the most likely place to find a drunken walker is somewhere near his starting point.” Empirical evidence of this phenomenon is not too hard to find on a Friday night. In this text, we only consider one very specific form of random walk, namely the Gaussian random walk which can be defined as: \\[X_t = X_{t-1} + W_t,\\] where \\(W_t\\) is a Gaussian white noise process with initial condition \\(X_0 = c\\). (Typically \\(c = 0\\).) This process can be expressed differently by backsubstitution as follows: \\[\\begin{aligned} {X_t} &amp;= {X_{t - 1}} + {W_t} \\\\ &amp;= \\left( {{X_{t - 2}} + {W_{t - 1}}} \\right) + {W_t} \\\\ &amp;= \\vdots \\\\ {X_t} &amp;= \\sum\\limits_{i = 1}^t {{W_i}} + X_0 = \\sum\\limits_{i = 1}^t {{W_i}} + c \\\\ \\end{aligned} \\] The code below presents an example of how to simulate a such process. n = 1000 # process length gamma2 = 1 # innovation variance Xt = gen_gts(n, RW(gamma2 = gamma2)) plot(Xt) 7.3.3 Autoregressive Process of Order 1 An autoregressive process of order 1 or AR(1) is a generalization of both the white noise and random walk processes which are both themselves special cases of an AR(1). A (Gaussian) AR(1) process can be defined as \\[{X_t} = {\\phi}{X_{t - 1}} + {W_t},\\] where \\(W_t\\) is a Gaussian white noise. Clearly, an AR(1) with \\(\\phi = 0\\) is a Gaussian white noise and when \\(\\phi = 1\\) the process becomes a random walk. Remark. We generally assume that an AR(1), as well as other time series models, have zero mean. The reason for this assumption is only to simplfy the notation but it is easy to consider an AR(1) process around an arbitrary mean \\(\\mu\\), i.e. \\[\\left(X_t - \\mu\\right) = \\phi \\left(X_{t-1} - \\mu \\right) + W_t,\\] which is of course equivalent to \\[X_t = \\left(1 - \\phi \\right) \\mu + \\phi X_{t-1} + W_t.\\] Thus, we will generally only work with zero mean processes since adding means is simple. Remark. An AR(1) is in fact a linear combination of past realisations of the white noise \\(W_t\\) process. Indeed, we have \\[\\begin{aligned} {X_t} &amp;= {\\phi_t}{X_{t - 1}} + {W_t} = {\\phi}\\left( {{\\phi}{X_{t - 2}} + {W_{t - 1}}} \\right) + {W_t} \\\\ &amp;= \\phi^2{X_{t - 2}} + {\\phi}{W_{t - 1}} + {W_t} = {\\phi^t}{X_0} + \\sum\\limits_{i = 0}^{t - 1} {\\phi^i{W_{t - i}}}. \\end{aligned}\\] Under the assumption of infinite past (i.e. \\(t \\in \\mathbb{Z}\\)) and \\(|\\phi| &lt; 1\\), we obtain \\[X_t = \\sum\\limits_{i = 0}^{\\infty} {\\phi^i {W_{t - i}}},\\] since \\(\\operatorname{lim}_{i \\to \\infty} \\; {\\phi^i}{X_{t-i}} = 0\\). The code below presents an example of how an AR(1) can be simulated. n = 1000 # process length phi = 0.5 # phi parameter sigma2 = 1 # innovation variance Xt = gen_gts(n, AR1(phi = phi, sigma2 = sigma2)) plot(Xt) 7.3.4 Moving Average Process of Order 1 As we have seen in the previous example, an AR(1) can be expressed as a linear combination of all past observations of \\((W_t)\\) while the next process, called a moving average process of order 1 or MA(1), is (in some sense) a “truncated” version of an AR(1). It is defined as \\[\\begin{equation} X_t = \\theta W_{t-1} + W_t, \\end{equation}\\] where (again) \\(W_t\\) denotes a Gaussian white noise process. An example on how to generate an MA(1) is given below: n = 1000 # process length sigma2 = 1 # innovation variance theta = 0.5 # theta parameter Xt = gen_gts(n, MA1(theta = theta, sigma2 = sigma2)) plot(Xt) 7.3.5 Linear Drift A linear drift is a very simple deterministic time series model which can be expressed as \\[X_t = X_{t-1} + \\omega, \\] where \\(\\omega\\) is a constant and with the initial condition \\(X_0 = c\\), where \\(c\\) is an arbitrary constant (typically zero). This process can be expressed in a more familiar form as follows: \\[ {X_t} = {X_{t - 1}} + \\omega = \\left( {{X_{t - 2}} + \\omega} \\right) + \\omega = t{\\omega} + c \\] Therefore, a (linear) drift corresponds to a simple linear model with slope \\(\\omega\\) and intercept \\(c\\). A drift can simply be generated using the code below: n = 100 # process length omega = 0.5 # slope parameter Xt = gen_gts(n, DR(omega = omega)) plot(Xt) 7.4 Composite Stochastic Processes A composite stochastic process can be defined as the sum of underlying (or latent) stochastic processes. In this text, we will use the term latent time series as a synomym for composite stochastic processes. A simple example of such a process is given by \\[\\begin{aligned} Y_t &amp;= Y_{t-1} + W_t + \\delta\\\\ X_t &amp;= Y_t + Z_t, \\end{aligned}\\] where \\(W_t\\) and \\(Z_t\\) are two independent Gaussian white noise processes. This model is often used as a first tool to approximate the number of individuals in the context ecological population dynamics. For example, suppose we want to study the population of Chamois in the Swiss Alps. Let \\(Y_t\\) denote the “true” number of individuals in this population at time \\(t\\). It is reasonable that \\(Y_t\\) is (approximately) the population at the previous time \\(t-1\\) (e.g the previous year) plus a random variation and a drift. This random variation is due to the natural randomness in ecological population dynamics and reflects changes such as the number of predators, the abundance of food, or weather conditions. On the other hand, ecological drift is often of particular interest for ecologists as it can be used to determine the “long” term trends of the population (e.g. if the population is increasing, decreasing, or stable). Of course, \\(Y_t\\) (the number of individauls) is typically unknown and we observe a noisy version of it, denoted as \\(X_t\\). This process corresponds to the true population plus a measurement error since some individuals may not be observed while others may have been counted several times. Interestingly, this process can clearly be expressed as a latent time series model (or composite stochastic process) as follows: \\[\\begin{aligned} R_t &amp;= R_{t-1} + W_t \\\\ S_t &amp;= \\delta t \\\\ X_t &amp;= R_t + S_t + Z_t, \\end{aligned}\\] where \\(R_t\\), \\(S_t\\) and \\(Z_t\\) denote, respectively, a random walk, a drift, and a white noise. The code below can be used to simulate such data: n = 1000 # process length delta = 0.005 # delta parameter (drift) sigma2 = 10 # variance parameter (white noise) gamma2 = 0.1 # innovation variance (random walk) model = WN(sigma2 = sigma2) + RW(gamma2 = gamma2) + DR(omega = delta) Xt = gen_lts(n, model) plot(Xt) In the above graph, the first three plots represent the latent (unobserved) processes (i.e. white noise, random walk, and drift) and the last one represents the sum of the three (i.e. \\((X_t)\\)). Let us consider a real example where these latent processes are useful to describe (and predict) the behavior of economic variables such as Personal Saving Rates (PSR). A process that is used for these settings is the “random-walk-plus-noise” model, meaning that the data can be explained by a random walk process in addition to which we observe some other process (e.g. a white noise model, an autoregressive model such as an AR(1), etc.). The PSR taken from the Federal Reserve of St. Louis from January 1, 1959, to May 1, 2015, is presented in the following plot: # Load savingrt dataset data(&quot;savingrt&quot;) # Simulate based on data savingrt = gts(as.vector(savingrt), start = 1959, freq = 12, unit_ts = &quot;%&quot;, name_ts = &quot;Saving Rates&quot;, data_name = &quot;US Personal Saving Rates&quot;) # Plot savingrt simulation plot(savingrt) It can be observed that the mean of this process seems to vary over time, suggesting that a random walk can indeed be considered as a possible model to explain this data. In addition, aside from some “spikes” and occasional sudden changes, the observations appear to gradually change from one time point to the other, suggesting that some other form of dependence between them could exist. "]
>>>>>>> e0d39ba584ef1df8f27481d0f11984e15001e5f3
]
