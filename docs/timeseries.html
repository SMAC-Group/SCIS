<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>An Introduction to Inertial Sensors Stochastic Calibration</title>
  <meta name="description" content="TO DO">
  <meta name="generator" content="bookdown 0.7.14 and GitBook 2.6.7">

  <meta property="og:title" content="An Introduction to Inertial Sensors Stochastic Calibration" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="TO DO" />
  <meta name="github-repo" content="SMAC-Group/SCIS" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="An Introduction to Inertial Sensors Stochastic Calibration" />
  
  <meta name="twitter:description" content="TO DO" />
  

<meta name="author" content="StÃ©phane Guerrier, Roberto Molinari, Yuming Zhang, Haotian Xu, Gaetan Bakalli, Ahmed Radi and Mucyo Karemera">


<meta name="date" content="2018-08-14">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="properties-of-statistical-estimators.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="timeseries.html"><a href="timeseries.html"><i class="fa fa-check"></i><b>2</b> Introduction to Time Series Analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="timeseries.html"><a href="timeseries.html#time-series"><i class="fa fa-check"></i><b>2.1</b> Time Series</a></li>
<li class="chapter" data-level="2.2" data-path="timeseries.html"><a href="timeseries.html#dependence-within-time-series"><i class="fa fa-check"></i><b>2.2</b> Dependence within Time Series</a></li>
<li class="chapter" data-level="2.3" data-path="timeseries.html"><a href="timeseries.html#stationarity"><i class="fa fa-check"></i><b>2.3</b> Stationarity</a></li>
<li class="chapter" data-level="2.4" data-path="timeseries.html"><a href="timeseries.html#linear-processes"><i class="fa fa-check"></i><b>2.4</b> Linear Processes</a></li>
<li class="chapter" data-level="2.5" data-path="timeseries.html"><a href="timeseries.html#basic-time-series-models"><i class="fa fa-check"></i><b>2.5</b> Basic Time Series Models</a></li>
<li class="chapter" data-level="2.6" data-path="timeseries.html"><a href="timeseries.html#fundamental-representations-of-time-series"><i class="fa fa-check"></i><b>2.6</b> Fundamental Representations of Time Series</a></li>
<li class="chapter" data-level="2.7" data-path="timeseries.html"><a href="timeseries.html#estimation-problems-with-time-series"><i class="fa fa-check"></i><b>2.7</b> Estimation Problems with Time Series</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html"><i class="fa fa-check"></i><b>3</b> Properties of Statistical Estimators</a><ul>
<li class="chapter" data-level="3.1" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#extremum-estimators"><i class="fa fa-check"></i><b>3.1</b> Extremum Estimators</a></li>
<li class="chapter" data-level="3.2" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#consistency"><i class="fa fa-check"></i><b>3.2</b> Consistency</a><ul>
<li class="chapter" data-level="3.2.1" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#consistency-of-extremum-estimators"><i class="fa fa-check"></i><b>3.2.1</b> Consistency of Extremum Estimators</a></li>
<li class="chapter" data-level="3.2.2" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#verification-of-condition-c1"><i class="fa fa-check"></i><b>3.2.2</b> Verification of Condition C1</a></li>
<li class="chapter" data-level="3.2.3" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#verification-of-condition-c4"><i class="fa fa-check"></i><b>3.2.3</b> Verification of Condition C4</a></li>
<li class="chapter" data-level="3.2.4" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#consistency-of-sample-autocovariance-and-autocorrelation-functions"><i class="fa fa-check"></i><b>3.2.4</b> Consistency of Sample AutoCovariance and AutoCorrelation Functions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#asymptotic-normality"><i class="fa fa-check"></i><b>3.3</b> Asymptotic Normality</a><ul>
<li class="chapter" data-level="3.3.1" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#clt-for-iid-random-variables"><i class="fa fa-check"></i><b>3.3.1</b> CLT for iid Random Variables</a></li>
<li class="chapter" data-level="3.3.2" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#clt-for-dependent-processes"><i class="fa fa-check"></i><b>3.3.2</b> CLT for Dependent Processes</a></li>
<li class="chapter" data-level="3.3.3" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#asymptotic-normality-of-sample-autocovariance-and-autocorrelation-functions"><i class="fa fa-check"></i><b>3.3.3</b> Asymptotic Normality of Sample AutoCovariance and AutoCorrelation Functions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html"><i class="fa fa-check"></i><b>4</b> Allan Variance Calibration Techniques</a><ul>
<li class="chapter" data-level="4.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#review-on-mle-based-methods"><i class="fa fa-check"></i><b>4.1</b> Review on MLE-based Methods</a></li>
<li class="chapter" data-level="4.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#an-introduction-of-the-allan-variance"><i class="fa fa-check"></i><b>4.2</b> An Introduction of the Allan Variance</a><ul>
<li class="chapter" data-level="4.2.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#definition-of-the-allan-variance"><i class="fa fa-check"></i><b>4.2.1</b> Definition of the Allan Variance</a></li>
<li class="chapter" data-level="4.2.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#spectral-ambiguity-of-the-allan-variance"><i class="fa fa-check"></i><b>4.2.2</b> Spectral Ambiguity of the Allan Variance</a></li>
<li class="chapter" data-level="4.2.3" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#properties-of-the-allan-variance"><i class="fa fa-check"></i><b>4.2.3</b> Properties of the Allan Variance</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#estimation-of-the-allan-variance"><i class="fa fa-check"></i><b>4.3</b> Estimation of the Allan Variance</a><ul>
<li class="chapter" data-level="4.3.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#consistency-of-the-moav-estimator"><i class="fa fa-check"></i><b>4.3.1</b> Consistency of the MOAV Estimator</a></li>
<li class="chapter" data-level="4.3.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#asymptotic-normality-of-the-moav-estimator"><i class="fa fa-check"></i><b>4.3.2</b> Asymptotic Normality of the MOAV Estimator</a></li>
<li class="chapter" data-level="4.3.3" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#confidence-interval-of-the-moav-estimator"><i class="fa fa-check"></i><b>4.3.3</b> Confidence Interval of the MOAV Estimator</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#allan-variance-based-estimation"><i class="fa fa-check"></i><b>4.4</b> Allan Variance-based Estimation</a><ul>
<li class="chapter" data-level="4.4.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#allan-variance-log-log-representation"><i class="fa fa-check"></i><b>4.4.1</b> Allan Variance log-log Representation</a></li>
<li class="chapter" data-level="4.4.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#allan-variance-based-estimation-1"><i class="fa fa-check"></i><b>4.4.2</b> Allan Variance-based Estimation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-generalized-method-of-wavelet-moments.html"><a href="the-generalized-method-of-wavelet-moments.html"><i class="fa fa-check"></i><b>5</b> The Generalized Method of Wavelet Moments</a><ul>
<li class="chapter" data-level="5.1" data-path="the-generalized-method-of-wavelet-moments.html"><a href="the-generalized-method-of-wavelet-moments.html#the-wavelet-variance"><i class="fa fa-check"></i><b>5.1</b> The Wavelet Variance</a></li>
<li class="chapter" data-level="5.2" data-path="the-generalized-method-of-wavelet-moments.html"><a href="the-generalized-method-of-wavelet-moments.html#the-gmwm"><i class="fa fa-check"></i><b>5.2</b> The GMWM</a><ul>
<li class="chapter" data-level="5.2.1" data-path="the-generalized-method-of-wavelet-moments.html"><a href="the-generalized-method-of-wavelet-moments.html#choice-of-weighting-matrix"><i class="fa fa-check"></i><b>5.2.1</b> Choice of weighting matrix</a></li>
<li class="chapter" data-level="5.2.2" data-path="the-generalized-method-of-wavelet-moments.html"><a href="the-generalized-method-of-wavelet-moments.html#consistency-1"><i class="fa fa-check"></i><b>5.2.2</b> Consistency</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="extensions.html"><a href="extensions.html"><i class="fa fa-check"></i><b>6</b> Extensions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Inertial Sensors Stochastic Calibration</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="timeseries" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Introduction to Time Series Analysis</h1>
<p>In this chapter we give an introduction to time series analysis. For this purpose, it is organized in the following order:</p>
<ul>
<li>Definition and descriptive analysis of time series;</li>
<li>Dependence within time series (and fundamental representations);</li>
<li>Stationarity of time series;</li>
<li>Basic time series models;</li>
<li>Linear processes;</li>
<li>Latent (or composite) stochastic processes;</li>
<li>Estimation problems with time series.</li>
</ul>
<div id="time-series" class="section level2">
<h2><span class="header-section-number">2.1</span> Time Series</h2>

<div class="definition">
<p><span id="def:ts" class="definition"><strong>Definition 2.1  (Time Series)  </strong></span>A time series is a stochastic process (i.e.Â a sequence of random variables) defined on a common probability space. Let us denote this time-indexed stochastic process (time series) as <span class="math inline">\((X_t)_{t = 1,...,T}\)</span>, i.e. (<span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, â¦., <span class="math inline">\(X_T\)</span>), where the time <span class="math inline">\(t\)</span> belongs to the discrete index set. Therefore, we implicitly assume that</p>
<ul>
<li><span class="math inline">\(t\)</span> is non-random, i.e.Â the time at which each observation is measured is known, and</li>
<li>the time between two consecutive observations is constant (i.e.Â sampling occurs at regular intervals).
</div>
</li>
</ul>
<p>As for any data analysis procedure, the first step consists in representing the data in such a way as to highlight any important features or information that should be taken into account for the following statistical analysis. For time series, a typical first step is representing the observations over time (where the latter is represented on the x-axis and values of <span class="math inline">\(X_t\)</span> on the y-axis). This can be considered as the first step for the <strong>descriptive analysis</strong> of a time series, especially when their length is moderate.</p>
<p>With this in mind, when performing a descriptive analysis of a time series it is customary to check the following aspects:</p>
<ul>
<li>Trends
<ul>
<li>Seasonal (e.g.Â business cycles)</li>
<li>Non-seasonal (e.g.Â impact of economic indicators on stock returns)</li>
<li>Local fluctuations (e.g.Â vibrations observed before, during and after an earthquake)</li>
</ul></li>
<li>Changes in the statistical properties
<ul>
<li>Mean (e.g.Â economic crisis)</li>
<li>Variance (e.g.Â earnings)</li>
<li>States (e.g.Â bear/bull in finance)</li>
</ul></li>
<li>Model deviations (e.g.Â outliers)</li>
</ul>
<p>In order to give an idea of what the above characteristics imply for a time series, the following examples provide practical insight for some of them.</p>

<div class="example">
<span id="exm:exampleJJ" class="example"><strong>Example 2.1  (Johnson and Johnson Quarterly Earnings)  </strong></span>A first example of a time series is the quarterly earnings of the company Johnson and Johnson. In the graph below, we present these earnings between 1960 and 1980.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load simts package</span>
<span class="kw">library</span>(simts)

<span class="co"># Load data</span>
<span class="kw">data</span>(jj, <span class="dt">package =</span> <span class="st">&quot;astsa&quot;</span>)

<span class="co"># Construct gts object</span>
jj =<span class="st"> </span><span class="kw">gts</span>(jj, <span class="dt">start =</span> <span class="dv">1960</span>, <span class="dt">freq =</span> <span class="dv">4</span>)

<span class="co"># Plot time series</span>
<span class="kw">plot</span>(jj, <span class="dt">main =</span> <span class="st">&quot;Johnson and Johnson Quarterly Earnings&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;Time (year)&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Quarterly Earnings per Share ($)&quot;</span>) </code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-1"></span>
<img src="scis_files/figure-html/unnamed-chunk-1-1.png" alt="Johnson and Johnson Quarterly Earnings" width="624" />
<p class="caption">
Figure 2.1: Johnson and Johnson Quarterly Earnings
</p>
</div>
As we can see from the plot, the data contains a non-linear increasing trend as well as a seasonal component highlighted by the almost regularly-spaced peaks and valleys along time. In addition, we can notice that the variability of the data seems to increase with time (the seasonal variations appear to be larger towards the end of the plot). Hence, this simple visual representation can deliver important insight as to the behaviour of the time series and consequently determine the steps to take for further analysis (e.g.Â consider a non-linear model to explain the trend and consider approaches to model changing variance).
<div style="text-align: right">
<span class="math inline">\(\LARGE{\bullet}\)</span>
</div>
<p><br></p>

<div class="example">
<span id="exm:examplePrecipitation" class="example"><strong>Example 2.2  (Monthly Precipitation Data)  </strong></span>Let us consider another data set coming from the domain of hydrology. The data records monthly precipitation (in mm) over a certain period of time (1907 to 1972) and is interesting for hydrologists for the purpose of studying water cycles. The data are presented in the plot below:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load data</span>
<span class="kw">data</span>(hydro, <span class="dt">package =</span> <span class="st">&quot;simts&quot;</span>)

<span class="co"># Construct gts object</span>
hydro =<span class="st"> </span><span class="kw">gts</span>(hydro, <span class="dt">start =</span> <span class="dv">1907</span>, <span class="dt">freq =</span> <span class="dv">12</span>)

<span class="co"># Plot time series</span>
<span class="kw">plot</span>(hydro, <span class="dt">main =</span> <span class="st">&quot;Monthly Precipitation Data&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;Time (year)&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Mean Monthly Precipitation (mm)&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-2"></span>
<img src="scis_files/figure-html/unnamed-chunk-2-1.png" alt="Monthly Precipitation Data" width="624" />
<p class="caption">
Figure 2.2: Monthly Precipitation Data
</p>
</div>
The time series plot above differs considerably from the previous one since the values of the time series, being always non-negative, remain almost always between 0 and 1 (no apparent trend) and randomly shows some larger observations that go beyond the value of 2 (or even 3). The latter appear to be extreme observations which could qualify as outliers, i.e.Â observations that are not representative of the true underlying model that generates the time series and can considerably affect the statistical analysis if not dealt with.
<div style="text-align: right">
<span class="math inline">\(\LARGE{\bullet}\)</span>
</div>
<p><br></p>

<div class="example">
<span id="exm:exampleInertialSensor" class="example"><strong>Example 2.3  (Inertial Sensor Data)  </strong></span>Another example is provided by the data coming from the calibration procedure of an Inertial Measurement Unit (IMU). The signals (or time series) coming from an IMU are usually measured at high frequencies over a long time and are often characterized by linear trends and numerous underlying stochastic processes. The plot below represents the time series of an error signal coming from a gyroscope belonging to an IMU.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load data</span>
<span class="kw">data</span>(imu6, <span class="dt">package =</span> <span class="st">&quot;imudata&quot;</span>)

<span class="co"># Construct gts object</span>
imu =<span class="st"> </span><span class="kw">gts</span>(imu6[,<span class="dv">1</span>], <span class="dt">freq =</span> <span class="dv">100</span><span class="op">*</span><span class="dv">60</span><span class="op">*</span><span class="dv">60</span>)

<span class="co"># Plot time series</span>
<span class="kw">plot</span>(imu, <span class="dt">main =</span> <span class="st">&quot;Inertial Sensor Data&quot;</span>,
     <span class="dt">ylab =</span> <span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&quot;Angular Velocity &quot;</span>, (rad<span class="op">/</span>s))),
     <span class="dt">xlab =</span> <span class="st">&quot;Time (h)&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-3"></span>
<img src="scis_files/figure-html/unnamed-chunk-3-1.png" alt="Inertial Sensor Data" width="624" />
<p class="caption">
Figure 2.3: Inertial Sensor Data
</p>
</div>
As we can see from the plot, there wouldnât appear to be any linear trend, seasonality or increased variation in the time series. Indeed, from this plot it is difficult to detect any particular characteristic of this time series although a linear trend and other processes are present in this data. Therefore, especially when the length of the time series is considerable, this representation of time series can only give insight into few aspects (if none) and consequently is not suitable, on its own, to adequately inform the subsequent statistical analysis.
<div style="text-align: right">
<span class="math inline">\(\LARGE{\bullet}\)</span>
</div>
<p><br></p>
<p>In order to deliver a more appropriate (or more complete) representation of a time series, it is important to study the concept of dependence since (in a linear vision of time) past observations have an influence on present and (possibly) future ones. Hence, the next section gives an overview of this concept.</p>
</div>
<div id="dependence-within-time-series" class="section level2">
<h2><span class="header-section-number">2.2</span> Dependence within Time Series</h2>
<p>As mentioned above, it is straightforward to assume that observations measured through time are dependent on each other (in that observations at time <span class="math inline">\(t\)</span> have some form of impact on observations at time <span class="math inline">\(t+1\)</span> or beyond). Due to this characteristic, one of the main interests in time series is prediction where, if <span class="math inline">\((X_t)_{t=1,\ldots,T}\)</span> is an identically distributed but not independent sequence, we often want to know the value of <span class="math inline">\({X}_{T+h}\)</span> for <span class="math inline">\(h &gt; 0\)</span> (i.e.Â an estimator of <span class="math inline">\(\mathbb{E}[X_{T+h}| X_T,...]\)</span>). In order to tackle this challenge, we first need to understand the dependence between <span class="math inline">\(X_{1},\ldots,X_{T}\)</span> and, even before this, we have to formally define what <strong>independence</strong> is.</p>

<div class="definition">
<span id="def:IndepEvents" class="definition"><strong>Definition 2.2  (Independence of Events)  </strong></span>Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent if
<span class="math display">\[\begin{align*}
\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B),
\end{align*}\]</span>
with <span class="math inline">\(\mathbb{P}(A)\)</span> denoting the probability of event <span class="math inline">\(A\)</span> occuring and <span class="math inline">\(\mathbb{P}(A \cap B)\)</span> denoting the joint probability (i.e.Â the probability that events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> occur jointly). In general, <span class="math inline">\(A_{1},\ldots,A_{n}\)</span> are independent if
<span class="math display">\[\begin{align*}
\mathbb{P}(A_1 \ldots A_n) = \mathbb{P}(A_1) \ldots \mathbb{P}(A_n) \;\; \forall \; A_i \in S, \;\; i=1,\ldots,n
\end{align*}\]</span>
where <span class="math inline">\(S\)</span> is the sample space.
</div>
<p> <br></p>

<div class="definition">
<span id="def:IndepRV" class="definition"><strong>Definition 2.3  (Independence of Random Variables)  </strong></span>Two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> with Cumulative Distribution Functions (CDF) <span class="math inline">\(F_X(x)\)</span> and <span class="math inline">\(F_Y(y)\)</span>, respectively, are independent if and only if their joint CDF <span class="math inline">\(F_{X,Y}(x,y)\)</span> is such that
<span class="math display">\[\begin{align*}
F_{X,Y}(x,y) = F_{X}(x) F_{Y}(y).
\end{align*}\]</span>
In general, random variables <span class="math inline">\(X_1, \ldots, X_n\)</span> with CDF <span class="math inline">\(F_{X_1}(x_1), \ldots, F_{X_n}(x_n)\)</span> are respectively independent if and only if their joint CDF <span class="math inline">\(F_{X_1, \ldots, X_n}(x_1, \ldots, x_n)\)</span> is such that
<span class="math display">\[\begin{align*}
F_{X_1,\ldots,X_n}(x_1,\ldots,x_n) = F_{X_1}(x_1) \ldots F_{X_n}(x_n).
\end{align*}\]</span>
</div>
<p> <br></p>

<div class="definition">
<span id="def:iid" class="definition"><strong>Definition 2.4  (iid sequence)  </strong></span>The sequence <span class="math inline">\(X_{1},X_{2},\ldots,X_{T}\)</span> is said to be independent and identically distributed (i.e.Â iid) if and only if
<span class="math display">\[\begin{align*}
\mathbb{P}(X_{i}&lt;x) = \mathbb{P}(X_{j}&lt;x) \;\; \forall x \in \mathbb{R}, \forall i,j \in \{1,\ldots,T\},
\end{align*}\]</span>
and
<span class="math display">\[\begin{align*}
\mathbb{P}(X_{1}&lt;x_{1},X_{2}&lt;x_{2},\ldots,X_{T}&lt;x_{T})=\mathbb{P}(X_{1}&lt;x_1) \ldots \mathbb{P}(X_{T}&lt;x_T) \;\; \forall T\geq2, x_1, \ldots, x_T \in \mathbb{R}.
\end{align*}\]</span>
</div>
<p> <br></p>
<p>The basic idea behind the above definitions of independence is the fact that the probability of an event regarding variable <span class="math inline">\(X_i\)</span> remains unaltered no matter what occurs for variable <span class="math inline">\(X_j\)</span> (for <span class="math inline">\(i \neq j\)</span>). From this definition, we can now start exploring the concept of dependence, starting from <strong>linear dependence</strong> within a time series. For this purpose, below we define a quantity called AutoCovariance (ACV).</p>

<div class="definition">
<span id="def:ACV" class="definition"><strong>Definition 2.5  (AutoCovariance)  </strong></span>Autocovariance denoted as <span class="math inline">\(\gamma_X(t, t+h)\)</span> is defined as
<span class="math display">\[\begin{align*}
\gamma_X(t, t+h) = \text{Cov}(X_{t},X_{t+h})=   \mathbb{E}(X_{t}X_{t+h})-\mathbb{E}(X_{t})\mathbb{E}(X_{t+h}),
\end{align*}\]</span>
where <span class="math inline">\(\text{Cov}(\cdot)\)</span> denotes the covariance and
<span class="math display">\[\begin{align*}
\mathbb{E}(X_{t}) = \int_{-\infty}^{\infty}x \, f(x) \, dx \;\; \text{and} \;\; \mathbb{E}(X_{t},X_{t+h}) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}x_{t}\,x_{t+h}\, f(x_{t},x_{t+h}) \, dx_{t}\,dx_{t+h},
\end{align*}\]</span>
where <span class="math inline">\(f(x)\)</span> denotes the density of <span class="math inline">\(X_t\)</span> and <span class="math inline">\(f(x_{t},x_{t+h})\)</span> denotes the joint density of <span class="math inline">\(X_{t}\)</span> and <span class="math inline">\(X_{t+h}\)</span>.
</div>
<p> <br></p>
<p>Notice that, when two variable are independent, <span class="math inline">\(\mathbb{E}(X_{t}X_{t+h}) = \mathbb{E}(X_{t})\mathbb{E}(X_{t+h})\)</span> and hence <span class="math inline">\(\gamma_X(t, t+h) = 0\)</span>. In a nutshell, the ACV measures the degree to which the mean-behaviour of a variable (e.g. <span class="math inline">\(X_{t+h}\)</span>) changes when another one changes (e.g. <span class="math inline">\(X_t\)</span>) and hence, to what extent they are linearly dependent.</p>

<div class="exercise">
<p><span id="exr:propertiesACV" class="exercise"><strong>Remark 2.1  (Properties of ACV)  </strong></span></p>
<ol style="list-style-type: decimal">
<li><p>ACV is symmetric, i.e. <span class="math inline">\(\gamma_X(t, t+h) = \gamma_X(t+h, t)\)</span> as <span class="math inline">\(\text{Cov}(X_{t},X_{t+h}) = \text{Cov}(X_{t+h},X_{t})\)</span>. Under stationarity (will be discussed very soon, see Section <a href="timeseries.html#stationarity">2.3</a> for more details), <span class="math inline">\(\gamma_X(h) = \gamma_X(-h)\)</span>, i.e.Â ACV is an even function.</p></li>
<li><p>Variance of the process <span class="math inline">\(\text{Var}(X_t) = \gamma_X(t, t) \geq 0\)</span>. Under stationarity, <span class="math inline">\(\text{Var}(X_t) = \gamma_X(0)\)</span> and <span class="math inline">\(\mid \gamma_X(h) \mid \leq \gamma_X(0)\)</span> by Cauchy-Schwarz inequality.</p></li>
<li><p>Scale dependent: ACV <span class="math inline">\(\gamma_X(t, t+h)\)</span> is scale dependent like any covariance. So <span class="math inline">\(\gamma_X(t, t+h) \in \mathbb{R}\)</span>.</p>
<ul>
<li>If <span class="math inline">\(\mid \gamma_X(t, t+h) \mid\)</span> is âcloseâ to 0, then <span class="math inline">\(X_{t}\)</span> and <span class="math inline">\(X_{t+h}\)</span> are âless (linearly) dependentâ.</li>
<li>If <span class="math inline">\(\mid \gamma_X(t, t+h) \mid\)</span> is âfarâ from 0, then <span class="math inline">\(X_{t}\)</span> and <span class="math inline">\(X_{t+h}\)</span> are âmore (linearly) dependentâ.</li>
</ul></li>
</ol>
<p>However in general, it is difficult to assess what âcloseâ and âfarâ from zero mean.</p>
<ol start="4" style="list-style-type: decimal">
<li>In general, <span class="math inline">\(\gamma_X(t, t+h)=0\)</span> does not imply <span class="math inline">\(X_{t}\)</span> and <span class="math inline">\(X_{t+h}\)</span> are independent. However, if <span class="math inline">\(X_{t}\)</span> and <span class="math inline">\(X_{t+h}\)</span> are joint normally distributed, then <span class="math inline">\(\gamma_X(t, t+h)=0\)</span> implies that <span class="math inline">\(X_{t}\)</span> and <span class="math inline">\(X_{t+h}\)</span> are independent.
</div>
 <br></li>
</ol>
<p>However the ACV is a quantity whose bounds are not known <em>a priori</em> and it can consequently be impossible to determine whether the degree of linear dependence is large or not. For this reason another measure of linear dependence can be used which is related to the ACV. Indeed, the AutoCorrelation Function (ACF) is a commonly used metric in time series analysis and is defined below.</p>

<div class="definition">
<span id="def:ACF" class="definition"><strong>Definition 2.6  (AutoCorrelation)  </strong></span>AutoCorrelation (ACF) denoted as <span class="math inline">\(\rho_X(t, t+h)\)</span> is defined as
<span class="math display">\[\begin{align*}
\rho_X(t,t+h) = \text{Corr}(X_{t},X_{t+h}) = \frac{\text{Cov}(X_{t},X_{t+h})}{\sqrt{\text{Var}(X_{t})} \sqrt{\text{Var}(X_{t+h})}},
\end{align*}\]</span>
where <span class="math inline">\(\text{Var}(\cdot)\)</span> denotes the variance.
</div>
<p> <br></p>

<div class="exercise">
<p><span id="exr:propertiesACF" class="exercise"><strong>Remark 2.2  (Properties of ACF)  </strong></span></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mid \rho_X(t,t+h) \mid \leq 1\)</span> and <span class="math inline">\(\mid \rho_X(t,t) \mid = 1\)</span>.</p></li>
<li><p>ACF is symmetric, i.e. <span class="math inline">\(\rho_X(t, t+h) = \rho_X(t+h, t)\)</span> as <span class="math inline">\(\text{Corr}(X_{t},X_{t+h}) = \text{Corr}(X_{t+h},X_{t})\)</span>. Under stationarity, <span class="math inline">\(\gamma_X(h) = \gamma_X(-h)\)</span>, i.e.Â ACF is an even function.</p></li>
<li>Scale invariant: ACF <span class="math inline">\(\rho_X(t,t+h)\)</span> is scale free like any correlation. Moreover, if <span class="math inline">\(\rho_X(t,t+h)\)</span> is âcloseâ to <span class="math inline">\(\pm 1\)</span>, then this implies that there is âstrongâ (linear) dependence between <span class="math inline">\(X_{t}\)</span> and <span class="math inline">\(X_{t+h}\)</span>.
</div>
<p> <br></p></li>
</ol>
<p>Since the ACV is bounded by the product of the standard deviations of the two variables, we have that the ACF is bounded between 1 and -1. Therefore it is possible to better interpret the linear dependence where an ACF of zero indicates an absence of linear dependence while an ACF close to 1 or -1 indicates respectively a strong positive or negative linear dependence.</p>
<p>As underlined up to now, both ACV and ACF are appropriate to measure linear dependence only. Therefore if the ACV and ACF are both zero, this does not imply that there isnât dependence between the variables. Indeed, aside from linear dependence, other forms of dependence such as monotonic or nonlinear dependence also exist which the ACV or ACF donât measure directly. The figure below shows scatterplots (one variable on the x-axis and another on the y-axis) along with the values of the ACF:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-4"></span>
<img src="images/Cor.png" alt="Different forms of dependence and their ACF values" width="80%" />
<p class="caption">
Figure 2.4: Different forms of dependence and their ACF values
</p>
</div>
<p>As can be seen, the first row in the figure shows a consistent behaviour of the ACF value since the absolute value goes closer to 1 the more the points go closer to forming a line. However, the following rows show plots where there is clearly a strong relation (dependence) between the variables but the ACF doesnât detect this characteristic since this relation is not linear.</p>
<p>Finally, it is worth noting that correlation does NOT imply causation. For example, if <span class="math inline">\(\rho(t, t+h) \neq 0\)</span>, this does not imply that <span class="math inline">\(X_t \to X_{t+h}\)</span> is causal. More specifically, the presence (or absence) of causality cannot be detected through statistical tools although some approximated metrics have been proposed to measure this concept <span class="citation">(e.g Granger causality, see Granger <a href="#ref-granger1969investigating">1969</a>)</span>.</p>
<p>For the following sections we will simplify the notations <span class="math inline">\(\gamma_X(t, t+h)\)</span> and <span class="math inline">\(\rho_X(t, t+h)\)</span> to be <span class="math inline">\(\gamma(h)\)</span> and <span class="math inline">\(\rho(h)\)</span> when there is no ambiguity (i.e.Â only one time series is considered and the ACF only depends on the time-lag <span class="math inline">\(h\)</span>).</p>
</div>
<div id="stationarity" class="section level2">
<h2><span class="header-section-number">2.3</span> Stationarity</h2>
<p>In this section we are going to introduce the concept of stationarity, one of the most important characteristics of time series data. First let us consider an example of <strong>non-stationary processes</strong>.</p>

<div class="example">
<span id="exm:exampleNonStationary" class="example"><strong>Example 2.4  (Non-Stationary Process)  </strong></span>
<span class="math display">\[\begin{equation*}
X_t  \sim \mathcal{N} \left(0, Y_t^2\right) \;\; \text{where $Y_t$ is unobserved and such that} \;\; Y_t  \overset{iid}{\sim} \mathcal{N} \left(0, 1\right).
\end{equation*}\]</span>
<p>In this case, it is clear that the estimation of <span class="math inline">\(\text{Var}(X_t)\)</span> is difficult since only <span class="math inline">\(X_t\)</span> is useful for the estimation. So in fact, <span class="math inline">\(X_t^2\)</span> is our best guess for <span class="math inline">\(\text{Var}(X_t)\)</span>.</p>
<div style="text-align: right">
<span class="math inline">\(\LARGE{\bullet}\)</span>
</div>
</div>

<p>On the other hand, let us consider an example of <strong>stationary processes</strong> where averaging becomes meaningful for such process.</p>

<div class="example">
<span id="exm:exampleStationary" class="example"><strong>Example 2.5  (Stationary Process)  </strong></span>
<span class="math display">\[\begin{equation*}
X_t = \theta W_{t-1} + W_t \;\;\; \text{where} \;\;\;  W_t  \stackrel{iid}{\sim} \mathcal{N} \left(0, 1\right).
\end{equation*}\]</span>
<p>In this case, we can guess that a natural estimator of <span class="math inline">\(\text{Var}(X_t)\)</span> can be <span class="math inline">\(\hat{\sigma}^2 = \frac{1}{T} \sum_{i = 1}^T X_i^2\)</span>. That is, now averages are meaningful for such process.</p>
<div style="text-align: right">
<span class="math inline">\(\LARGE{\bullet}\)</span>
</div>
</div>

<p>We formalize the above idea by introducing the concept of stationarity. There exist two forms of stationarity, which are defined below:</p>

<div class="definition">
<span id="def:StrongStationarity" class="definition"><strong>Definition 2.7  (Strong Stationarity)  </strong></span>The time series <span class="math inline">\(X_{t}\)</span> is strongly stationary if the joint probability distribution is invariant under a shift in time, i.e.
<span class="math display">\[\begin{equation*}
\mathbb{P}(X_{t}\leq x_{0},\ldots,X_{t+k}\leq x_{k}) = \mathbb{P}(X_{t+h}\leq x_{0} ,\ldots,X_{t+h+k}\leq x_{k})
\end{equation*}\]</span>
for any time shift <span class="math inline">\(h\)</span> and any <span class="math inline">\(x_{0}, x_{1},x_{2},\cdots,x_{k}\)</span> belong to the domain of <span class="math inline">\(X_t,\cdots,X_{t+k}\)</span> and <span class="math inline">\(X_{t+h},\cdots,X_{t+h+k}\)</span>.
</div>


<div class="definition">
<span id="def:WeakStationarity" class="definition"><strong>Definition 2.8  (Weak Stationarity)  </strong></span>The time series <span class="math inline">\((X_{t})_{t \in \mathbb{N}}\)</span> is weakly stationary if the mean and autocovariance are finite and invariant under a shift in time, i.e.
<span class="math display">\[\begin{equation*}
\begin{aligned}
\mathbb{E}\left[X_t\right] &amp;= \mu &lt; \infty,\\
\mathbb{E}\left[X_t^2\right]  &amp;= \mu_2 &lt; \infty,\\
\text{Cov}(X_{t},X_{t+h})&amp;= \text{Cov}(X_{t + k},X_{t+h + k}) = \gamma( h ).
\end{aligned}
\end{equation*}\]</span>
for any time shift <span class="math inline">\(h\)</span>. For convenience, we use the abbreviation âstationaryâ to indicate âweakly stationaryâ by default.
</div>

<p>The stationarity of <span class="math inline">\(X_{t}\)</span> is important because it provides a framework in which averaging makes sense. The concept of averaging is essentially meaningless unless properties like mean and covariance are either fixed or evolve in a known manner.</p>

<div class="exercise">
<span id="exr:StatImpAcvAcf" class="exercise"><strong>Remark 2.3  (Implication on the ACV and ACF)  </strong></span>If a process is weakly stationary or strongly stationary and <span class="math inline">\(\text{Cov}(X_{t},X_{t+h})\)</span> exists for all <span class="math inline">\(h \in \mathbb{Z}\)</span>, then we have both ACV and ACF only depend on the lag between observations, i.e.
<span class="math display">\[\begin{equation*}
\begin{aligned}
\gamma(t, t+h) &amp;= \text{Cov}(X_{t},X_{t+h})= \text{Cov}(X_{t + k},X_{t+h + k}) = \gamma(t+k, t+h+k) = \gamma(h),\\
\rho(t, t+h) &amp;= \text{Corr}(X_{t},X_{t+h})= \text{Corr}(X_{t + k},X_{t+h + k}) = \rho(t+k, t+h+k) = \rho(h).
\end{aligned}
\end{equation*}\]</span>
</div>


<div class="exercise">
<p><span id="exr:RelationStationary" class="exercise"><strong>Remark 2.4  (Relation between Strong and Weak Stationarity)  </strong></span></p>
<p>In general, neither type of stationarity implies the other one. However,</p>
<ul>
<li>If <span class="math inline">\(X_{t}\)</span> is Normal (Gaussian) with <span class="math inline">\(\sigma^2 = \text{Var} (X_{t}) &lt; \infty\)</span>, then weak stationarity implies strong stationarity.</li>
<li>If <span class="math inline">\(X_{t}\)</span> is strongly stationary, <span class="math inline">\(\mathbb{E}(X_t) &lt; \infty\)</span> and <span class="math inline">\(\mathbb{E}(X_t^2) &lt; \infty\)</span>, then <span class="math inline">\(X_{t}\)</span> is weakly stationary.
</div>
</li>
</ul>

<div class="example">
<p><span id="exm:exampleRelationStat1" class="example"><strong>Example 2.6  (Strong Stationarity does NOT imply Weak Stationarity)  </strong></span>An iid Cauchy process is strongly but not weakly stationary as the mean of the process does not exist.</p>
<div style="text-align: right">
<span class="math inline">\(\LARGE{\bullet}\)</span>
</div>
</div>


<div class="example">
<span id="exm:exampleRelationStat2" class="example"><strong>Example 2.7  (Weak Stationarity does NOT imply Strong Stationarity)  </strong></span>Let <span class="math inline">\(X_t \overset{iid}{\sim} \exp(1)\)</span> (i.e.Â exponential distribution with <span class="math inline">\(\lambda = 1\)</span>) and <span class="math inline">\(Y_t \overset{iid}{\sim} \mathcal{N}(1,1)\)</span>. Then, let
<span class="math display">\[\begin{equation*}
Z_t = \left\{
\begin{array}{cl}
X_t &amp;\text{if } t \in \left\{2k | k \in \mathbb{N}\right\}\\
Y_t &amp;\text{if }  t \in \left\{2k + 1 | k \in \mathbb{N}\right\},
\end{array}
\right.
\end{equation*}\]</span>
<p>we have <span class="math inline">\(Z_t\)</span> is weakly stationary but not strongly stationary.</p>
<div style="text-align: right">
<span class="math inline">\(\LARGE{\bullet}\)</span>
</div>
</div>


<div class="exercise">
<p><span id="exr:StatProcesses" class="exercise"><strong>Remark 2.5  (Stationarity of Latent Time Series Processes)  </strong></span></p>
<ul>
<li>(Weakly) Stationary: WN, QN, AR1</li>
<li>(Weakly) Non-Stationary: DR, RW
</div>
</li>
</ul>

<div class="proof">
 <span class="proof"><em>Proof</em> (AR1 is weakly stationary). </span> Consider an AR1 process defined as:
<span class="math display">\[\begin{equation*}
X_t = \phi X_{t-1} + Z_t, \;\;\; Z_t \overset{iid}{\sim} \mathcal{N}(0,\nu^2),
\end{equation*}\]</span>
where <span class="math inline">\(\mid \phi \mid &lt; 1\)</span> and <span class="math inline">\(\nu^2 &lt; \infty\)</span>. Then we have <span class="math display">\[\begin{aligned}
{X_t}  &amp;=   {\phi }{X_{t - 1}} + {Z_t} = \phi \left[ {\phi {X_{t - 2}} + {Z_{t - 1}}} \right] + {Z_t} =  {\phi ^2}{X_{t - 2}} + \phi {Z_{t - 1}} + {Z_t}  \\
&amp; \; \vdots  \\
&amp;=   {\phi ^k}{X_{t-k}} + \sum\limits_{j = 0}^{k - 1} {{\phi ^j}{Z_{t - j}}} .
\end{aligned} \]</span> By taking the limit in <span class="math inline">\(k\)</span> (which is perfectly valid as we assume <span class="math inline">\(t \in \mathbb{Z}\)</span>), we obtain
<span class="math display">\[\begin{equation*}
\begin{aligned}
X_t = \mathop {\lim }\limits_{k \to \infty} \; {X_t}  =  \sum\limits_{j = 0}^{\infty} {{\phi ^j}{Z_{t - j}}}.
\end{aligned}
\end{equation*}\]</span>
So we have
<span class="math display">\[\begin{equation*}
\begin{aligned}
\mathbb{E}\left[X_t\right] &amp;=  \sum\limits_{j = 0}^{\infty} {{\phi ^j}{\mathbb{E} [Z_{t - j}]}} = 0, \\
\text{Var}\left(X_t\right) &amp;= \text{Var}\left(\sum\limits_{j = 0}^{\infty} {{\phi ^j}{Z_{t - j}}}\right) = \sum\limits_{j = 0}^{\infty} {\phi^{2j}} \text{Var}\left(Z_{t-j}\right) = \nu^2 \sum\limits_{j = 0}^{\infty} {\phi^{2j}} = \frac{\nu^2}{1-\phi^2} &lt; \infty.
\end{aligned}
\end{equation*}\]</span>
Moreover, assuming for notational simplicity that <span class="math inline">\(h &gt; 1\)</span>, we obtain
<span class="math display">\[\begin{equation*}
\begin{aligned}
\text{Cov}\left(X_t, X_{t+h}\right) &amp;= \phi \text{Cov}\left(X_t, X_{t+h-1}\right) = \phi^2 \text{Cov}\left(X_t, X_{t+h-2}\right) = \ldots = \phi^h \text{Cov}(X_t, X_t).
\end{aligned}
\end{equation*}\]</span>
In general, when <span class="math inline">\(h \in \mathbb{Z}\)</span> we obtain
<span class="math display">\[\begin{equation*}
\begin{aligned}
\text{Cov}\left(X_t, X_{t+h}\right) &amp; = \phi^{|h|} \text{Cov}(X_t, X_t) = \phi^{|h|} \frac{\nu^2}{1-\phi^2},
\end{aligned}
\end{equation*}\]</span>
which is a function of the lag <span class="math inline">\(h\)</span> only. Therefore, this AR1 process is weakly stationary.
</div>
<p> <br></p>
</div>
<div id="linear-processes" class="section level2">
<h2><span class="header-section-number">2.4</span> Linear Processes</h2>
<p>In this section we introduce the concept of linear processes. As a matter of fact, considered stationary models can all, so far, be represented as linear processes.</p>

<div class="definition">
<span id="def:LinearProc" class="definition"><strong>Definition 2.9  (Linear Processes)  </strong></span>A stochastic process <span class="math inline">\((X_t)\)</span> is said to be a linear process if it can be expressed as a linear combination of an iid Gaussian sequence (i.e.Â white noise process), i.e.: <span class="math display">\[{X_t} = \mu + \sum\limits_{j =  - \infty }^\infty  {{\psi _j}{W_{t - j}}} \]</span> where <span class="math inline">\(W_t \overset{iid}{\sim} \mathcal{N}(0, \sigma^2)\)</span> and <span class="math inline">\(\sum\limits_{j = - \infty }^\infty {\left| {{\psi _j}} \right|} &lt; \infty\)</span>.
</div>

<p>Notice that the condition <span class="math inline">\(\sum\limits_{j = - \infty }^\infty {\left| {{\psi _j}} \right|} &lt; \infty\)</span> is required in the definition of linear processes in order to ensure that the series has a limit and is related to the absolutely summable covariance structure, which is defined below.</p>

<div class="definition">
<span id="def:AbsSumCov" class="definition"><strong>Definition 2.10  (Absolutely Summable Covariance Structure)  </strong></span>A process <span class="math inline">\((X_t)\)</span> is said to have an absolutely summable covariance structure if <span class="math inline">\(\sum\limits_{h = - \infty }^\infty {\left| \gamma_X(h) \right|} &lt; \infty\)</span>.
</div>


<div class="exercise">
<p><span id="exr:propertiesLinearProc" class="exercise"><strong>Remark 2.6  (Properties of Linear Processes)  </strong></span></p>
<ol style="list-style-type: decimal">
<li><p>All linear processes are stationary since <span class="math display">\[\begin{aligned}
\mathbb{E}[X_t] &amp;= \mu, \\
\gamma(h) &amp;= \sigma^2\sum\limits_{j =  - \infty }^\infty  {{\psi _j}{\psi _{j+h}}}.
\end{aligned}\]</span></p></li>
<li>All linear processes have absolutely summable covariance structures.
</div>
</li>
</ol>

<div class="proof">
 <span class="proof"><em>Proof</em> (ACV of Linear Processes). </span> 
<span class="math display">\[\begin{align*}
\gamma(h) &amp;= \text{Cov}(X_t, X_{t+h}) = \text{Cov}(\mu+\sum_{j=-\infty}^{\infty} \psi_j W_{t-j}, \mu+\sum_{j=-\infty}^{\infty} \psi_j W_{t+h-j})\\
&amp;= \text{Cov}(\sum_{j=-\infty}^{\infty} \psi_j W_{t-j}, \sum_{j=-\infty}^{\infty} \psi_j W_{t-(j-h)}) \\
&amp;= \text{Cov}(\sum_{j=-\infty}^{\infty} \psi_j W_{t-j}, \sum_{j=-\infty}^{\infty} \psi_{j+h} W_{t-j}) \\
&amp;= \sum_{j=-\infty}^{\infty} \psi_j \psi_{j+h} \text{Cov}(W_{t-j}, W_{t-j}) \\
&amp;= \sigma^2\sum\limits_{j =  - \infty }^\infty  {{\psi _j}{\psi _{j+h}}}.
\end{align*}\]</span>
</div>
<p> <br></p>

<div class="proof">
 <span class="proof"><em>Proof</em> (All linear processes have absolutely summable covariance structures.). </span> 
<span class="math display">\[\begin{align*}
\sum_{h=-\infty}^{\infty} \mid \gamma(h) \mid &amp;= \sum_{h=-\infty}^{\infty} \sigma^2 \mid \sum_{j=-\infty}^{\infty} \psi_j \psi_{j+h} \mid \\
&amp;\leq \sigma^2 \sum_{h=-\infty}^{\infty} \sum_{j=-\infty}^{\infty} \mid \psi_j \psi_{j+h} \mid \\
&amp;= \sigma^2 \sum_{j=-\infty}^{\infty} \sum_{h=-\infty}^{\infty} \mid \psi_j \mid \cdot \mid \psi_{j+h} \mid \\
&amp;= \sigma^2 \sum_{j=-\infty}^{\infty} \mid \psi_j \mid \sum_{h=-\infty}^{\infty} \mid \psi_{j+h} \mid \\
&amp;= \sigma^2 \big( \sum_{j=-\infty}^{\infty} \mid \psi_j \mid \big)^2 &lt; \infty
\end{align*}\]</span>
So with the assumption that <span class="math inline">\(\sum_{j=-\infty}^{\infty} \mid \psi_j \mid &lt; \infty\)</span>, we obtain that all linear processes have absolutely summable covariance structures. Notice that here we have shown that the condition <span class="math inline">\(\sum\limits_{j = - \infty }^\infty {\left| {{\psi _j}} \right|} &lt; \infty\)</span> is actually stronger than <span class="math inline">\(\sum\limits_{h = - \infty }^\infty {\left| \gamma(h) \right|} &lt; \infty\)</span>.
</div>
<p> <br></p>

<div class="example">
<span id="exm:AR1isLP" class="example"><strong>Example 2.8  (AR1 is a linear process)  </strong></span>When we prove above that AR1 is weakly stationary, we have shown that for an AR1 process <span class="math inline">\(X_t = \phi X_{t-1} + Z_t, \;\;\; Z_t \overset{iid}{\sim} \mathcal{N}(0,\nu^2)\)</span>, it can be represented as
<span class="math display">\[\begin{align*}
X_t = \sum\limits_{j = 0}^{\infty} {{\phi ^j}{Z_{t - j}}}.
\end{align*}\]</span>
<p>Therefore, AR1 is a linear process.</p>
<div style="text-align: right">
<span class="math inline">\(\LARGE{\bullet}\)</span>
</div>
</div>

</div>
<div id="basic-time-series-models" class="section level2">
<h2><span class="header-section-number">2.5</span> Basic Time Series Models</h2>
<p>We first introduce some latent time series models that are commonly used, especially in the calibration procedure of inertial sensors.</p>

<div class="definition">
<span id="def:WN" class="definition"><strong>Definition 2.11  (Gaussian White Noise)  </strong></span>The Gaussian White Noise (WN) process with parameter <span class="math inline">\(\sigma^2 \in \mathbb{R}^+\)</span> is defined as
<span class="math display">\[\begin{equation*}
        X_t  \overset{iid}{\sim} \mathcal{N}\left(0, \sigma^2 \right)
\end{equation*}\]</span>
where âiidâ stands for âindependent and identically distributedâ.
</div>


<div class="definition">
<span id="def:QN" class="definition"><strong>Definition 2.12  (Quantization Noise)  </strong></span>The Quantization Noise (QN) process with parameter <span class="math inline">\(Q^2 \in \mathbb{R}^+\)</span> is a process with Power Spectral Density (PSD) of the form
<span class="math display">\[\begin{equation*}
        S_{X}(f) = 4 Q^2 \sin^2 \left( \frac{\pi f}{\Delta t} \right) \Delta t, \;\; f &lt; \frac{\Delta t}{2}.
\end{equation*}\]</span>
</div>


<div class="definition">
<span id="def:DR" class="definition"><strong>Definition 2.13  (Drift)  </strong></span>The Drift (DR) process with parameter <span class="math inline">\(\omega \in \Omega\)</span>, where <span class="math inline">\(\Omega\)</span> is either <span class="math inline">\(\mathbb{R}^+\)</span> or <span class="math inline">\(\mathbb{R}^-\)</span>, is defined as
<span class="math display">\[\begin{equation*}
X_t = \omega t.
\end{equation*}\]</span>
</div>


<div class="definition">
<span id="def:RW" class="definition"><strong>Definition 2.14  (Random Walk)  </strong></span>The Random Walk (RW) process with parameter <span class="math inline">\(\gamma^2 \in \mathbb{R}^+\)</span> is defined as
<span class="math display">\[\begin{equation*}
X_t = X_{t-1} + \epsilon_t \;\; \text{where}\;\; \epsilon_t  \overset{iid}{\sim} \mathcal{N}\left(0, \gamma^2 \right)\;\; \text{and}\;\; X_0 = 0.
\end{equation*}\]</span>
</div>


<div class="definition">
<span id="def:AR" class="definition"><strong>Definition 2.15  (Auto-Regressive)  </strong></span>The Auto-Regressive process of Order 1 (AR1) with parameter <span class="math inline">\(\phi \in (-1, +1)\)</span> and <span class="math inline">\(\upsilon^2 \in \mathbb{R}^+\)</span> is defined as
<span class="math display">\[\begin{equation*}
X_t = \phi X_{t-1} + Z_t, \;\;\; Z_t \overset{iid}{\sim} \mathcal{N}(0,\upsilon^2).
\end{equation*}\]</span>
</div>


<div class="definition">
<span id="def:GM" class="definition"><strong>Definition 2.16  (Gauss Markov)  </strong></span>The Gauss Markov process of Order 1 (GM) with parameter <span class="math inline">\(\beta \in \mathbb{R}\)</span> and <span class="math inline">\(\sigma_G^2 \in \mathbb{R}^+\)</span> is defined as
<span class="math display">\[\begin{equation*}
      X_t = \exp(-\beta \Delta_t) X_{t-1} + Z_t, \;\;\; 
      Z_t \overset{iid}{\sim} \mathcal{N}(0,\sigma^2_{G}(1-\exp(-2\beta\Delta t)))
\end{equation*}\]</span>
where <span class="math inline">\(\Delta t\)</span> denotes the time between <span class="math inline">\(X_t\)</span> and <span class="math inline">\(X_{t-1}\)</span>.
</div>


<div class="exercise">
<span id="exr:GMandAR1" class="exercise"><strong>Remark 2.7  (GM and AR1)  </strong></span>A GM process is a one-to-one reparametrization of an AR1 process. In the following, we will only discuss AR1 processes but all results remain valid for GM processes.
</div>

<p>With the above defined latent time series processes, we introduce the composite stochastic process, which is widely used in the estimation procedure of inertial sensor stochastic calibration.</p>

<div class="definition">
<span id="def:CompStocProc" class="definition"><strong>Definition 2.17  (Composite Stochastic Process)  </strong></span>A composite stochastic process is a sum of latent processes. We implicitly assume that these latent processes are independent.
</div>


<div class="example">
<span class="example" id="exm:exampleCompStocProc"><strong>Example 2.9  (2*AR1 + WN)  </strong></span>The composite stochastic process of â2*AR1 + WN&quot; is given as
<span class="math display">\[\begin{align}
Y_t &amp;= \phi_1 Y_{t-1} + Z_t, \;\;\; Z_t \overset{iid}{\sim} \mathcal{N}(0,\upsilon_1^2),\\
W_t &amp;= \phi_2 W_{t-1} + U_t, \;\;\; U_t \overset{iid}{\sim} \mathcal{N}(0,\upsilon_2^2),\\
Q_t &amp;\overset{iid}{\sim} \mathcal{N}(0,\sigma^2),\\
X_t &amp;= Y_t + W_t + Q_t,
\end{align}\]</span>
<p>where <span class="math inline">\(Y_t\)</span>, <span class="math inline">\(W_t\)</span> and <span class="math inline">\(Q_t\)</span> are independent and only <span class="math inline">\(X_t\)</span> is observed.</p>
<div style="text-align: right">
<span class="math inline">\(\LARGE{\bullet}\)</span>
</div>
</div>

</div>
<div id="fundamental-representations-of-time-series" class="section level2">
<h2><span class="header-section-number">2.6</span> Fundamental Representations of Time Series</h2>
<p>We conclude this chapter by summarizing the fundamental representations of time series. If two processes have the same fundamental representations, then these two processes are the same. There are two most commonly used fundamental representations of time series, i.e.</p>
<ul>
<li>ACV and ACF;</li>
<li>Power Spectral Density (PSD).</li>
</ul>

<div class="exercise">
<span id="exr:FundaRepreACVACF" class="exercise"><strong>Remark 2.8  (ACV and ACF as fundamental representation)  </strong></span>If we consider a zero mean normally distributed process, it is clear that its joint distribution is fully characterized by the autocovariances <span class="math inline">\(\mathbb{E}[X_t X_{t+h}]\)</span> since the joint probability density only depends on these covariances. Once we know the autocovariances we know everything there is to know about the process and therefore: if two processes have the same autocovariance function, then they are the same process.
</div>


<div class="exercise">
<span id="exr:FundaReprePSD" class="exercise"><strong>Remark 2.9  (PSD as fundamental representation)  </strong></span>The PSD is defined as
<span class="math display">\[\begin{equation*}
    S_X(f) = \int_{- \infty}^{\infty} \gamma_{X}(h)\,e^{-ifh}dh,
\end{equation*}\]</span>
<p>where <span class="math inline">\(f\)</span> is a frequency. Hence, the PSD is a Fourier Transform (FT) of the autocovariance function which describes the variance of a time series over frequencies (with respect to lags <span class="math inline">\(h\)</span>).</p>
Given that the definition of the PSD, as for the autocovariance function, once we know the PSD we know everything there is to know about the process and therefore: if two processes have the same PSD, then they are the same process.
</div>

</div>
<div id="estimation-problems-with-time-series" class="section level2">
<h2><span class="header-section-number">2.7</span> Estimation Problems with Time Series</h2>
<p>Estimation in the context of time series is not as straightforward as in the iid case. For example, let us consider the easiest case of estimation: the sample mean of a stationary time series.</p>

<div class="example">
<span id="exm:EstSampleMean" class="example"><strong>Example 2.10  (Estimation with Sample Mean)  </strong></span>Let <span class="math inline">\((X_t)\)</span> be a stationary time series, so we have that <span class="math inline">\(\mathbb{E}[X_t] = \mu\)</span> and the value of <span class="math inline">\(\mu\)</span> can be estimated by the sample mean, i.e.
<span class="math display">\[\begin{equation*}
    \bar{X} = \frac{1}{T} \sum_{t = 1}^T X_t.
\end{equation*}\]</span>
Using the properties of a stationary process, we obtain
<span class="math display">\[\begin{equation*}
\text{Var} \left(\bar{X}\right) = \frac{\gamma(0)}{T} \sum_{h = -T}^{T} \left(1 - \frac{|h|}{T}\right) \rho(h)
\end{equation*}\]</span>
since
<span class="math display">\[\begin{align*}
\text{Var}(\bar{X}) &amp;= \frac{1}{T^2} \text{Var}(\sum_{t=1}^T X_t) = \frac{1}{T^2} (\sum_{t=1}^T \text{Var}(X_t) + 2\underset{1 \leq t&lt;s\leq T}{\sum\sum} \text{Cov}(X_t, X_s)) \\
&amp;= \frac{1}{T^2} (T\gamma(0) + 2(T-1)\gamma(1) + 2(T-2)\gamma(2) + \ldots + 2\gamma(T-1)) \\
&amp;= \frac{\gamma(0)}{T^2} (T+ 2(T-1)\rho(1) + \ldots + 2\rho(T-1)) \\
&amp;= \frac{\gamma(0)}{T} + \frac{2\gamma(0)}{T} \sum_{h=1}^{T-1} (1-\frac{h}{T})\rho(h) \\
&amp;= \frac{\gamma(0)}{T} \sum_{h = -(T-1)}^{T-1} \left(1 - \frac{|h|}{T}\right) \rho(h) \\
&amp;= \frac{\gamma(0)}{T} \sum_{h = -T}^{T} \left(1 - \frac{|h|}{T}\right) \rho(h).
\end{align*}\]</span>
<div style="text-align: right">
<span class="math inline">\(\LARGE{\bullet}\)</span>
</div>
</div>


<div class="example">
<span id="exm:EstSampleMeaninAR1" class="example"><strong>Example 2.11  (Estimation with Sample Mean in AR1)  </strong></span>As in the previous example, let us consider a stationary AR1 process, i.e.
<span class="math display">\[\begin{equation*}
X_t = \phi X_{t-1} + Z_t, \;\;\; \text{where} \;\;\; |\phi| &lt; 1 \;\;\; \text{and} \;\;\;  Z_t  \overset{iid}{\sim} \mathcal{N} \left(0, \nu^2\right).
\end{equation*}\]</span>
<p>We have obtained before that in AR1, <span class="math inline">\(\gamma(h) = \phi^h \sigma^2 \left(1 - \phi^2\right)^{-1}\)</span>. Therefore, we obtain (after some computations):</p>
<span class="math display">\[\begin{equation*}
\text{Var} \left( {\bar X} \right) = \frac{\nu^2 \left( T - 2\phi - T \phi^2 + 2 \phi^{T + 1}\right)}{T^2\left(1-\phi^2\right)\left(1-\phi\right)^2}.
\end{equation*}\]</span>
Unfortunately, deriving such an exact formula is often difficult when considering more complex models. Therefore, asymptotic approximations are often employed to simplify the calculation. For example, in this AR1 case we have
<span class="math display">\[\begin{equation*}
\lim_{T \to \infty } \; T \text{Var} \left( {\bar X} \right) = \frac{\nu^2}{\left(1-\phi\right)^2},
\end{equation*}\]</span>
providing the following approximate formula
<span class="math display">\[\begin{equation*}
\text{Var} \left( {\bar X} \right) \approx \frac{\nu^2}{T \left(1-\phi\right)^2}.
\end{equation*}\]</span>
<p>Alternatively, simulation methods can also be employed. For example, one could compute <span class="math inline">\(\text{Var} \left( {\bar X} \right)\)</span> as follows:</p>
<p>Step 1: Simulate under the assumed model, i.e. <span class="math inline">\(X_t^* \sim F_{\theta_0}\)</span>, where <span class="math inline">\(F_{\theta_0}\)</span> denotes the true model (in this case an AR1 process).</p>
<p>Step 2: Compute <span class="math inline">\({\bar X^*}\)</span> (i.e.Â average based on <span class="math inline">\((X_t^*)\)</span>).</p>
<p>Step 3: Repeat Steps 1 and 2 <span class="math inline">\(B\)</span> times.</p>
<p>Step 4: Compute the empirical variance <span class="math inline">\({\bar X^*}\)</span> (based on <span class="math inline">\(B\)</span> independent replications).</p>
<p>The above procedure is known as Monte-Carlo method (in this case it is actually a Monte-Carlo integral) and is closely related to the concept of parametric bootstrap (see <span class="citation">Efron and Tibshirani (<a href="#ref-efron1994introduction">1994</a>)</span>) which is a very popular tool in statistics.</p>
<div style="text-align: right">
<span class="math inline">\(\LARGE{\bullet}\)</span>
</div>
</div>

<p>Now we define the classical estimators of <span class="math inline">\(\gamma(h)\)</span> and <span class="math inline">\(\rho(h)\)</span> for AutoCovariance and AutoCorrelation functions.</p>

<div class="definition">
<span id="def:sampleACV" class="definition"><strong>Definition 2.18  (Sample AutoCovariance Function)  </strong></span>The sample autocovariance function is defined as
<span class="math display">\[\begin{equation*}
\hat{\gamma}(h) = \frac{1}{T} \sum_{t = 1}^{T-h} \left(X_{t} - \bar{X}\right) \left(X_{t+h} - \bar{X}\right)
\end{equation*}\]</span>
with <span class="math inline">\(\hat{\gamma}(h) = \hat{\gamma}(-h)\)</span> for <span class="math inline">\(h = 0, 1, ..., k\)</span>, where <span class="math inline">\(k\)</span> is a fixed integer.
</div>


<div class="definition">
<span id="def:sampleACF" class="definition"><strong>Definition 2.19  (Sample AutoCorrelation Function)  </strong></span>The sample autocorrelation function is defined as
<span class="math display">\[\begin{equation*}
\hat{\rho}(h) = \frac{\hat{\gamma}(h)}{\hat{\gamma}(0)}
\end{equation*}\]</span>
with <span class="math inline">\(\hat{\rho}(h) = \hat{\rho}(-h)\)</span> for <span class="math inline">\(h = 0, 1, ..., k\)</span>, where <span class="math inline">\(k\)</span> is a fixed integer.
</div>

<p>We will discuss more about the properties of these estimators in the next chapter.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-granger1969investigating">
<p>Granger, C. W. J. 1969. âInvestigating Causal Relations by Econometric Models and Cross-Spectral Methods.â <em>Econometrica: Journal of the Econometric Society</em>. JSTOR, 424â38.</p>
</div>
<div id="ref-efron1994introduction">
<p>Efron, B., and R. J. Tibshirani. 1994. <em>An Introduction to the Bootstrap</em>. CRC press.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="properties-of-statistical-estimators.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/01-timeseries.Rmd",
"text": "Edit"
},
"download": ["scis.pdf", "scis.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
