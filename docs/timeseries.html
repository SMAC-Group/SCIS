<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>An Introduction to Inertial Sensors Stochastic Calibration</title>
  <meta name="description" content="TO DO">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="An Introduction to Inertial Sensors Stochastic Calibration" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="TO DO" />
  <meta name="github-repo" content="SMAC-Group/SCIS" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="An Introduction to Inertial Sensors Stochastic Calibration" />
  
  <meta name="twitter:description" content="TO DO" />
  

<meta name="author" content="Stéphane Guerrier, Roberto Molinari, Haotian Xu, Ahmed Radi, Gaetan Bakalli, Yuming Zhang and Mucyo Karemera">


<meta name="date" content="2018-06-04">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="a-review-of-the-properties-of-statistical-estimators.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="timeseries.html"><a href="timeseries.html"><i class="fa fa-check"></i><b>2</b> Introduction to Time Series Analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="timeseries.html"><a href="timeseries.html#time-series"><i class="fa fa-check"></i><b>2.1</b> Time Series</a></li>
<li class="chapter" data-level="2.2" data-path="timeseries.html"><a href="timeseries.html#latent-time-series-processes-and-composite-stochastic-process"><i class="fa fa-check"></i><b>2.2</b> Latent Time Series Processes and Composite Stochastic Process</a></li>
<li class="chapter" data-level="2.3" data-path="timeseries.html"><a href="timeseries.html#dependence-within-time-series"><i class="fa fa-check"></i><b>2.3</b> Dependence within Time Series</a></li>
<li class="chapter" data-level="2.4" data-path="timeseries.html"><a href="timeseries.html#stationarity"><i class="fa fa-check"></i><b>2.4</b> Stationarity</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="a-review-of-the-properties-of-statistical-estimators.html"><a href="a-review-of-the-properties-of-statistical-estimators.html"><i class="fa fa-check"></i><b>3</b> A Review of the Properties of Statistical Estimators</a></li>
<li class="chapter" data-level="4" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html"><i class="fa fa-check"></i><b>4</b> Allan Variance Calibration Techniques</a><ul>
<li class="chapter" data-level="4.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#spectral-ambiguity-of-the-av"><i class="fa fa-check"></i><b>4.1</b> Spectral Ambiguity of the AV</a></li>
<li class="chapter" data-level="4.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#properties-of-the-allan-variance"><i class="fa fa-check"></i><b>4.2</b> Properties of the Allan Variance</a></li>
<li class="chapter" data-level="4.3" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#estimation"><i class="fa fa-check"></i><b>4.3</b> Estimation</a><ul>
<li class="chapter" data-level="4.3.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#consistency"><i class="fa fa-check"></i><b>4.3.1</b> Consistency</a></li>
<li class="chapter" data-level="4.3.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#asymptotic-normality"><i class="fa fa-check"></i><b>4.3.2</b> Asymptotic Normality</a></li>
<li class="chapter" data-level="4.3.3" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#confidence-interval-of-the-moav-estimator"><i class="fa fa-check"></i><b>4.3.3</b> Confidence Interval of the MOAV Estimator</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#allan-variance-based-estimation"><i class="fa fa-check"></i><b>4.4</b> Allan variance based estimation</a><ul>
<li class="chapter" data-level="4.4.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#allan-variance-log-log-representation"><i class="fa fa-check"></i><b>4.4.1</b> Allan Variance log-log Representation</a></li>
<li class="chapter" data-level="4.4.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#allan-deviation-of-a-wn-process"><i class="fa fa-check"></i><b>4.4.2</b> Allan Deviation of a WN process</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-generalized-method-of-wavelet-moments.html"><a href="the-generalized-method-of-wavelet-moments.html"><i class="fa fa-check"></i><b>5</b> The Generalized Method of Wavelet Moments</a></li>
<li class="chapter" data-level="6" data-path="extensions.html"><a href="extensions.html"><i class="fa fa-check"></i><b>6</b> Extensions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Inertial Sensors Stochastic Calibration</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="timeseries" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Introduction to Time Series Analysis</h1>
<p>In this chapter, we will provide an introduction to time series analysis. This chapter is organized with the following outline:</p>
<ul>
<li>Definition and descriptive analysis of time series;</li>
<li>Latent time series processes and composite stochastic process;</li>
<li>Dependence within time series;</li>
<li>Concept of stationarity;</li>
<li>Fundamental representation of time series.</li>
</ul>
<div id="time-series" class="section level2">
<h2><span class="header-section-number">2.1</span> Time Series</h2>

<div class="definition">
<p><span id="def:ts" class="definition"><strong>Definition 2.1  (Time Series)  </strong></span>A time series is a stochastic process (i.e. a sequence of random variables (r.v.)), defined on a common probability space denoted as <span class="math inline">\((X_t)_{t = 1,...,T}\)</span> (i.e. {<span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, …., <span class="math inline">\(X_T\)</span>}). Note that the time <span class="math inline">\(t\)</span> is not continuous and belongs to discrete index sets. Therefore, we implicitly assume that</p>
<ul>
<li><span class="math inline">\(t\)</span> is not random, i.e. the time at which each observation is measured is known, and</li>
<li>the time between two consecutive observations is constant.
</div>
</li>
</ul>
<p>When recording values of a time series over an extended period of time, it is usually difficult to discern any trend or pattern of the time series by simply looking at the values. However, when these data points are displayed on a plot with time on x-axis and <span class="math inline">\(X_t\)</span> on y-axis, some features of the time series jump out. So it is often useful to understand a time series process by performing a descriptive analysis, especially when we have data of small or moderate size.</p>
<p>When we perform descriptive analysis, we usually check the following in the time series data/graph:</p>
<ul>
<li>Trends
<ul>
<li>Seasonal (e.g. business cycles)</li>
<li>Non-seasonal (e.g. impact of economic indicators on stock returns)</li>
<li>Local fluctuation (e.g. vibrations observed before, during and after an earthquake)</li>
</ul></li>
<li>Changes in the statistical properties
<ul>
<li>Mean (e.g. economic crisis)</li>
<li>Variance (e.g. earnings)</li>
<li>States (e.g. bear/bull in finance)</li>
</ul></li>
<li>Model deviations (e.g. outliers)</li>
</ul>

<div class="example">
<span id="exm:exampleJJ" class="example"><strong>Example 2.1  (Johnson and Johnson Quarterly Earnings)  </strong></span>A traditional example of a time series is the quarterly earnings of the company Johnson and Johson. In the graph below, we present these earnings between 1960 and 1980.
</div>
<p></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load data</span>
<span class="kw">data</span>(jj, <span class="dt">package =</span> <span class="st">&quot;astsa&quot;</span>)
<span class="co"># Construct gts object</span>
jj =<span class="st"> </span><span class="kw">gts</span>(jj, <span class="dt">start =</span> <span class="dv">1960</span>, <span class="dt">freq =</span> <span class="dv">4</span>)
<span class="co"># Plot time series</span>
<span class="kw">plot</span>(jj, <span class="dt">main =</span> <span class="st">&quot;Johnson and Johnson Quarterly Earnings&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Quarterly Earnings per Share ($)&quot;</span>) </code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-1"></span>
<img src="scis_files/figure-html/unnamed-chunk-1-1.png" alt="Johnson and Johnson Quarterly Earnings" width="624" />
<p class="caption">
Figure 2.1: Johnson and Johnson Quarterly Earnings
</p>
</div>
<p>As we can see from the graph, the data contains a non-linear increasing trend as well as a yearly seasonal component. In addition, we can notice that the variability of the data seems to increase with time. By plotting the time series graph for this data, we can discern some important features of the data, which is helpful for us to conduct further analysis such as selecting suitable models for this data.</p>

<div class="example">
<span id="exm:examplePrecipitation" class="example"><strong>Example 2.2  (Monthly Precipitation Data)  </strong></span>Now we consider another data set coming from the domain of hydrology. The data records monthly precipitation (in mm) over a certain period of time (1907 to 1972) and is interesting for scientists in order to study water cycles. The data are presented in the graph below:
</div>
<p></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load data</span>
<span class="kw">data</span>(hydro, <span class="dt">package =</span> <span class="st">&quot;simts&quot;</span>)
<span class="co"># Construct gts object</span>
hydro =<span class="st"> </span><span class="kw">gts</span>(hydro, <span class="dt">start =</span> <span class="dv">1907</span>, <span class="dt">freq =</span> <span class="dv">12</span>)
<span class="co"># Plot data</span>
<span class="kw">plot</span>(hydro, <span class="dt">main =</span> <span class="st">&quot;Monthly Precipitation Data&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Mean Monthly Precipitation (mm)&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-2"></span>
<img src="scis_files/figure-html/unnamed-chunk-2-1.png" alt="Monthly Precipitation Data" width="624" />
<p class="caption">
Figure 2.2: Monthly Precipitation Data
</p>
</div>
<p>From the above time series graph, we can observe some extreme observations (i.e. outliers) in this data more easily than if we simply look at all the values of this data.</p>

<div class="example">
<span id="exm:exampleInertialSensor" class="example"><strong>Example 2.3  (Inertial Sensor Data)  </strong></span>Now we consider the data coming from the calibration procedure of an Inertial Measurement Unit (IMU). The signals coming from an IMU are usually measured at high frequencies over a long time and are often characterized by linear trends and numerous underlying stochastic processes. We present the time series graph of some data from an IMU below.
</div>
<p></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load data</span>
<span class="kw">data</span>(imu6, <span class="dt">package =</span> <span class="st">&quot;imudata&quot;</span>)
<span class="co"># Construct gts object</span>
imu =<span class="st"> </span><span class="kw">gts</span>(imu6[,<span class="dv">1</span>], <span class="dt">freq =</span> <span class="dv">100</span>*<span class="dv">60</span>*<span class="dv">60</span>)
<span class="co"># Plot data</span>
<span class="kw">plot</span>(imu, <span class="dt">main =</span> <span class="st">&quot;Inertial Sensor Data&quot;</span>,
     <span class="dt">ylab =</span> <span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&quot;Error &quot;</span>, (rad/s^<span class="dv">2</span>))))</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-3"></span>
<img src="scis_files/figure-html/unnamed-chunk-3-1.png" alt="Inertial Sensor Data" width="624" />
<p class="caption">
Figure 2.3: Inertial Sensor Data
</p>
</div>
<p>As we can see from the graph, although a linear trend and other processes are present in this data, it is practically impossible to discern any feature of the data based on the time series graph. In general, the descriptive analysis in classical time series analysis is not appropriate for the analysis of inertial sensors as these data are usually very large in order to perform parameter estimation.</p>
</div>
<div id="latent-time-series-processes-and-composite-stochastic-process" class="section level2">
<h2><span class="header-section-number">2.2</span> Latent Time Series Processes and Composite Stochastic Process</h2>
<p>We first introduce some latent time series processes that are commonly used, especially in the calibration procedure of inertial sensors.</p>

<div class="definition">
<span id="def:WN" class="definition"><strong>Definition 2.2  (Gaussian White Noise)  </strong></span>The Gaussian White Noise (WN) process with parameter <span class="math inline">\(\sigma^2 \in \mathbb{R}^+\)</span> is defined as
<span class="math display">\[\begin{equation*}
        X_t  \overset{iid}{\sim} \mathcal{N}\left(0, \sigma^2 \right)
\end{equation*}\]</span>
where “iid” stands for “independent and identically distributed”.
</div>
<p></p>

<div class="definition">
<span id="def:QN" class="definition"><strong>Definition 2.3  (Quantization Noise)  </strong></span>The Quantization Noise (QN) process with parameter <span class="math inline">\(Q^2 \in \mathbb{R}^+\)</span> is a process with Power Spectral Density (PSD) of the form
<span class="math display">\[\begin{equation*}
        S_{X}(f) = 4 Q^2 \sin^2 \left( \frac{\pi f}{\Delta t} \right) \Delta t, \;\; f &lt; \frac{\Delta t}{2}.
\end{equation*}\]</span>
</div>
<p></p>

<div class="definition">
<span id="def:DR" class="definition"><strong>Definition 2.4  (Drift)  </strong></span>The Drift (DR) process with parameter <span class="math inline">\(\omega \in \Omega\)</span>, where <span class="math inline">\(\Omega\)</span> is either <span class="math inline">\(\mathbb{R}^+\)</span> or <span class="math inline">\(\mathbb{R}^-\)</span>, is defined as
<span class="math display">\[\begin{equation*}
X_t = \omega t.
\end{equation*}\]</span>
</div>
<p></p>

<div class="definition">
<span id="def:RW" class="definition"><strong>Definition 2.5  (Random Walk)  </strong></span>The Random Walk (RW) process with parameter <span class="math inline">\(\gamma^2 \in \mathbb{R}^+\)</span> is defined as
<span class="math display">\[\begin{equation*}
X_t = X_{t-1} + \epsilon_t \;\; \text{where}\;\; \epsilon_t  \overset{iid}{\sim} \mathcal{N}\left(0, \gamma^2 \right)\;\; \text{and}\;\; X_0 = 0.
\end{equation*}\]</span>
</div>
<p></p>

<div class="definition">
<span id="def:AR" class="definition"><strong>Definition 2.6  (Auto-Regressive)  </strong></span>The Auto-Regressive Process of Order 1 (AR1) process with parameter <span class="math inline">\(\phi \in (-1, +1)\)</span> and <span class="math inline">\(\upsilon^2 \in \mathbb{R}^+\)</span> is defined as
<span class="math display">\[\begin{equation*}
X_t = \phi X_{t-1} + Z_t, \;\;\; Z_t \overset{iid}{\sim} \mathcal{N}(0,\upsilon^2).
\end{equation*}\]</span>
</div>
<p></p>

<div class="definition">
<span id="def:GM" class="definition"><strong>Definition 2.7  (Gauss Markov)  </strong></span>The Gauss Markov Process of Order 1 (GM) process with parameter <span class="math inline">\(\beta \in \mathbb{R}\)</span> and <span class="math inline">\(\sigma_G^2 \in \mathbb{R}^+\)</span> is defined as
<span class="math display">\[\begin{equation*}
      X_t = \exp(-\beta \Delta_t) X_{t-1} + Z_t, \;\;\; 
      Z_t \overset{iid}{\sim} \mathcal{N}(0,\sigma^2_{G}(1-\exp(-2\beta\Delta t)))
\end{equation*}\]</span>
where <span class="math inline">\(\Delta t\)</span> denotes the time between <span class="math inline">\(X_t\)</span> and <span class="math inline">\(X_{t-1}\)</span>.
</div>
<p></p>

<div class="exercise">
<span id="exr:GMandAR1" class="exercise"><strong>Remark 2.1  (GM and AR1)  </strong></span>A GM process is a one-to-one reparametrization of an AR1 process. In the following, we will only discuss AR1 processes but all results remain valid for GM processes.
</div>
<p></p>
<p>With the above defined latent time series processes, we introduce the composite stochastic process, which is widely used in the estimation procedure of inertial sensor stochastic calibration.</p>

<div class="definition">
<span id="def:CompStocProc" class="definition"><strong>Definition 2.8  (Composite Stochastic Processes)  </strong></span>A composite stochastic process is a sum of latent processes. We implicitly assume that these latent processes are independent.
</div>
<p></p>

<div class="example">
<span class="example" id="exm:exampleCompStocProc"><strong>Example 2.4  (2*AR1 + WN)  </strong></span>The composite stochastic process of “2*AR1 + WN&quot; is given as
<span class="math display">\[\begin{align}
Y_t &amp;= \phi_1 Y_{t-1} + Z_t, \;\;\; Z_t \overset{iid}{\sim} \mathcal{N}(0,\upsilon_1^2),\\
W_t &amp;= \phi_2 W_{t-1} + U_t, \;\;\; U_t \overset{iid}{\sim} \mathcal{N}(0,\upsilon_2^2),\\
Q_t &amp;\overset{iid}{\sim} \mathcal{N}(0,\sigma^2),\\
X_t &amp;= Y_t + W_t + Q_t,
\end{align}\]</span>
where <span class="math inline">\(Y_t\)</span>, <span class="math inline">\(W_t\)</span> and <span class="math inline">\(Q_t\)</span> are independent and only <span class="math inline">\((X_t)\)</span> is observed.
</div>
<p></p>
</div>
<div id="dependence-within-time-series" class="section level2">
<h2><span class="header-section-number">2.3</span> Dependence within Time Series</h2>
<p>One of the main purpose of time series analysis is to make predictions. That is, if <span class="math inline">\((X_t)_{t=1,\ldots,T}\)</span> is an identically distributed but not independent sequence, what is the best predictor for <span class="math inline">\({X}_{T+h}\)</span> for <span class="math inline">\(h &gt; 0\)</span> (i.e. an estimator of <span class="math inline">\(\mathbb{E}[X_{T+h}| X_T,...]\)</span>)? In order to answer this question, we need to understand the dependence between <span class="math inline">\(X_{1},\ldots,X_{T}\)</span>. Before we start to consider the dependence within time series, let us first take a review on independence.</p>

<div class="definition">
<span id="def:IndepEvents" class="definition"><strong>Definition 2.9  (Independence of Events)  </strong></span>Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent if
<span class="math display">\[\begin{align*}
\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B).
\end{align*}\]</span>
In general, <span class="math inline">\(A_{1},\ldots,A_{n}\)</span> are independent if
<span class="math display">\[\begin{align*}
\mathbb{P}(B_1 \ldots B_n) = \mathbb{P}(B_1) \ldots \mathbb{P}(B_n) \;\; \text{for all} \;\; B_i = A_i \;\; \text{or} \;\; S, \;\; i=1,\ldots,n.
\end{align*}\]</span>
</div>
<p></p>

<div class="definition">
<span id="def:IndepRV" class="definition"><strong>Definition 2.10  (Independence of Random Variables)  </strong></span>Two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> with Cumulative Distribution Functions (CDF) <span class="math inline">\(F_X(x)\)</span> and <span class="math inline">\(F_Y(y)\)</span> respectively are independent if and only if their joint CDF <span class="math inline">\(F_{X,Y}(x,y)\)</span> is such that
<span class="math display">\[\begin{align*}
F_{X,Y}(x,y) = F_{X}(x) F_{Y}(y).
\end{align*}\]</span>
In general, random variables <span class="math inline">\(X_1, \ldots, X_n\)</span> with CDF <span class="math inline">\(F_{X_1}(x_1), \ldots, F_{X_n}(x_n)\)</span> respectively are independent if and only if their joint CDF <span class="math inline">\(F_{X_1, \ldots, X_n}(x_1, \ldots, x_n)\)</span> is such that
<span class="math display">\[\begin{align*}
F_{X_1,\ldots,X_n}(x_1,\ldots,x_n) = F_{X_1}(x_1) \ldots F_{X_n}(x_n).
\end{align*}\]</span>
</div>
<p></p>

<div class="definition">
<span id="def:iid" class="definition"><strong>Definition 2.11  (iid sequence)  </strong></span>The sequence <span class="math inline">\(X_{1},X_{2},\ldots,X_{T}\)</span> is said to be independent and identically distributed (i.e. iid) if and only if
<span class="math display">\[\begin{align*}
\mathbb{P}(X_{i}&lt;x) = \mathbb{P}(X_{j}&lt;x) \;\; \forall x \in \mathbb{R}, \forall i,j \in \{1,\ldots,T\}
\end{align*}\]</span>
and
<span class="math display">\[\begin{align*}
\mathbb{P}(X_{1}&lt;x_{1},X_{2}&lt;x_{2},\ldots,X_{T}&lt;x_{T})=\mathbb{P}(X_{1}&lt;x_1) \ldots \mathbb{P}(X_{T}&lt;x_T) \;\; \forall T\geq2, x_1, \ldots, x_T \in \mathbb{R}.
\end{align*}\]</span>
</div>
<p></p>
<p>Now we start to consider the dependence, specifically the linear dependence, within time series. Notice that it is difficult to consider the dependence between <span class="math inline">\(T\)</span> random variables at a time. So we need to consider only two random variables at a time.</p>

<div class="definition">
<span id="def:ACV" class="definition"><strong>Definition 2.12  (AutoCovariance)  </strong></span>AutoCovariance (ACV) denoted as <span class="math inline">\(\gamma_X(t, t+h)\)</span> is defined as
<span class="math display">\[\begin{align*}
\gamma_X(t, t+h) = \text{Cov}(X_{t},X_{t+h})=   \mathbb{E}(X_{t}X_{t+h})-\mathbb{E}(X_{t})\mathbb{E}(X_{t+h})
\end{align*}\]</span>
where
<span class="math display">\[\begin{align*}
\mathbb{E}(X_{t}) = \int_{-\infty}^{\infty}x f(x) dx \;\; \text{and} \;\; \mathbb{E}(X_{t},X_{t+h}) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}x_{1}x_{2} f(x_{1},x_{2}) dx_{1}dx_{2}
\end{align*}\]</span>
where <span class="math inline">\(f(x_{1},x_{2})\)</span> denotes the joint density of <span class="math inline">\(X_{t}\)</span> and <span class="math inline">\(X_{t+h}\)</span>.
</div>
<p></p>

<div class="exercise">
<p><span id="exr:propertiesACV" class="exercise"><strong>Remark 2.2  (Properties of ACV)  </strong></span></p>
<ol style="list-style-type: decimal">
<li><p>ACV is symmetric, i.e. <span class="math inline">\(\gamma_X(t, t+h) = \gamma_X(t+h, t)\)</span> as <span class="math inline">\(\text{Cov}(X_{t},X_{t+h}) = \text{Cov}(X_{t+h},X_{t})\)</span>. Under stationarity (will be discussed very soon), <span class="math inline">\(\gamma_X(h) = \gamma_X(-h)\)</span>, i.e. ACV is an even function.</p></li>
<li><p>Variance of the process <span class="math inline">\(\text{Var}(X_t) = \gamma_X(t, t) \geq 0\)</span>. Under stationarity, <span class="math inline">\(\text{Var}(X_t) = \gamma_X(0)\)</span> and <span class="math inline">\(\mid \gamma_X(h) \mid \leq \gamma_X(0)\)</span> by Cauchy-Schwarz inequality.</p></li>
<li><p>Scale dependent: ACV <span class="math inline">\(\gamma_X(t, t+h)\)</span> is scale dependent like any covariance. So <span class="math inline">\(\gamma_X(t, t+h) \in \mathbb{R}\)</span>.</p>
<ul>
<li>If <span class="math inline">\(\mid \gamma_X(t, t+h) \mid\)</span> is “close” to 0, then they are “less (linearly) dependent”.</li>
<li>If <span class="math inline">\(\mid \gamma_X(t, t+h) \mid\)</span> is “far” from 0, <span class="math inline">\(X_{t}\)</span> and <span class="math inline">\(X_{t+h}\)</span> are “more (linearly) dependent”.</li>
</ul></li>
</ol>
<p>However in general, it is difficult to assess what “close” and “far” from zero mean.</p>
<ol start="4" style="list-style-type: decimal">
<li>In general, <span class="math inline">\(\gamma_X(t, t+h)=0\)</span> does not imply <span class="math inline">\(X_{t}\)</span> and <span class="math inline">\(X_{t+h}\)</span> are independent. However, if <span class="math inline">\(X_{t}\)</span> and <span class="math inline">\(X_{t+h}\)</span> are joint normally distributed, then <span class="math inline">\(\gamma_X(t, t+h)=0\)</span> implies that <span class="math inline">\(X_{t}\)</span> and <span class="math inline">\(X_{t+h}\)</span> are independent.
</div>
</li>
</ol>
<p>Another measure of linear dependence which is related to the ACV is the AutoCorrelation. This is arguably the most commonly used metric in time series analysis.</p>

<div class="definition">
<span id="def:ACF" class="definition"><strong>Definition 2.13  (AutoCorrelation)  </strong></span>AutoCorrelation (ACF) denoted as <span class="math inline">\(\rho_X(t, t+h)\)</span> is defined as
<span class="math display">\[\begin{align*}
\rho_X(t,t+h) = \text{Corr}(X_{t},X_{t+h}) = \frac{\text{Cov}(X_{t},X_{t+h})}{\sqrt{\text{Var}(X_{t})} \sqrt{\text{Var}(X_{t+h})}}.
\end{align*}\]</span>
</div>
<p></p>

<div class="exercise">
<p><span id="exr:propertiesACF" class="exercise"><strong>Remark 2.3  (Properties of ACF)  </strong></span></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mid \rho_X(t,t+h) \mid \leq 1\)</span> and <span class="math inline">\(\mid \rho_X(t,t) \mid = 1\)</span>.</p></li>
<li><p>ACF is symmetric, i.e. <span class="math inline">\(\rho_X(t, t+h) = \rho_X(t+h, t)\)</span> as <span class="math inline">\(\text{Corr}(X_{t},X_{t+h}) = \text{Corr}(X_{t+h},X_{t})\)</span>. Under stationarity, <span class="math inline">\(\gamma_X(h) = \gamma_X(-h)\)</span>, i.e. ACF is an even function.</p></li>
<li>Scale invariant: ACF <span class="math inline">\(\rho_X(t,t+h)\)</span> is scale free like any correlation. Moreover, if <span class="math inline">\(\rho_X(t,t+h)\)</span> is “close” to <span class="math inline">\(\pm 1\)</span>, then this implies that there is “strong” (linear) dependence between <span class="math inline">\(X_{t}\)</span> and <span class="math inline">\(X_{t+h}\)</span>.
</div>
<p></p></li>
</ol>
<p>We can simplify the notations <span class="math inline">\(\gamma_X(t, t+h)\)</span> and <span class="math inline">\(\rho_X(t, t+h)\)</span> to be <span class="math inline">\(\gamma(t, t+h)\)</span> and <span class="math inline">\(\rho(t, t+h)\)</span> when there is no ambiguity (i.e. only one time series is considered).</p>
<p>Notice that both ACV and ACF are appropriate to measure linear dependence only. Besides linear dependence, other forms of dependence such as monotonic or nonlinear dependence also exist. However, both ACV and ACF are less helpful to measure these dependence as they might have ACV and ACF to be zero. Here is an example:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-4"></span>
<img src="images/Cor.png" alt="Different forms of dependence and their ACF values" width="80%" />
<p class="caption">
Figure 2.4: Different forms of dependence and their ACF values
</p>
</div>
<p>It is worth noting that correlation does NOT imply causation. For example, if <span class="math inline">\(\rho(t, t+h) \neq 0\)</span>, it does not imply that <span class="math inline">\(X_t \to X_{t+h}\)</span> is causal. Actually, real causation doesn’t exist in statistics but there exists approximated metric to measure this concept such as Granger causality (see <span class="citation">Granger (<a href="#ref-granger1969investigating">1969</a>)</span>). This idea is clearly illustrated in the image below:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-5"></span>
<img src="images/corr_cat.jpg" alt="Correlation does NOT imply causation." width="50%" />
<p class="caption">
Figure 2.5: Correlation does NOT imply causation.
</p>
</div>
</div>
<div id="stationarity" class="section level2">
<h2><span class="header-section-number">2.4</span> Stationarity</h2>
<p>In this section we are going to introduce the concept of stationarity, one of the most important characteristics of time series data. First let us consider an example of <strong>non-stationary process</strong>.</p>

<div class="example">
<span id="exm:exampleNonStationary" class="example"><strong>Example 2.5  (Non-Stationary Process)  </strong></span>
<span class="math display">\[\begin{equation*}
X_t  \sim \mathcal{N} \left(0, Y_t^2\right) \;\; \text{where $Y_t$ is unobserved and such that} \;\; Y_t  \overset{iid}{\sim} \mathcal{N} \left(0, 1\right).
\end{equation*}\]</span>
In this case, it is clear that the estimation of <span class="math inline">\(\text{Var}(X_t)\)</span> is difficult since only <span class="math inline">\(X_t\)</span> is useful for the estimation. So in fact, <span class="math inline">\(X_t^2\)</span> is our best guess for <span class="math inline">\(\text{Var}(X_t)\)</span>.
</div>
<p></p>
<p>On the other hand, let us consider an example of <strong>stationary process</strong> where averaging becomes meaningful for such process.</p>

<div class="example">
<span id="exm:exampleStationary" class="example"><strong>Example 2.6  (Stationary Process)  </strong></span>
<span class="math display">\[\begin{equation*}
X_t = \theta W_{t-1} + W_t \;\;\; \text{where} \;\;\;  W_t  \stackrel{iid}{\sim} \mathcal{N} \left(0, 1\right).
\end{equation*}\]</span>
In this case, we can guess that a natural estimator of <span class="math inline">\(\text{Var}(X_t)\)</span> can be <span class="math inline">\(\hat{\sigma}^2 = \frac{1}{T} \sum_{i = 1}^T X_i^2\)</span>. That is, now averages are meaningful for such process.
</div>
<p></p>
<p>There exist two forms of stationarity, which are defined below:</p>

<div class="definition">
<span id="def:StrongStationarity" class="definition"><strong>Definition 2.14  (Strong Stationarity)  </strong></span>The time series <span class="math inline">\(X_{t}\)</span> is strongly stationary if the joint probability distribution is invariant under a shift in time, i.e.
<span class="math display">\[\begin{equation*}
\mathbb{P}(X_{t}\leq x_{0},\ldots,X_{t+k}\leq x_{k}) = \mathbb{P}(X_{t+h}\leq x_{0} ,\ldots,X_{t+h+k}\leq x_{k})
\end{equation*}\]</span>
for any time shift <span class="math inline">\(h\)</span> and any <span class="math inline">\(x_{0}, x_{1},x_{2},\cdots,x_{k}\)</span> belong to the domain of <span class="math inline">\(X_t,\cdots,X_{t+k}\)</span> and <span class="math inline">\(X_{t+h},\cdots,X_{t+h+k}\)</span>.
</div>
<p></p>

<div class="definition">
<span id="def:WeakStationarity" class="definition"><strong>Definition 2.15  (Weak Stationarity)  </strong></span>The time series <span class="math inline">\((X_{t})_{t \in \mathbb{N}}\)</span> is weakly stationary if the mean and autocovariance are finite and invariant under a shift in time, i.e.
<span class="math display">\[\begin{equation*}
\begin{aligned}
\mathbb{E}\left[X_t\right] &amp;= \mu &lt; \infty,\\
\mathbb{E}\left[X_t^2\right]  &amp;= \mu_2 &lt; \infty,\\
\text{Cov}(X_{t},X_{t+h})&amp;= \text{Cov}(X_{t + k},X_{t+h + k}) = \gamma( h ).
\end{aligned}
\end{equation*}\]</span>
for any time shift <span class="math inline">\(h\)</span>. For convenience, we use the abbreviation “stationary” to indicate “weakly stationary” by default.
</div>
<p></p>
<p>The stationarity of <span class="math inline">\(X_{t}\)</span> is important because it provides a framework in which averaging makes sense. The concept of averaging is essentially meaningless unless properties like mean and covariance are either fixed or evolve in a known manner.</p>

<div class="exercise">
<span id="exr:StatImpAcvAcf" class="exercise"><strong>Remark 2.4  (Implication on the ACV and ACF)  </strong></span>If a process is weakly stationary or strongly stationary and <span class="math inline">\(\text{Cov}(X_{t},X_{t+h})\)</span> exists for all <span class="math inline">\(h \in \mathbb{Z}\)</span>, then we have both ACV and ACF only depend on the lag between observations, i.e.
<span class="math display">\[\begin{equation*}
\begin{aligned}
\gamma(t, t+h) &amp;= \text{Cov}(X_{t},X_{t+h})= \text{Cov}(X_{t + k},X_{t+h + k}) = \gamma(t+k, t+h+k) = \gamma(h),\\
\rho(t, t+h) &amp;= \text{Corr}(X_{t},X_{t+h})= \text{Corr}(X_{t + k},X_{t+h + k}) = \rho(t+k, t+h+k) = \rho(h).
\end{aligned}
\end{equation*}\]</span>
</div>
<p></p>

<div class="exercise">
<p><span id="exr:RelationStationary" class="exercise"><strong>Remark 2.5  (Relation between Strong and Weak Stationarity)  </strong></span></p>
<p>In general, neither type of stationarity implies the other one. However,</p>
<ul>
<li>If <span class="math inline">\(X_{t}\)</span> is Normal (Gaussian) with <span class="math inline">\(\sigma^2 = \text{Var} (X_{t}) &lt; \infty\)</span>, then weak stationarity implies strong stationarity.</li>
<li>If <span class="math inline">\(X_{t}\)</span> is strongly stationary, <span class="math inline">\(\mathbb{E}(X_t) &lt; \infty\)</span> and <span class="math inline">\(\mathbb{E}(X_t^2) &lt; \infty\)</span>, then <span class="math inline">\(X_{t}\)</span> is weakly stationary.
</div>
</li>
</ul>

<div class="example">
<span id="exm:exampleRelationStat1" class="example"><strong>Example 2.7  (Strong Stationarity does NOT imply Weak Stationarity)  </strong></span>An iid Cauchy process is strongly but not weakly stationary as the mean of the process does not exist.
</div>
<p></p>

<div class="example">
<span id="exm:exampleRelationStat2" class="example"><strong>Example 2.8  (Weak Stationarity does NOT imply Strong Stationarity)  </strong></span>Let <span class="math inline">\(X_t \overset{iid}{\sim} \exp(1)\)</span> (i.e. exponential distribution with <span class="math inline">\(\lambda = 1\)</span>) and <span class="math inline">\(Y_t \overset{iid}{\sim} \mathcal{N}(1,1)\)</span>. Then, let
<span class="math display">\[\begin{equation*}
Z_t = \left\{
\begin{array}{cl}
X_t &amp;\text{if } t \in \left\{2k | k \in \mathbb{N}\right\}\\
Y_t &amp;\text{if }  t \in \left\{2k + 1 | k \in \mathbb{N}\right\},
\end{array}
\right.
\end{equation*}\]</span>
we have <span class="math inline">\(Z_t\)</span> is weakly stationary but not strongly stationary.
</div>
<p></p>

<div class="exercise">
<p><span id="exr:StatProcesses" class="exercise"><strong>Remark 2.6  (Stationarity of Latent Time Series Processes)  </strong></span></p>
<ul>
<li>(Weakly) Stationary: WN, QN, AR1</li>
<li>(Weakly) Non-Stationary: DR, RW
</div>
</li>
</ul>

<div class="proof">
 <span class="proof"><em>Proof</em> (AR1 is weakly stationary). </span> Consider an AR1 process defined as:
<span class="math display">\[\begin{equation*}
X_t = \phi X_{t-1} + Z_t, \;\;\; Z_t \overset{iid}{\sim} \mathcal{N}(0,\nu^2),
\end{equation*}\]</span>
where <span class="math inline">\(\mid \phi \mid &lt; 1\)</span> and <span class="math inline">\(\nu^2 &lt; \infty\)</span>. Then we have <span class="math display">\[\begin{aligned}
{X_t}  &amp;=   {\phi }{X_{t - 1}} + {Z_t} = \phi \left[ {\phi {X_{t - 2}} + {Z_{t - 1}}} \right] + {Z_t} =  {\phi ^2}{X_{t - 2}} + \phi {Z_{t - 1}} + {Z_t}  \\
&amp; \; \vdots  \\
&amp;=   {\phi ^k}{X_{t-k}} + \sum\limits_{j = 0}^{k - 1} {{\phi ^j}{Z_{t - j}}} .
\end{aligned} \]</span> By taking the limit in <span class="math inline">\(k\)</span> (which is perfectly valid as we assume <span class="math inline">\(t \in \mathbb{Z}\)</span>), we obtain
<span class="math display">\[\begin{equation*}
\begin{aligned}
X_t = \mathop {\lim }\limits_{k \to \infty} \; {X_t}  =  \sum\limits_{j = 0}^{\infty} {{\phi ^j}{Z_{t - j}}}.
\end{aligned}
\end{equation*}\]</span>
So we have
<span class="math display">\[\begin{equation*}
\begin{aligned}
\mathbb{E}\left[X_t\right] &amp;=  \sum\limits_{j = 0}^{\infty} {{\phi ^j}{\mathbb{E} [Z_{t - j}]}} = 0, \\
\text{Var}\left(X_t\right) &amp;= \text{Var}\left(\sum\limits_{j = 0}^{\infty} {{\phi ^j}{Z_{t - j}}}\right) = \sum\limits_{j = 0}^{\infty} {\phi^{2j}} \text{Var}\left(Z_{t-j}\right) = \nu^2 \sum\limits_{j = 0}^{\infty} {\phi^{2j}} = \frac{\nu^2}{1-\phi^2} &lt; \infty.
\end{aligned}
\end{equation*}\]</span>
Moreover, assuming for notational simplicity that <span class="math inline">\(h &gt; 1\)</span>, we obtain
<span class="math display">\[\begin{equation*}
\begin{aligned}
\text{Cov}\left(X_t, X_{t+h}\right) &amp;= \phi \text{Cov}\left(X_t, X_{t+h-1}\right) = \phi^2 \text{Cov}\left(X_t, X_{t+h-2}\right) = \ldots = \phi^h \text{Cov}(X_t, X_t).
\end{aligned}
\end{equation*}\]</span>
In general, when <span class="math inline">\(h \in \mathbb{Z}\)</span> we obtain
<span class="math display">\[\begin{equation*}
\begin{aligned}
\text{Cov}\left(X_t, X_{t+h}\right) &amp; = \phi^{|h|} \text{Cov}(X_t, X_t) = \phi^{|h|} \frac{\nu^2}{1-\phi^2},
\end{aligned}
\end{equation*}\]</span>
which is a function of the lag <span class="math inline">\(h\)</span> only. Therefore, this AR1 process is weakly stationary.
</div>
<p></p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-granger1969investigating">
<p>Granger, C. W. J. 1969. “Investigating Causal Relations by Econometric Models and Cross-Spectral Methods.” <em>Econometrica: Journal of the Econometric Society</em>. JSTOR, 424–38.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="a-review-of-the-properties-of-statistical-estimators.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/01-timeseries.Rmd",
"text": "Edit"
},
"download": ["scis.pdf", "scis.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
