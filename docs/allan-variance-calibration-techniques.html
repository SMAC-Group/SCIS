<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>An Introduction to Inertial Sensors Stochastic Calibration</title>
  <meta name="description" content="TO DO">
  <meta name="generator" content="bookdown 0.7.10 and GitBook 2.6.7">

  <meta property="og:title" content="An Introduction to Inertial Sensors Stochastic Calibration" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="TO DO" />
  <meta name="github-repo" content="SMAC-Group/SCIS" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="An Introduction to Inertial Sensors Stochastic Calibration" />
  
  <meta name="twitter:description" content="TO DO" />
  

<meta name="author" content="StÃ©phane Guerrier, Roberto Molinari, Yuming Zhang, Haotian Xu, Gaetan Bakalli, Ahmed Radi and Mucyo Karemera">


<meta name="date" content="2018-07-17">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="properties-of-statistical-estimators.html">
<link rel="next" href="the-generalized-method-of-wavelet-moments.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="timeseries.html"><a href="timeseries.html"><i class="fa fa-check"></i><b>2</b> Introduction to Time Series Analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="timeseries.html"><a href="timeseries.html#time-series"><i class="fa fa-check"></i><b>2.1</b> Time Series</a></li>
<li class="chapter" data-level="2.2" data-path="timeseries.html"><a href="timeseries.html#dependence-within-time-series"><i class="fa fa-check"></i><b>2.2</b> Dependence within Time Series</a></li>
<li class="chapter" data-level="2.3" data-path="timeseries.html"><a href="timeseries.html#stationarity"><i class="fa fa-check"></i><b>2.3</b> Stationarity</a></li>
<li class="chapter" data-level="2.4" data-path="timeseries.html"><a href="timeseries.html#linear-processes"><i class="fa fa-check"></i><b>2.4</b> Linear Processes</a></li>
<li class="chapter" data-level="2.5" data-path="timeseries.html"><a href="timeseries.html#basic-time-series-models"><i class="fa fa-check"></i><b>2.5</b> Basic Time Series Models</a></li>
<li class="chapter" data-level="2.6" data-path="timeseries.html"><a href="timeseries.html#fundamental-representations-of-time-series"><i class="fa fa-check"></i><b>2.6</b> Fundamental Representations of Time Series</a></li>
<li class="chapter" data-level="2.7" data-path="timeseries.html"><a href="timeseries.html#estimation-problems-with-time-series"><i class="fa fa-check"></i><b>2.7</b> Estimation Problems with Time Series</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html"><i class="fa fa-check"></i><b>3</b> Properties of Statistical Estimators</a><ul>
<li class="chapter" data-level="3.1" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#extremum-estimators"><i class="fa fa-check"></i><b>3.1</b> Extremum Estimators</a></li>
<li class="chapter" data-level="3.2" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#consistency"><i class="fa fa-check"></i><b>3.2</b> Consistency</a><ul>
<li class="chapter" data-level="3.2.1" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#consistency-of-extremum-estimators"><i class="fa fa-check"></i><b>3.2.1</b> Consistency of Extremum Estimators</a></li>
<li class="chapter" data-level="3.2.2" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#verification-of-condition-c1"><i class="fa fa-check"></i><b>3.2.2</b> Verification of Condition C1</a></li>
<li class="chapter" data-level="3.2.3" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#verification-of-condition-c4"><i class="fa fa-check"></i><b>3.2.3</b> Verification of Condition C4</a></li>
<li class="chapter" data-level="3.2.4" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#consistency-of-sample-autocovariance-and-autocorrelation-functions"><i class="fa fa-check"></i><b>3.2.4</b> Consistency of Sample AutoCovariance and AutoCorrelation Functions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#asymptotic-normality"><i class="fa fa-check"></i><b>3.3</b> Asymptotic Normality</a><ul>
<li class="chapter" data-level="3.3.1" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#clt-for-iid-random-variables"><i class="fa fa-check"></i><b>3.3.1</b> CLT for iid Random Variables</a></li>
<li class="chapter" data-level="3.3.2" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#clt-for-dependent-processes"><i class="fa fa-check"></i><b>3.3.2</b> CLT for Dependent Processes</a></li>
<li class="chapter" data-level="3.3.3" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#asymptotic-normality-of-sample-autocovariance-and-autocorrelation-functions"><i class="fa fa-check"></i><b>3.3.3</b> Asymptotic Normality of Sample AutoCovariance and AutoCorrelation Functions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html"><i class="fa fa-check"></i><b>4</b> Allan Variance Calibration Techniques</a><ul>
<li class="chapter" data-level="4.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#spectral-ambiguity-of-the-av"><i class="fa fa-check"></i><b>4.1</b> Spectral Ambiguity of the AV</a></li>
<li class="chapter" data-level="4.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#properties-of-the-allan-variance"><i class="fa fa-check"></i><b>4.2</b> Properties of the Allan Variance</a></li>
<li class="chapter" data-level="4.3" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#estimation"><i class="fa fa-check"></i><b>4.3</b> Estimation</a><ul>
<li class="chapter" data-level="4.3.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#consistency-1"><i class="fa fa-check"></i><b>4.3.1</b> Consistency</a></li>
<li class="chapter" data-level="4.3.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#asymptotic-normality-1"><i class="fa fa-check"></i><b>4.3.2</b> Asymptotic Normality</a></li>
<li class="chapter" data-level="4.3.3" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#confidence-interval-of-the-moav-estimator"><i class="fa fa-check"></i><b>4.3.3</b> Confidence Interval of the MOAV Estimator</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#allan-variance-based-estimation"><i class="fa fa-check"></i><b>4.4</b> Allan variance based estimation</a><ul>
<li class="chapter" data-level="4.4.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#allan-variance-log-log-representation"><i class="fa fa-check"></i><b>4.4.1</b> Allan Variance log-log Representation</a></li>
<li class="chapter" data-level="4.4.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#allan-deviation-of-a-wn-process"><i class="fa fa-check"></i><b>4.4.2</b> Allan Deviation of a WN process</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-generalized-method-of-wavelet-moments.html"><a href="the-generalized-method-of-wavelet-moments.html"><i class="fa fa-check"></i><b>5</b> The Generalized Method of Wavelet Moments</a></li>
<li class="chapter" data-level="6" data-path="extensions.html"><a href="extensions.html"><i class="fa fa-check"></i><b>6</b> Extensions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="7" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>7</b> Introduction</a><ul>
<li class="chapter" data-level="7.1" data-path="introduction-1.html"><a href="introduction-1.html#time-series-1"><i class="fa fa-check"></i><b>7.1</b> Time Series</a></li>
<li class="chapter" data-level="7.2" data-path="introduction-1.html"><a href="introduction-1.html#eda"><i class="fa fa-check"></i><b>7.2</b> Exploratory Data Analysis for Time Series</a></li>
<li class="chapter" data-level="7.3" data-path="introduction-1.html"><a href="introduction-1.html#basicmodels"><i class="fa fa-check"></i><b>7.3</b> Basic Time Series Models</a><ul>
<li class="chapter" data-level="7.3.1" data-path="introduction-1.html"><a href="introduction-1.html#wn"><i class="fa fa-check"></i><b>7.3.1</b> White noise processes</a></li>
<li class="chapter" data-level="7.3.2" data-path="introduction-1.html"><a href="introduction-1.html#rw"><i class="fa fa-check"></i><b>7.3.2</b> Random Walk Processes</a></li>
<li class="chapter" data-level="7.3.3" data-path="introduction-1.html"><a href="introduction-1.html#ar1"><i class="fa fa-check"></i><b>7.3.3</b> Autoregressive Process of Order 1</a></li>
<li class="chapter" data-level="7.3.4" data-path="introduction-1.html"><a href="introduction-1.html#ma1"><i class="fa fa-check"></i><b>7.3.4</b> Moving Average Process of Order 1</a></li>
<li class="chapter" data-level="7.3.5" data-path="introduction-1.html"><a href="introduction-1.html#drift"><i class="fa fa-check"></i><b>7.3.5</b> Linear Drift</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="introduction-1.html"><a href="introduction-1.html#lts"><i class="fa fa-check"></i><b>7.4</b> Composite Stochastic Processes</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Inertial Sensors Stochastic Calibration</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="allan-variance-calibration-techniques" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Allan Variance Calibration Techniques</h1>
<ul>
<li>The Allan Variance (AV) is a statistical technique originally developed in the mid-1960s to study the stability of precision oscillators <span class="citation">(see e.g. Allan <a href="#ref-allan1966statistics">1966</a>)</span>.</li>
<li>It can provide information on the types and magnitude of various superimposed noise terms (i.e.Â composite stochastic processes).</li>
<li>This method has been adapted to characterize the properties of a variety of devices including inertial sensors <span class="citation">(see El-Sheimy, Hou, and Niu <a href="#ref-elsheimy08av">2008</a>)</span>.</li>
<li>The AV is a measure of variability developed for long term memory processes and can in fact be interpreted as a Haar wavelet coefficient variance <span class="citation">(see Percival and Guttorp <a href="#ref-percival1994long">1994</a>)</span>. We will discuss this connection further on.</li>
</ul>

<div class="definition">
<p><span id="def:defAV" class="definition"><strong>Definition 4.1  (Allan Variance)  </strong></span>We consider the AV at dyadic scales (<span class="math inline">\(\tau_j\)</span>) starting from local averages of the process which can be denoted as</p>
<span class="math display">\[\begin{equation*} 
    \bar{X}_{t}^{(j)} \equiv \frac{1}{\tau_j} \sum_{i = 1}^{\tau_j} X_{t - \tau_j + i}\, ,
    \label{mean.noav}
\end{equation*}\]</span>
<p>where <span class="math inline">\(\tau_j \equiv 2^j, \; j \in \left\{x \in \mathbb{N} \, : \; 1 \leq x &lt; \log_2 (T) - 1 \right\}\)</span> therefore determines the number of consecutive observations considered for the average. Then, the AV is defined as</p>
<span class="math display">\[\begin{equation*}
    \text{AV}_j \left(X_t \right) \equiv \frac{1}{2} \, \mathbb{E}\left[ \left(\bar{X}_{t}^{(j)} - \bar{X}_{t-\tau_j}^{(j)} \right)^2 \right].
\end{equation*}\]</span>
</div>


<div class="exercise">
<span id="exr:alterDefScale" class="exercise"><strong>Remark 4.1  (Alternative scale definition)  </strong></span>The definition of the AV is actually valid for <span class="math inline">\(\tau_j = \lfloor2^j\rfloor\)</span> with <span class="math inline">\(j \in \left\{x \in \mathbb{R} \, : \; 1 \leq x &lt; \log_2 (T) - 1 \right\}\)</span>. In some case, it could use to consider this alternative definition <span class="citation">(see e.g. El-Sheimy, Hou, and Niu <a href="#ref-elsheimy08av">2008</a>)</span> but we shall restrict ourself here to the case where <span class="math inline">\(j \in \left\{x \in \mathbb{N} \, : \; 1 \leq x &lt; \log_2 (T) - 1 \right\}\)</span>.
</div>


<div class="exercise">
<span id="exr:notationAV" class="exercise"><strong>Remark 4.2  (Notation of the Allan Variance)  </strong></span>For notational simplicity, we may sometimes replace <span class="math inline">\(\text{AV}_j \left(X_t \right)\)</span> by simply <span class="math inline">\(\phi_j^2\)</span> when the dependence of the AV to the process <span class="math inline">\((X_t)\)</span> is evident.
</div>

<p>As highlighted earlier, the AV is, among others, a widely and commonly used approach in engineering for sensor calibration as it is linked to the properties of the process <span class="math inline">\((X_t)\)</span> as shown in the following lemma <span class="citation">(see e.g. Percival and Walden <a href="#ref-percival2006wavelet">2006</a> for a proof)</span>.</p>

<div class="lemma">
<p><span id="lem:lemmavpsd" class="lemma"><strong>Lemma 4.1  (AV connection to PSD)  </strong></span>For a stationary process <span class="math inline">\((X_t)\)</span> with PSD <span class="math inline">\(S_{X}(f)\)</span> we have</p>
<span class="math display">\[\begin{equation*}
\phi_j^2 \equiv \text{AV}_j \left(X_t \right) = 4  \int_0^{\infty}  \frac{\sin^4(\pi f \tau_j)}{(\pi f \tau_j)^2} S_{X}(f) df. 
\label{eq:allanvariancePSD_LInk}
\end{equation*}\]</span>
</div>

<p>Therefore, this result establishes a direct connection between the AV and PSD. A natural question is therefore whether the mapping PSD <span class="math inline">\(\mapsto\)</span> AV is one-to-one. <span class="citation">Greenhall (<a href="#ref-greenhall1998spectral">1998</a>)</span> (see Theorem 1) showed that this is actually not the case. This is illustrated in the following section ????? (ADD REF).</p>
<div id="spectral-ambiguity-of-the-av" class="section level2">
<h2><span class="header-section-number">4.1</span> Spectral Ambiguity of the AV</h2>
<p>Consider two (independent) stochastic processes <span class="math inline">\((X_t)\)</span> and <span class="math inline">\((Y_t)\)</span> with respective PSD <span class="math inline">\(S_X(f)\)</span> and <span class="math inline">\(S_Y(f)\)</span>. Suppose that <span class="math inline">\(S_X(f) \neq S_Y(f)\)</span>, then the two processes will have the same AV if</p>
<span class="math display">\[\begin{equation*}
     \Delta \equiv \int_0^{\infty}  \frac{\sin^4(\pi f \tau_j)}{(\pi f \tau_j)^2} \Phi(f) df = 0,
\end{equation*}\]</span>
<p>where <span class="math inline">\(\Phi(f) \equiv S_{X}(f) - S_{Y}(f)\)</span>. To show that it is possible that <span class="math inline">\(\Delta = 0\)</span> when <span class="math inline">\(\Phi(f) \neq 0\)</span>, we will use the following critical identity:</p>
<span class="math display">\[\begin{equation}
    \label{ident:av:indet}
    \sin^4(x) = \sin^2(x) - \frac{1}{4} \sin^2(2x).
\end{equation}\]</span>
<p>First, we note that <span class="math inline">\(\Delta\)</span> may be expressed using () as follows:
<span class="math display">\[\begin{equation*}
    \begin{aligned}
        \Delta &amp;=  \int_{0}^{\infty} \frac{\sin^4\left(\tau \pi f \right)}{\left(\tau \pi f \right)^2} \Phi(f) df \\
        &amp;= \lim_{n \rightarrow -\infty} \int_{2^{n}}^{\infty} \frac{\sin^2\left(\tau \pi f \right) - \frac{1}{4} \sin^2\left(2 \tau \pi f \right) }{\left(\tau \pi f \right)^2} \Phi(f) df .
    \end{aligned}
\end{equation*}\]</span></p>
<p>Second, by the change of variable <span class="math inline">\(u = 2f\)</span> in the second term we obtain</p>
<span class="math display">\[\begin{equation*}
    \begin{aligned}
        \Delta = \lim_{n \rightarrow -\infty} &amp; \Bigg[ \int_{2^{n}}^{\infty} \frac{\sin^2\left(\tau \pi f \right)}{\left(\tau \pi f \right)^2} \Phi(f) df - \frac{1}{2}\int_{2^{n+1}}^{\infty} \frac{\sin^2\left(\tau \pi u \right)}{\left(\tau \pi u \right)^2} \Phi(f) du \Bigg].
    \end{aligned}
\end{equation*}\]</span>
<p>Now suppose that <span class="math inline">\(\Phi(f) = 2 \Phi(2f)\)</span>. In this case, we have <span class="math inline">\(\Phi(f) = 2 \Phi(u)\)</span> and therefore we obtain
<span class="math display">\[\begin{equation*}
    \begin{aligned}
        \Delta &amp;= \lim_{n \rightarrow -\infty} \int_{2^{n}}^{2^{n+1}} \frac{\sin^2\left(\tau \pi f \right)}{\left(\tau \pi f \right)^2} \Phi(f) df = 0.
    \end{aligned}
\end{equation*}\]</span></p>

<div class="exercise">
<span id="exr:unnamed-chunk-6" class="exercise"><strong>Remark 4.3  </strong></span>This result demonstrates that the mapping from PSD to Allan variance is not necessarily one-to-one. <span class="citation">Greenhall (<a href="#ref-greenhall1998spectral">1998</a>)</span> showed that in the continuous case (i.e. <span class="math inline">\(\tau_j \in \mathbb{R}\)</span>) <span class="math inline">\(\Delta = 0\)</span> if and only if <span class="math inline">\(\Phi(f) = 2 \Phi(2f)\)</span>. However, the ``only ifââ part of this results (while conjectured) is unknown in the discrete case.
</div>

</div>
<div id="properties-of-the-allan-variance" class="section level2">
<h2><span class="header-section-number">4.2</span> Properties of the Allan Variance</h2>
<p>One reason of explaining the widespread use of the Allan variance for sensor calibration is due to the following additivity property, which is particularly convenient to identify composite stochastic processes (see Definition REF MISSING!).</p>

<div class="corollary">
<p><span id="cor:coroaddav" class="corollary"><strong>Corollary 4.1  (Additivity of the AV)  </strong></span>Consider two (independent) stochastic processes <span class="math inline">\((X_t)\)</span> and <span class="math inline">\((Y_t)\)</span> with respective PSD <span class="math inline">\(S_X(f)\)</span> and <span class="math inline">\(S_Y(f)\)</span>. Suppose that we observe the process <span class="math inline">\(Z_t = X_t + Y_t\)</span>. Then, we have</p>
<span class="math display">\[\begin{equation*}
    \text{AV}_j \left(Z_t \right) = \text{AV}_j \left(X_t \right) + \text{AV}_j \left(Y_t \right).
\end{equation*}\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> The proof of this result is direct from Lemma REF MISSING HERE. Indeed, since <span class="math inline">\(S_Z(f) = S_X(f) + S_Y(f)\)</span>, we have</p>
<span class="math display">\[\begin{equation*}
    \begin{aligned}
    \text{AV}_j \left(Z_t \right) &amp;= 4  \int_0^{\infty}  \frac{\sin^4(\pi f \tau_j)}{(\pi f \tau_j)^2} S_{Z}(f) df\\
    &amp;= 4  \int_0^{\infty}  \frac{\sin^4(\pi f \tau_j)}{(\pi f \tau_j)^2} S_{X}(f) df + 4  \int_0^{\infty}  \frac{\sin^4(\pi f \tau_j)}{(\pi f \tau_j)^2} S_{Y}(f) df\\
    &amp;= \text{AV}_j \left(X_t \right) + \text{AV}_j \left(Y_t \right).
    \end{aligned}
\end{equation*}\]</span>
</div>

<p><br></p>
<p>While Lemma <a href="allan-variance-calibration-techniques.html#lem:lemmavpsd">4.1</a> is an important results which is very convenient to determine the theoretical AV of a certain stochastic process. However, the applicability of this results is often limited since the integral defined in (REF MISSING HERE) can be intractable. An alternative to Lemma <a href="allan-variance-calibration-techniques.html#lem:lemmavpsd">4.1</a> has been proposed by <span class="citation">Zhang (<a href="#ref-zhang2008allan">2008</a>)</span> and is far advantageous from a computational standpoint.</p>

<div class="lemma">
<span id="lem:lemmaavtoacf" class="lemma"><strong>Lemma 4.2  (AV connection to ACF)  </strong></span>For a stationary process <span class="math inline">\((X_t)\)</span> with variance <span class="math inline">\(\sigma^2_X\)</span> and ACF <span class="math inline">\(\rho(h)\)</span> we have
<span class="math display">\[\begin{equation*}
\label{stat.av}
    \text{AV}_j \left(X_t \right)  = \frac{\sigma_X^2}{\tau_j^2} \bigg(\tau_j\left[1-\rho(\tau_j)\right] 
   + \sum_{i=1}^{\tau_j-1} i \left[2 \rho(\tau_j-i) - \rho(i) - \rho(2\tau_j-i)\right]\bigg).
\end{equation*}\]</span>
</div>

<p>The proof of this result is instructive and is presented in <span class="citation">Xu et al. (<a href="#ref-xu2017study">2017</a>)</span>.</p>

<div class="exercise">
<span id="exr:unnamed-chunk-8" class="exercise"><strong>Remark 4.4  </strong></span>Using Lemma <a href="allan-variance-calibration-techniques.html#lem:lemmaavtoacf">4.2</a>, the exact form of the AV for different stationary processes, such as the general class of ARMA models, can easily be derived. Moreover, <span class="citation">Zhang (<a href="#ref-zhang2008allan">2008</a>)</span> provided the theoretical AV for non-stationary processes such as the random walk and ARFIMA models for which the AV, as mentioned earlier, represents a better measure of uncertainty compared to other methods.
</div>

<p><br></p>

<div class="exercise">
<span id="exr:unnamed-chunk-9" class="exercise"><strong>Remark 4.5  </strong></span>Lemma <a href="allan-variance-calibration-techniques.html#lem:lemmaavtoacf">4.2</a> was extended to non-stationary processes in <span class="citation">Xu et al. (<a href="#ref-xu2017study">2017</a>)</span>.
</div>

<p><br></p>

<div class="example">
<p><span id="exm:avma1" class="example"><strong>Example 4.1  (Theoretical AV of an MA(1) process)  </strong></span>From the autocovariance we obtain</p>
<span class="math display">\[\begin{equation*}
        \rho(h) = \text{corr}\left(X_t, X_{t-h} \right) =\left\{
  \begin{array}{cl}
    1 &amp;\text{if } h = 0\\
    \frac{\theta}{1 + \theta^2} &amp;\text{if } |h| = 1\\
    0 &amp;\text{if } |h| &gt; 1.\\
  \end{array}
\right.
\end{equation*}\]</span>
<p>We can now apply the formula given in Lemma <a href="allan-variance-calibration-techniques.html#lem:lemmaavtoacf">4.2</a>, which leads to</p>
<span class="math display">\[\begin{equation*}
    \begin{aligned}
        \text{AV}_j \left(X_t \right)  &amp;= \frac{\left(1 + \theta^2 \right) \sigma^2}{\tau_j^2} \bigg(\tau_j
   + \sum_{i=1}^{\tau_j-1} i \left[2 \rho(\tau_j-i) - \rho(i) - \rho(2\tau_j-i)\right]\bigg)\\
   &amp;=\frac{\left(1 + \theta^2 \right) \sigma^2}{\tau_j^2} \bigg(\tau_j
   + 2 \sum_{i=1}^{\tau_j-1} i \rho(\tau_j-i) -\sum_{i=1}^{\tau_j-1} i \rho(i) - \sum_{i=1}^{\tau_j-1} i \rho(2\tau_j-i)\bigg)\\
   &amp;=\frac{\left(1 + \theta^2 \right) \sigma^2}{\tau_j^2} \left(\tau_j
   + 2  (\tau_j - 1) \rho(1) -  \rho(1) \right)\\
   &amp;=\frac{\left(1 + \theta^2 \right) \sigma^2}{\tau_j^2} \bigg(\tau_j
   +  (2\tau_j - 3) \frac{\theta}{1 + \theta^2} \bigg).
   \end{aligned}
\end{equation*}\]</span>
<div style="text-align: right">
<span class="math inline">\(\LARGE{\bullet}\)</span>
</div>
</div>

</div>
<div id="estimation" class="section level2">
<h2><span class="header-section-number">4.3</span> Estimation</h2>
<p>Several estimators of the AV have been introduced in the literature. The most commonly is (probably) the Maximum-Overlapping AV (MOAV) estimator proposed by <span class="citation">Percival and Guttorp (<a href="#ref-percival1994long">1994</a>)</span>, which is defined as follows:</p>

<div class="definition">
<span id="def:defmoav" class="definition"><strong>Definition 4.2  (Maximum-Overlapping AV Estimator)  </strong></span>The MOAV is defined as:
<span class="math display">\[\begin{eqnarray}
\label{eq:MOAVNS_est}
        \hat{\phi}_j^2 \equiv \widehat{\text{AV}}_j \left(X_t \right) = \frac{1}{2 \left(T - 2\tau_j + 1\right)} \sum_{k = 2 \tau_j}^{T} \left(\bar{X}_{k}^{(j)} - \bar{X}_{k-\tau_j}^{(j)} \right)^2.
\end{eqnarray}\]</span>
</div>

<p>We will now study the properties of this estimator through the following lemmas.</p>
<div id="consistency-1" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Consistency</h3>

<div class="lemma">
<p><span id="lem:lemmconsistencyAV" class="lemma"><strong>Lemma 4.3  (Consistency)  </strong></span>Let <span class="math inline">\((X_t)\)</span> be such that:</p>
<ul>
<li><span class="math inline">\((X_t - X_{t-1})\)</span> is a (strongly) stationary process,</li>
<li><span class="math inline">\((X_t - X_{t-1})^2\)</span> has absolutely summable covariance structure,</li>
<li><span class="math inline">\(\mathbb{E}\left[(X_t - X_{t-1})^4\right] &lt; \infty\)</span>,</li>
</ul>
Then, we have
<span class="math display">\[\widehat{\text{AV}}_j \left(X_t \right) \overset{ \mathcal{P} }{\longrightarrow} \text{AV}_j \left(X_t \right).\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> TO BE ADDED
</div>


<div class="exercise">
<span id="exr:unnamed-chunk-11" class="exercise"><strong>Remark 4.6  (Connection to Wavelet Variance)  </strong></span>This result is closely related by the results of <span class="citation">Percival (<a href="#ref-percival1995estimation">1995</a>)</span> on the wavelet variance. We shall explore the connection between the AV and wavelet variance in the next section.
</div>

</div>
<div id="asymptotic-normality-1" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Asymptotic Normality</h3>
<p>Compare to consistency, the asymptotic normality requires stronger conditions given in the following lemma.</p>

<div class="lemma">
<p><span id="lem:lemmaasyav" class="lemma"><strong>Lemma 4.4  (Asymptotic normality)  </strong></span>Let <span class="math inline">\((X_t)\)</span> be such that:</p>
<ul>
<li><span class="math inline">\((X_t - X_{t-1})\)</span> is a (strongly) stationary process.</li>
<li><span class="math inline">\((X_t - X_{t-1})\)</span> is strong mixing process with mixing coefficient <span class="math inline">\(\alpha(n)\)</span> such that <span class="math inline">\(\sum_{n=1}^{\infty} \alpha(n)^{\frac{\delta}{2+\delta}} &lt; \infty\)</span> for some <span class="math inline">\(\delta &gt; 0\)</span>.</li>
<li><span class="math inline">\(\mathbb{E}\left[\left(X_t - X_{t-1}\right)^{4+\delta}\right] &lt; \infty\)</span> for some <span class="math inline">\(\delta &gt; 0\)</span>.</li>
</ul>
Then, under these conditions we have that <span class="math display">\[\sqrt{T}\left(\widehat{\text{AV}}_j \left(X_t \right) - \text{AV}_j \left(X_t \right) \right) \overset{ \mathcal{D} }{\longrightarrow}
\mathcal{N}(0, \sigma^2_T/T),\]</span> where <span class="math inline">\(\sigma^2_T \equiv \sum_{h = -\infty}^{\infty}\text{cov}\left( \left(\bar{X}_{t}^{(j)} - \bar{X}_{t-\tau_j}^{(j)} \right)^2, \left(\bar{X}_{t+h}^{(j)} - \bar{X}_{t+h-\tau_j}^{(j)} \right)^2 \right)\)</span>.
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> TO BE ADDED
</div>

</div>
<div id="confidence-interval-of-the-moav-estimator" class="section level3">
<h3><span class="header-section-number">4.3.3</span> Confidence Interval of the MOAV Estimator</h3>
<p>Based on the asymptotic normality results (Lemma ), we can construct the <span class="math inline">\(1-\alpha\)</span> confidence intervals for <span class="math inline">\(\widehat{\text{AV}}_j \left(X_t \right)\)</span> as
%
<span class="math display">\[\begin{equation*}
    \text{CI}\left(\text{AV}_j \left(X_t \right)\right) = \left[ \widehat{\text{AV}}_j \left(X_t \right) \pm z_{1 - \frac{\alpha}{2}} \frac{\sigma_{T}}{T} \right],
\end{equation*}\]</span>
%
where <span class="math inline">\(z_{1 - \frac{\alpha}{2}} \equiv \boldsymbol{\Phi}^{-1}\left( 1- \frac{\alpha}{2} \right)\)</span> is the <span class="math inline">\((1- \frac{\alpha}{2})\)</span> quantile of a standard normal distribution.\
However, the so called ``Long-Run Varianceââ <span class="math inline">\(\sigma^2_{T}\)</span> is usually unknown. Many methods have been proposed to consistently estimate it under mild conditions <span class="citation">(see e.g. Newey and West <a href="#ref-newey1986simple">1986</a>)</span>.</p>

<div class="exercise">
<span id="exr:unnamed-chunk-13" class="exercise"><strong>Remark 4.7  </strong></span>Gaussian-based confidence intervals are often problematic with the AV as the lower limit of CI can very well be negative. Next, we will discuss an alternative method to construct the CI for such statistic.
</div>

</div>
</div>
<div id="allan-variance-based-estimation" class="section level2">
<h2><span class="header-section-number">4.4</span> Allan variance based estimation</h2>
<div id="allan-variance-log-log-representation" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Allan Variance log-log Representation</h3>
<p>As illustrated in Lemmas <a href="allan-variance-calibration-techniques.html#lem:lemmavpsd">4.1</a> and <a href="allan-variance-calibration-techniques.html#lem:lemmaavtoacf">4.2</a> the AV depends on the properties of the stochastic process <span class="math inline">\((X_t)\)</span>. We will see that ``log-logââ representation of the AV is often useful for the identify various processes that may compose <span class="math inline">\((X_t)\)</span>.</p>
<p>For example, letâs suppose that <span class="math inline">\(X_t\)</span> is a white noise process. We showed in REF MISSING HERE that the theoretical AV of such process is given</p>
<span class="math display">\[\begin{equation*}
      \phi_j^2 \equiv \text{AV}_j(X_t) = \frac{\sigma^2}{\tau_j}.
\end{equation*}\]</span>
<p>Therefore, we have that the Allan Deviation or AD (i.e. <span class="math inline">\(\sqrt{\text{AV}_j(X_t)}\)</span> or <span class="math inline">\(\phi_j\)</span>) is such that</p>
<span class="math display">\[\begin{equation}
       \log\left( \phi_j \right) = \log \left(\sqrt{\frac{\sigma^2}{\tau_j}}\right) = \log \left(\sigma\right) - \frac{1}{2} \log (\tau_j).
       \label{eq:av:wn}
\end{equation}\]</span>
<p>Thus, the log of the AD is linear in <span class="math inline">\(\tau_j\)</span> with a slope of <span class="math inline">\(-1/2\)</span> and with intercept <span class="math inline">\(\log (\sigma)\)</span>. Let us start by considering a simple simulated example.</p>
</div>
<div id="allan-deviation-of-a-wn-process" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Allan Deviation of a WN process</h3>
<p>Simulation based on a white noise process with <span class="math inline">\(\sigma^2 = 10^2\)</span> and <span class="math inline">\(T = 10^5\)</span>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load packages</span>
<span class="kw">library</span>(av)      <span class="co"># Package for Allan Variance functions</span>
<span class="kw">library</span>(simts)   <span class="co"># Package for time series simulations</span>

<span class="co"># Simulate white noise</span>
Xt =<span class="st"> </span><span class="kw">gen_gts</span>(<span class="kw">WN</span>(<span class="dt">sigma2 =</span> <span class="dv">1</span>), <span class="dt">n =</span> <span class="dv">10</span><span class="op">^</span><span class="dv">5</span>)

<span class="co"># Compute allan variance</span>
av =<span class="st"> </span><span class="kw">avar</span>(Xt)

<span class="co"># Allan Variance log-log Representation</span>
<span class="kw">plot</span>(av)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:exampleAVwhitenoise"></span>
<img src="scis_files/figure-html/exampleAVwhitenoise-1.png" alt="ADD A NICE CAPTION" width="624" />
<p class="caption">
Figure 4.1: ADD A NICE CAPTION
</p>
</div>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-allan1966statistics">
<p>Allan, D. W. 1966. âStatistics of Atomic Frequency Standards.â <em>Proceedings of the IEEE</em> 54 (2). IEEE:221â30.</p>
</div>
<div id="ref-elsheimy08av">
<p>El-Sheimy, N., H. Hou, and X. Niu. 2008. âAnalysis and Modeling of Inertial Sensors using Allan Variance.â <em>IEEE Transactions on Instrumentation and Measurement</em> 57 (1). IEEE:140â49.</p>
</div>
<div id="ref-percival1994long">
<p>Percival, D. B., and P. Guttorp. 1994. âLong-memory processes, the Allan variance and wavelets.â In <em>Wavelet Analysis and Its Applications</em>, 4:325â44. Elsevier.</p>
</div>
<div id="ref-percival2006wavelet">
<p>Percival, D. B., and A. T. Walden. 2006. <em>Wavelet methods for time series analysis</em>. Cambridge university press.</p>
</div>
<div id="ref-greenhall1998spectral">
<p>Greenhall, C. A. 1998. âSpectral Ambiguity of Allan Variance.â <em>IEEE Transactions on Instrumentation and Measurement</em> 47 (3). IEEE:623â27.</p>
</div>
<div id="ref-zhang2008allan">
<p>Zhang, N. F. 2008. âAllan Variance of Time Series Models for Measurement Data.â <em>Metrologia</em> 45 (5). IOP Publishing:549.</p>
</div>
<div id="ref-xu2017study">
<p>Xu, H., StÃ©phane Guerrier, Roberto Molinari, and Yuming Zhang. 2017. âA Study of the Allan Variance for Constant-Mean Nonstationary Processes.â <em>IEEE Signal Processing Letters</em> 24 (8). IEEE:1257â60.</p>
</div>
<div id="ref-percival1995estimation">
<p>Percival, D. P. 1995. âOn Estimation of the Wavelet Variance.â <em>Biometrika</em> 82 (3). Oxford University Press:619â31.</p>
</div>
<div id="ref-newey1986simple">
<p>Newey, Whitney K, and Kenneth D West. 1986. âA Simple, Positive Semi-Definite, Heteroskedasticity and Autocorrelationconsistent Covariance Matrix.â National Bureau of Economic Research Cambridge, Mass., USA.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="properties-of-statistical-estimators.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="the-generalized-method-of-wavelet-moments.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/03-allanvariance.Rmd",
"text": "Edit"
},
"download": ["scis.pdf", "scis.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
