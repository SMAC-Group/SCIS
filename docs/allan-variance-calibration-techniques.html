<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>An Introduction to Inertial Sensors Stochastic Calibration</title>
  <meta name="description" content="TO DO">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="An Introduction to Inertial Sensors Stochastic Calibration" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="TO DO" />
  <meta name="github-repo" content="SMAC-Group/SCIS" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="An Introduction to Inertial Sensors Stochastic Calibration" />
  
  <meta name="twitter:description" content="TO DO" />
  

<meta name="author" content="StÃ©phane Guerrier, Roberto Molinari, Yuming Zhang, Haotian Xu, Gaetan Bakalli, Ahmed Radi and Mucyo Karemera">


<meta name="date" content="2018-07-22">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="properties-of-statistical-estimators.html">
<link rel="next" href="the-generalized-method-of-wavelet-moments.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="timeseries.html"><a href="timeseries.html"><i class="fa fa-check"></i><b>2</b> Introduction to Time Series Analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="timeseries.html"><a href="timeseries.html#time-series"><i class="fa fa-check"></i><b>2.1</b> Time Series</a></li>
<li class="chapter" data-level="2.2" data-path="timeseries.html"><a href="timeseries.html#dependence-within-time-series"><i class="fa fa-check"></i><b>2.2</b> Dependence within Time Series</a></li>
<li class="chapter" data-level="2.3" data-path="timeseries.html"><a href="timeseries.html#stationarity"><i class="fa fa-check"></i><b>2.3</b> Stationarity</a></li>
<li class="chapter" data-level="2.4" data-path="timeseries.html"><a href="timeseries.html#linear-processes"><i class="fa fa-check"></i><b>2.4</b> Linear Processes</a></li>
<li class="chapter" data-level="2.5" data-path="timeseries.html"><a href="timeseries.html#basic-time-series-models"><i class="fa fa-check"></i><b>2.5</b> Basic Time Series Models</a></li>
<li class="chapter" data-level="2.6" data-path="timeseries.html"><a href="timeseries.html#fundamental-representations-of-time-series"><i class="fa fa-check"></i><b>2.6</b> Fundamental Representations of Time Series</a></li>
<li class="chapter" data-level="2.7" data-path="timeseries.html"><a href="timeseries.html#estimation-problems-with-time-series"><i class="fa fa-check"></i><b>2.7</b> Estimation Problems with Time Series</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html"><i class="fa fa-check"></i><b>3</b> Properties of Statistical Estimators</a><ul>
<li class="chapter" data-level="3.1" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#extremum-estimators"><i class="fa fa-check"></i><b>3.1</b> Extremum Estimators</a></li>
<li class="chapter" data-level="3.2" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#consistency"><i class="fa fa-check"></i><b>3.2</b> Consistency</a><ul>
<li class="chapter" data-level="3.2.1" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#consistency-of-extremum-estimators"><i class="fa fa-check"></i><b>3.2.1</b> Consistency of Extremum Estimators</a></li>
<li class="chapter" data-level="3.2.2" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#verification-of-condition-c1"><i class="fa fa-check"></i><b>3.2.2</b> Verification of Condition C1</a></li>
<li class="chapter" data-level="3.2.3" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#verification-of-condition-c4"><i class="fa fa-check"></i><b>3.2.3</b> Verification of Condition C4</a></li>
<li class="chapter" data-level="3.2.4" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#consistency-of-sample-autocovariance-and-autocorrelation-functions"><i class="fa fa-check"></i><b>3.2.4</b> Consistency of Sample AutoCovariance and AutoCorrelation Functions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#asymptotic-normality"><i class="fa fa-check"></i><b>3.3</b> Asymptotic Normality</a><ul>
<li class="chapter" data-level="3.3.1" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#clt-for-iid-random-variables"><i class="fa fa-check"></i><b>3.3.1</b> CLT for iid Random Variables</a></li>
<li class="chapter" data-level="3.3.2" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#clt-for-dependent-processes"><i class="fa fa-check"></i><b>3.3.2</b> CLT for Dependent Processes</a></li>
<li class="chapter" data-level="3.3.3" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#asymptotic-normality-of-sample-autocovariance-and-autocorrelation-functions"><i class="fa fa-check"></i><b>3.3.3</b> Asymptotic Normality of Sample AutoCovariance and AutoCorrelation Functions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html"><i class="fa fa-check"></i><b>4</b> Allan Variance Calibration Techniques</a><ul>
<li class="chapter" data-level="4.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#review-on-mle-based-methods"><i class="fa fa-check"></i><b>4.1</b> Review on MLE-based Methods</a></li>
<li class="chapter" data-level="4.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#an-introduction-of-the-allan-variance"><i class="fa fa-check"></i><b>4.2</b> An Introduction of the Allan Variance</a><ul>
<li class="chapter" data-level="4.2.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#definition-of-the-allan-variance"><i class="fa fa-check"></i><b>4.2.1</b> Definition of the Allan Variance</a></li>
<li class="chapter" data-level="4.2.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#spectral-ambiguity-of-the-allan-variance"><i class="fa fa-check"></i><b>4.2.2</b> Spectral Ambiguity of the Allan Variance</a></li>
<li class="chapter" data-level="4.2.3" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#properties-of-the-allan-variance"><i class="fa fa-check"></i><b>4.2.3</b> Properties of the Allan Variance</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#estimation-of-the-allan-variance"><i class="fa fa-check"></i><b>4.3</b> Estimation of the Allan Variance</a><ul>
<li class="chapter" data-level="4.3.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#consistency-of-the-moav-estimator"><i class="fa fa-check"></i><b>4.3.1</b> Consistency of the MOAV Estimator</a></li>
<li class="chapter" data-level="4.3.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#asymptotic-normality-of-the-moav-estimator"><i class="fa fa-check"></i><b>4.3.2</b> Asymptotic Normality of the MOAV Estimator</a></li>
<li class="chapter" data-level="4.3.3" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#confidence-interval-of-the-moav-estimator"><i class="fa fa-check"></i><b>4.3.3</b> Confidence Interval of the MOAV Estimator</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#allan-variance-based-estimation"><i class="fa fa-check"></i><b>4.4</b> Allan Variance-based Estimation</a><ul>
<li class="chapter" data-level="4.4.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#allan-variance-log-log-representation"><i class="fa fa-check"></i><b>4.4.1</b> Allan Variance log-log Representation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-generalized-method-of-wavelet-moments.html"><a href="the-generalized-method-of-wavelet-moments.html"><i class="fa fa-check"></i><b>5</b> The Generalized Method of Wavelet Moments</a></li>
<li class="chapter" data-level="6" data-path="extensions.html"><a href="extensions.html"><i class="fa fa-check"></i><b>6</b> Extensions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Inertial Sensors Stochastic Calibration</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="allan-variance-calibration-techniques" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Allan Variance Calibration Techniques</h1>
<p>In this chapter, we will study the Allan variance (AV) and the corresponding calibration techniques. For this purpose, this chapter is organized in the following order:</p>
<ul>
<li>Review on MLE-based methods;</li>
<li>An introduction of Allan variance;</li>
<li>Allan variance-based estimation.</li>
</ul>
<div id="review-on-mle-based-methods" class="section level2">
<h2><span class="header-section-number">4.1</span> Review on MLE-based Methods</h2>
<p>In general, inertial sensor stochastic calibration involves the estimation of the parameters of composite stochastic processes. These models are typically difficult to estimate because of their latent features. In the definition below we characterize the class of composite stochastic processes we shall consider here.</p>

<div class="definition">
<p><span id="def:defCompStocProcesses" class="definition"><strong>Definition 4.1  (Composite Stochastic Processes for IMU Calibration)  </strong></span>Let <span class="math inline">\((X_t)\)</span> be a sum of latent independent stochastic process such that:</p>
<ul>
<li><p><span class="math inline">\((X_t)\)</span> is made of a sum which includes a subset or all processes in the set <span class="math inline">\(\{\)</span><strong>QN</strong>, <strong>WN</strong>, <strong>AR1</strong>, <strong>RW</strong>, <strong>DR</strong><span class="math inline">\(\}\)</span>, where processes in the subset <span class="math inline">\(\{\)</span><strong>QN</strong>, <strong>WN</strong>, <strong>RW</strong>, <strong>DR</strong><span class="math inline">\(\}\)</span> can be included only up to once and process <strong>AR1</strong> can be included <span class="math inline">\(k\)</span> times (<span class="math inline">\(0 \leq k &lt; \infty\)</span>).</p></li>
<li>Let <span class="math inline">\(\mathcal{Q}\)</span> denote an arbitrary compact subset of <span class="math inline">\(\mathbb{R}^+\)</span>. Then, the innovation process for processes <strong>WN</strong>, <strong>RW</strong> and <strong>AR1</strong> have respective variances <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(\gamma^2\)</span> and <span class="math inline">\(\nu^2\)</span> such that <span class="math inline">\(\sigma^2, \gamma^2 \text{ and } \nu^2 \in \mathcal{Q}\)</span> and processes <strong>QN</strong> and <strong>DR</strong> have <span class="math inline">\(Q^2 \in \mathcal{Q}\)</span> and <span class="math inline">\(|\omega|\in \mathcal{Q}\)</span> respectively.
</div>
<p></p></li>
</ul>
<p>There are three main class of estimation techniques that can be used for the estimation of the class of composite stochastic processes for IMU calibration. The methods are the following:</p>
<ul>
<li><p><strong>Maximum likelihood based methods</strong>: Although these methods are optimal in theory, their applicability is extremely limited due to numerical reasons and tends to perform badly in practice.</p></li>
<li><p><strong>Allan variance-based methods</strong>: This class of method is the most popular approach for IMU calibration. However, they typically lead to inconsistent estimators and their finite sample performance is often much lower than GMWM-based technique.</p></li>
<li><p><strong>GMWM and related methods</strong>: In our (very biased) opinion, these techniques are currently the optimal choice for the estimation of the parameters of the class of processes considered in Definition <a href="allan-variance-calibration-techniques.html#def:defCompStocProcesses">4.1</a>. As we will see, this method is in fact a formalized version Allan variance-based methods.</p></li>
</ul>
<p>Maximum likelihood based approaches are generally inappropriate for the estimation of the class of processes considered in Definition <a href="allan-variance-calibration-techniques.html#def:defCompStocProcesses">4.1</a>. In this chapter, we shall avoid a technical discussion on likelihood based method and refer the readers to <span class="citation">Stebler et al. (<a href="#ref-stebler2011constrained">2011</a>)</span> and <span class="citation">Guerrier, Molinari, and Balamuta (<a href="#ref-guerrier2016discussion">2016</a>)</span> for more details. Instead, we will consider an example to illustrate the numerical issues of this technique.</p>

<div class="example">
<span id="exm:exMLEmethod" class="example"><strong>Example 4.1  (MLE-based IMU calibration)  </strong></span>Suppose we have a composite stochastic process composed of a WN and an AR1 process, i.e.
<span class="math display">\[\begin{equation*}
\begin{aligned}
Y_t &amp;= \phi_0 Y_{t-1} + Z_t, \;\;\; Z_t \overset{iid}{\sim} \mathcal{N}\left(0, \nu^2_0 \right), \\
U_t &amp;\overset{iid}{\sim} \mathcal{N}\left(0, \sigma^2_0 \right), \;\;\;\; X_t = Y_t + U_t.
\end{aligned}
\end{equation*}\]</span>
<p>Then we want to estimate the parameter <span class="math inline">\(\boldsymbol{\theta}_0 = \left[\phi_0 \;\; \nu^2_0 \;\; \sigma^2_0\right]^T\)</span>.</p>
Since the process is Gaussian, we have
<span class="math display">\[\begin{equation*}
\begin{aligned}
\mathbf{X} \sim \mathcal{N} \left(\mathbf{0}, \boldsymbol{\Sigma}(\boldsymbol{\theta}_0)\right),
\end{aligned}
\end{equation*}\]</span>
where <span class="math inline">\(\mathbf{X} \equiv [X_1, ..., X_T]^T\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}(\boldsymbol{\theta}_0) \equiv \text{Cov}(\mathbf{X})\)</span>. Since <span class="math inline">\(Y_t\)</span> and <span class="math inline">\(U_t\)</span> are independent, we have
<span class="math display">\[\begin{equation*}
\boldsymbol{\Sigma}(\boldsymbol{\theta}_0) = \text{Cov}(\mathbf{X}) = \text{Cov}(\mathbf{Y}) + \text{Cov}(\mathbf{U}) = \frac{\nu^2_0}{1 - \phi_0^2} \left[ \phi_0^{|i-j|}\right]_{i,j = 1, ..., T} + \sigma^2_0 \mathbf{I}_T,
\end{equation*}\]</span>
<p>where <span class="math inline">\(\mathbf{Y} \equiv [Y_1, ..., Y_T]^T\)</span>, <span class="math inline">\(\mathbf{U} \equiv [U_1, ..., U_T]^T\)</span> and where <span class="math inline">\(\mathbf{I}_T\)</span> denotes the identity matrix of dimension <span class="math inline">\(T\)</span>. Note that the form of <span class="math inline">\(\text{Cov}(\mathbf{Y})\)</span> is due to the autocovariance of an AR1 which has been discussed in Chapter 2.</p>
So we can now write the log-likelihood function of the model considered here which, up to a constant, can be expressed as
<span class="math display">\[\begin{equation*}
\mathcal{L}\left(\boldsymbol{\theta} | \mathbf{X} \right) = - \log \left( \det \left( \boldsymbol{\Sigma}(\boldsymbol{\theta}) \right)\right)  - \mathbf{X}^T \boldsymbol{\Sigma}(\boldsymbol{\theta})^{-1} \mathbf{X}.
\end{equation*}\]</span>
Therefore, we can find the maximum likelihood estimator for <span class="math inline">\(\boldsymbol{\theta}_0\)</span>:
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; \mathcal{L}\left(\boldsymbol{\theta} | \mathbf{X} \right) = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmin}} \; \text{log} \left( \det \left( \boldsymbol{\Sigma}(\boldsymbol{\theta}) \right)\right)  +  \mathbf{X}^T \boldsymbol{\Sigma}(\boldsymbol{\theta})^{-1} \mathbf{X}.
\end{equation*}\]</span>
<p>Unfortunately, the applicability of this estimator is essentially impossible when <span class="math inline">\(T &gt; 10^5\)</span> since every evaluation of this the function <span class="math inline">\(\mathcal{L}\left(\boldsymbol{\theta} | \mathbf{X} \right)\)</span> requires to invert a <span class="math inline">\(T \times T\)</span> matrix, which entails a considerable (and often unrealistic) computational burden.</p>
An alternative approach to compute maximum likelihood estimator for <span class="math inline">\(\boldsymbol{\theta}_0\)</span> is based on the EM-algorithm of <span class="citation">Dempster, Laird, and Rubin (<a href="#ref-dempster1977maximum">1977</a>)</span>. If the process <span class="math inline">\((Y_t)\)</span> (or <span class="math inline">\(U_t\)</span>) was observed, then we could easily estimate the parameters of our model by considering separately the likelihood of both processes. Since the composite process is a state-space model we could use the following approach:
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; \mathcal{L}\left(\boldsymbol{\theta} | \mathbf{X}, \hat{\mathbf{Y}}(\boldsymbol{\theta}) \right),
\end{equation*}\]</span>
<p>where <span class="math inline">\(\hat{\mathbf{Y}}(\boldsymbol{\theta})\)</span> denotes the estimation of <span class="math inline">\(\mathbf{Y}\)</span> (i.e.Â the states) based on a Kalman filter assuming <span class="math inline">\(\boldsymbol{\theta}\)</span> to be the correct parameter vector. Unfortunately, this approach suffers from the same computational limitations as the maximum likelihood estimator approach.</p>
<div style="text-align: right">
<span class="math inline">\(\LARGE{\bullet}\)</span>
</div>
</div>
<p></p>
</div>
<div id="an-introduction-of-the-allan-variance" class="section level2">
<h2><span class="header-section-number">4.2</span> An Introduction of the Allan Variance</h2>
<div id="definition-of-the-allan-variance" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Definition of the Allan Variance</h3>
<p>The Allan variance (AV) is a statistical technique originally developed in the mid-1960s to study the stability of precision oscillators <span class="citation">(see e.g. Allan <a href="#ref-allan1966statistics">1966</a>)</span>. It can provide information on the types and magnitude of various superimposed noise terms (i.e.Â composite stochastic processes). This method has been adapted to characterize the properties of a variety of devices including inertial sensors <span class="citation">(see El-Sheimy, Hou, and Niu <a href="#ref-elsheimy08av">2008</a>)</span>. The AV is a measure of variability developed for long term memory processes and can in fact be interpreted as a Haar wavelet coefficient variance <span class="citation">(see D. B. Percival and Guttorp <a href="#ref-percival1994long">1994</a>)</span>. We will discuss this connection further on.</p>

<div class="definition">
<p><span id="def:defAV" class="definition"><strong>Definition 4.2  (Allan Variance)  </strong></span>We consider the AV at dyadic scales (<span class="math inline">\(\tau_j\)</span>) starting from local averages of the process which can be denoted as</p>
<span class="math display">\[\begin{equation*} 
    \bar{X}_{t}^{(j)} \equiv \frac{1}{\tau_j} \sum_{i = 1}^{\tau_j} X_{t - \tau_j + i}\, ,
    \label{mean.noav}
\end{equation*}\]</span>
<p>where <span class="math inline">\(\tau_j \equiv 2^j, \; j \in \left\{x \in \mathbb{N} \, : \; 1 \leq x &lt; \log_2 (T) - 1 \right\}\)</span> and therefore determines the number of consecutive observations considered for the average. Then, the AV is defined as</p>
<span class="math display">\[\begin{equation*}
    \text{AV}_j \left(X_t \right) \equiv \frac{1}{2} \, \mathbb{E}\left[ \left(\bar{X}_{t}^{(j)} - \bar{X}_{t-\tau_j}^{(j)} \right)^2 \right].
\end{equation*}\]</span>
</div>
<p></p>

<div class="exercise">
<span id="exr:alterDefScale" class="exercise"><strong>Remark 4.1  (Alternative scale definition)  </strong></span>The definition of the AV is actually valid for <span class="math inline">\(\tau_j = \lfloor2^j\rfloor\)</span> with <span class="math inline">\(j \in \left\{x \in \mathbb{R} \, : \; 1 \leq x &lt; \log_2 (T) - 1 \right\}\)</span>. In some case, it could be used to consider this alternative definition <span class="citation">(see e.g. El-Sheimy, Hou, and Niu <a href="#ref-elsheimy08av">2008</a>)</span> but we shall restrict ourselves here to the case where <span class="math inline">\(j \in \left\{x \in \mathbb{N} \, : \; 1 \leq x &lt; \log_2 (T) - 1 \right\}\)</span>.
</div>
<p></p>

<div class="exercise">
<span id="exr:notationAV" class="exercise"><strong>Remark 4.2  (Notation of the Allan Variance)  </strong></span>For notational simplicity, we may sometimes replace <span class="math inline">\(\text{AV}_j \left(X_t \right)\)</span> by simply <span class="math inline">\(\phi_j^2\)</span> when the dependence of the AV to the process <span class="math inline">\((X_t)\)</span> is evident.
</div>
<p></p>
<p>As highlighted earlier, the AV is, among others, a widely and commonly used approach in engineering for sensor calibration as it is linked to the properties of the process <span class="math inline">\((X_t)\)</span> as shown in the following lemma <span class="citation">(see e.g. D. B. Percival and Walden <a href="#ref-percival2006wavelet">2006</a> for the proof)</span>.</p>

<div class="lemma">
<p><span id="lem:lemmavpsd" class="lemma"><strong>Lemma 4.1  (AV Connection to PSD)  </strong></span>For a stationary process <span class="math inline">\((X_t)\)</span> with PSD <span class="math inline">\(S_{X}(f)\)</span> we have</p>
<span class="math display">\[\begin{equation*}
\phi_j^2 \equiv \text{AV}_j \left(X_t \right) = 4  \int_0^{\infty}  \frac{\sin^4(\pi f \tau_j)}{(\pi f \tau_j)^2} S_{X}(f) df. 
\label{eq:allanvariancePSD_LInk}
\end{equation*}\]</span>
</div>
<p></p>
<p>Therefore, this result establishes a direct connection between the AV and PSD. Therefore a natural question is whether the mapping PSD <span class="math inline">\(\mapsto\)</span> AV is one-to-one. <span class="citation">Greenhall (<a href="#ref-greenhall1998spectral">1998</a>)</span> (see Theorem 1) showed that this is actually not the case. This is illustrated in the followsing Section <a href="Spectral%20Ambiguity%20of%20the%20Allan%20Variance">4.2.2</a>.</p>
</div>
<div id="spectral-ambiguity-of-the-allan-variance" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Spectral Ambiguity of the Allan Variance</h3>
<p>Consider two (independent) stochastic processes <span class="math inline">\((X_t)\)</span> and <span class="math inline">\((Y_t)\)</span> with respective PSD <span class="math inline">\(S_X(f)\)</span> and <span class="math inline">\(S_Y(f)\)</span>. Suppose that <span class="math inline">\(S_X(f) \neq S_Y(f)\)</span>, then the two processes will have the same AV if</p>
<span class="math display">\[\begin{equation*}
     \Delta \equiv \int_0^{\infty}  \frac{\sin^4(\pi f \tau_j)}{(\pi f \tau_j)^2} \Phi(f) df = 0,
\end{equation*}\]</span>
<p>where <span class="math inline">\(\Phi(f) \equiv S_{X}(f) - S_{Y}(f)\)</span>. To show that it is possible that <span class="math inline">\(\Delta = 0\)</span> when <span class="math inline">\(\Phi(f) \neq 0\)</span>, we will use the following critical identity:</p>
<span class="math display">\[\begin{equation*}
\sin^4(x) = \sin^2(x) - \frac{1}{4} \sin^2(2x).
\end{equation*}\]</span>
<p>First, we note that <span class="math inline">\(\Delta\)</span> may be expressed using the above critical identity as follows:</p>
<span class="math display">\[\begin{equation*}
    \begin{aligned}
        \Delta &amp;=  \int_{0}^{\infty} \frac{\sin^4\left(\tau \pi f \right)}{\left(\tau \pi f \right)^2} \Phi(f) df \\
        &amp;= \lim_{n \rightarrow -\infty} \int_{2^{n}}^{\infty} \frac{\sin^2\left(\tau \pi f \right) - \frac{1}{4} \sin^2\left(2 \tau \pi f \right) }{\left(\tau \pi f \right)^2} \Phi(f) df .
    \end{aligned}
\end{equation*}\]</span>
<p>Second, by the change of variable <span class="math inline">\(u = 2f\)</span> in the second term, we obtain</p>
<span class="math display">\[\begin{equation*}
    \begin{aligned}
        \Delta = \lim_{n \rightarrow -\infty} &amp; \Bigg[ \int_{2^{n}}^{\infty} \frac{\sin^2\left(\tau \pi f \right)}{\left(\tau \pi f \right)^2} \Phi(f) df - \frac{1}{2}\int_{2^{n+1}}^{\infty} \frac{\sin^2\left(\tau \pi u \right)}{\left(\tau \pi u \right)^2} \Phi(f) du \Bigg].
    \end{aligned}
\end{equation*}\]</span>
Now suppose that <span class="math inline">\(\Phi(f) = 2 \Phi(2f)\)</span>. In this case, we have <span class="math inline">\(\Phi(f) = 2 \Phi(u)\)</span> and therefore we obtain
<span class="math display">\[\begin{equation*}
    \begin{aligned}
        \Delta &amp;= \lim_{n \rightarrow -\infty} \int_{2^{n}}^{2^{n+1}} \frac{\sin^2\left(\tau \pi f \right)}{\left(\tau \pi f \right)^2} \Phi(f) df = 0.
    \end{aligned}
\end{equation*}\]</span>
<p>This result demonstrates that the mapping from PSD to AV is not necessarily one-to-one. <span class="citation">Greenhall (<a href="#ref-greenhall1998spectral">1998</a>)</span> showed that in the continuous case (i.e. <span class="math inline">\(\tau_j \in \mathbb{R}\)</span>) <span class="math inline">\(\Delta = 0\)</span> if and only if <span class="math inline">\(\Phi(f) = 2 \Phi(2f)\)</span>. However, the ``only ifââ part of this result (while conjectured) is unknown in the discrete case.</p>
</div>
<div id="properties-of-the-allan-variance" class="section level3">
<h3><span class="header-section-number">4.2.3</span> Properties of the Allan Variance</h3>
<p>One reason of explaining the widespread use of the AV for sensor calibration is due to the following additivity property, which is particularly convenient to identify composite stochastic processes (see Definition 2.17).</p>

<div class="corollary">
<p><span id="cor:coroaddav" class="corollary"><strong>Corollary 4.1  (Additivity of the AV)  </strong></span>Consider two (independent) stochastic processes <span class="math inline">\((X_t)\)</span> and <span class="math inline">\((Y_t)\)</span> with respective PSD <span class="math inline">\(S_X(f)\)</span> and <span class="math inline">\(S_Y(f)\)</span>. Suppose that we observe the process <span class="math inline">\(Z_t = X_t + Y_t\)</span>. Then, we have</p>
<span class="math display">\[\begin{equation*}
    \text{AV}_j \left(Z_t \right) = \text{AV}_j \left(X_t \right) + \text{AV}_j \left(Y_t \right).
\end{equation*}\]</span>
</div>
<p></p>

<div class="proof">
<p> <span class="proof"><em>Proof</em> (Additivity of the AV). </span> The proof of Additivity of the AV is direct from Lemma <a href="allan-variance-calibration-techniques.html#lem:lemmavpsd">4.1</a>. Indeed, since <span class="math inline">\(S_Z(f) = S_X(f) + S_Y(f)\)</span>, we have</p>
<span class="math display">\[\begin{equation*}
    \begin{aligned}
    \text{AV}_j \left(Z_t \right) &amp;= 4  \int_0^{\infty}  \frac{\sin^4(\pi f \tau_j)}{(\pi f \tau_j)^2} S_{Z}(f) df\\
    &amp;= 4  \int_0^{\infty}  \frac{\sin^4(\pi f \tau_j)}{(\pi f \tau_j)^2} S_{X}(f) df + 4  \int_0^{\infty}  \frac{\sin^4(\pi f \tau_j)}{(\pi f \tau_j)^2} S_{Y}(f) df\\
    &amp;= \text{AV}_j \left(X_t \right) + \text{AV}_j \left(Y_t \right).
    \end{aligned}
\end{equation*}\]</span>
</div>
<p> <br></p>
<p>Lemma <a href="allan-variance-calibration-techniques.html#lem:lemmavpsd">4.1</a> is an important result which is very convenient to determine the theoretical AV of a certain stochastic process. However, the applicability of this result is often limited since the integral defined in Section <a href="Spectral%20Ambiguity%20of%20the%20Allan%20Variance">4.2.2</a> can be intractable. An alternative to Lemma <a href="allan-variance-calibration-techniques.html#lem:lemmavpsd">4.1</a> has been proposed by <span class="citation">N. F. Zhang (<a href="#ref-zhang2008allan">2008</a>)</span> and is far advantageous from a computational standpoint.</p>

<div class="lemma">
<span id="lem:lemmaavtoacf" class="lemma"><strong>Lemma 4.2  (AV Connection to ACF)  </strong></span>For a stationary process <span class="math inline">\((X_t)\)</span> with variance <span class="math inline">\(\sigma^2_X\)</span> and ACF <span class="math inline">\(\rho(h)\)</span> we have
<span class="math display">\[\begin{equation*}
\text{AV}_j \left(X_t \right)  = \frac{\sigma_X^2}{\tau_j^2} \bigg(\tau_j\left[1-\rho(\tau_j)\right] 
   + \sum_{i=1}^{\tau_j-1} i \left[2 \rho(\tau_j-i) - \rho(i) - \rho(2\tau_j-i)\right]\bigg).
\end{equation*}\]</span>
</div>
<p></p>
<p>The proof of this result is instructive and is presented in <span class="citation">Xu et al. (<a href="#ref-xu2017study">2017</a>)</span>.</p>
<p>Using Lemma <a href="allan-variance-calibration-techniques.html#lem:lemmaavtoacf">4.2</a>, the exact form of the AV for different stationary processes, such as the general class of ARMA models, can be derived. Moreover, <span class="citation">N. F. Zhang (<a href="#ref-zhang2008allan">2008</a>)</span> provided the theoretical AV for non-stationary processes such as the random walk and ARFIMA models for which the AV, as mentioned earlier, represents a better measure of uncertainty compared to other methods.</p>
<p>Lemma <a href="allan-variance-calibration-techniques.html#lem:lemmaavtoacf">4.2</a> was extended to non-stationary processes in <span class="citation">Xu et al. (<a href="#ref-xu2017study">2017</a>)</span>.</p>

<div class="example">
<span id="exm:avma1" class="example"><strong>Example 4.2  (Theoretical AV of an MA(1) Process)  </strong></span>From the autocovariance we obtain
<span class="math display">\[\begin{equation*}
        \rho(h) = \text{corr}\left(X_t, X_{t-h} \right) =\left\{
  \begin{array}{cl}
    1 &amp;\text{if } h = 0\\
    \frac{\theta}{1 + \theta^2} &amp;\text{if } |h| = 1\\
    0 &amp;\text{if } |h| &gt; 1.\\
  \end{array}
\right.
\end{equation*}\]</span>
<p>We can now apply the formula given in Lemma <a href="allan-variance-calibration-techniques.html#lem:lemmaavtoacf">4.2</a>, which leads to</p>
<span class="math display">\[\begin{equation*}
    \begin{aligned}
        \text{AV}_j \left(X_t \right)  &amp;= \frac{\left(1 + \theta^2 \right) \sigma^2}{\tau_j^2} \bigg(\tau_j
   + \sum_{i=1}^{\tau_j-1} i \left[2 \rho(\tau_j-i) - \rho(i) - \rho(2\tau_j-i)\right]\bigg)\\
   &amp;=\frac{\left(1 + \theta^2 \right) \sigma^2}{\tau_j^2} \bigg(\tau_j
   + 2 \sum_{i=1}^{\tau_j-1} i \rho(\tau_j-i) -\sum_{i=1}^{\tau_j-1} i \rho(i) - \sum_{i=1}^{\tau_j-1} i \rho(2\tau_j-i)\bigg)\\
   &amp;=\frac{\left(1 + \theta^2 \right) \sigma^2}{\tau_j^2} \left(\tau_j
   + 2  (\tau_j - 1) \rho(1) -  \rho(1) \right)\\
   &amp;=\frac{\left(1 + \theta^2 \right) \sigma^2}{\tau_j^2} \bigg(\tau_j
   +  (2\tau_j - 3) \frac{\theta}{1 + \theta^2} \bigg).
   \end{aligned}
\end{equation*}\]</span>
<div style="text-align: right">
<span class="math inline">\(\LARGE{\bullet}\)</span>
</div>
</div>
<p></p>
</div>
</div>
<div id="estimation-of-the-allan-variance" class="section level2">
<h2><span class="header-section-number">4.3</span> Estimation of the Allan Variance</h2>
<p>Several estimators of the AV have been introduced in the literature. The most commonly used one is (probably) the Maximum-Overlapping AV (MOAV) estimator proposed by <span class="citation">D. B. Percival and Guttorp (<a href="#ref-percival1994long">1994</a>)</span>, which is defined as follows:</p>

<div class="definition">
<span id="def:defmoav" class="definition"><strong>Definition 4.3  (Maximum-Overlapping AV Estimator)  </strong></span>The MOAV is defined as:
<span class="math display">\[\begin{eqnarray*}
\hat{\phi}_j^2 \equiv \widehat{\text{AV}}_j \left(X_t \right) = \frac{1}{2 \left(T - 2\tau_j + 1\right)} \sum_{k = 2 \tau_j}^{T} \left(\bar{X}_{k}^{(j)} - \bar{X}_{k-\tau_j}^{(j)} \right)^2.
\end{eqnarray*}\]</span>
</div>
<p></p>
<p>We will now study the properties of this estimator through the following lemmas.</p>
<div id="consistency-of-the-moav-estimator" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Consistency of the MOAV Estimator</h3>

<div class="lemma">
<p><span id="lem:lemmconsistencyAV" class="lemma"><strong>Lemma 4.3  (Consistency of the MOAV Estimator)  </strong></span>Let <span class="math inline">\((X_t)\)</span> be such that:</p>
<ul>
<li><span class="math inline">\((X_t - X_{t-1})\)</span> is a (strongly) stationary process,</li>
<li><span class="math inline">\((X_t - X_{t-1})^2\)</span> has absolutely summable covariance structure,</li>
<li><span class="math inline">\(\mathbb{E}\left[(X_t - X_{t-1})^4\right] &lt; \infty\)</span>,</li>
</ul>
Then, we have <span class="math display">\[\widehat{\text{AV}}_j \left(X_t \right) \overset{ \mathcal{P} }{\longrightarrow} \text{AV}_j \left(X_t \right).\]</span>
</div>
<p></p>

<div class="proof">
<p> <span class="proof"><em>Proof</em> (Consistency of the MOAV Estimator). </span> The proof of the result is direct from the theorem of Weak Law of Large Number for Dependent Process in Chapter 3.</p>
Let <span class="math inline">\(Z_t = X_t - X_{t-1}\)</span>, then since <span class="math inline">\(Z_t\)</span> is stationary with mean zero then so is <span class="math inline">\((X_t - X_{t-h})\)</span> for all <span class="math inline">\(h \in \mathbb{Z}\)</span>. This directly implies that <span class="math inline">\(\bar{X}_{t}^{(j)} - \bar{X}_{t-\tau_j}^{(j)}\)</span> is also stationary (since it is based on a linear combination of stationary processes) and so is <span class="math inline">\(Y_t \equiv (\bar{X}_{t}^{(j)} - \bar{X}_{t-\tau_j}^{(j)})^2\)</span> (since it is based on a time invariant transformation of a stationary process). Moreover, there exist constants <span class="math inline">\(c_h\)</span> such that
<span class="math display">\[\begin{equation*}
    \sum_{h = -\infty}^{\infty} \; \gamma_Y (h) = \sum_{h = -\infty}^{\infty} \; c_{|h|} \gamma_{Z^2} (h).
\end{equation*}\]</span>
Therefore, we obtain
<span class="math display">\[\begin{equation*}
    \sum_{h = -\infty}^{\infty} \; |\gamma_Y (h)| = \sum_{h = -\infty}^{\infty} \; |c_{|h|} \gamma_{Z^2} (h)| \leq \sup_{k = 1, ..., \infty} c_k \; \sum_{h = -\infty}^{\infty} \; |\gamma_{Z^2} (h)| &lt; \infty,
\end{equation*}\]</span>
since both terms are bounded. Using the same approach we have that <span class="math inline">\(\mathbb{E}\left[Y_t^2\right]\)</span> is bounded since <span class="math inline">\(\mathbb{E}\left[Z_t^4\right]\)</span> is bounded. Thus, we can apply WLLN for dependent process on the process <span class="math inline">\(Y_t\)</span>, i.e.
<span class="math display">\[\begin{equation*}
    \widehat{\text{AVar}}_j \left(X_t \right) = \frac{1}{2} \bar{Z}_t \overset{\mathcal{P}}{\mapsto} \frac{1}{2} \mathbb{E}[{Z}_t] = \text{AVar}_j \left(X_t \right),
\end{equation*}\]</span>
which concludes the proof.
</div>
<p> <br></p>
<p>This result is closely related by the results of <span class="citation">D. P. Percival (<a href="#ref-percival1995estimation">1995</a>)</span> on the wavelet variance. We shall explore the connection between the AV and wavelet variance later.</p>
</div>
<div id="asymptotic-normality-of-the-moav-estimator" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Asymptotic Normality of the MOAV Estimator</h3>
<p>Compared to consistency, the asymptotic normality of the MOAV estimator requires stronger conditions given in the following lemma.</p>

<div class="lemma">
<p><span id="lem:lemmaasyav" class="lemma"><strong>Lemma 4.4  (Asymptotic Normality of the MOAV Estimator)  </strong></span>Let <span class="math inline">\((X_t)\)</span> be such that:</p>
<ul>
<li><span class="math inline">\((X_t - X_{t-1})\)</span> is a (strongly) stationary process.</li>
<li><span class="math inline">\((X_t - X_{t-1})\)</span> is a strong mixing process with mixing coefficient <span class="math inline">\(\alpha(n)\)</span> such that <span class="math inline">\(\sum_{n=1}^{\infty} \alpha(n)^{\frac{\delta}{2+\delta}} &lt; \infty\)</span> for some <span class="math inline">\(\delta &gt; 0\)</span>.</li>
<li><span class="math inline">\(\mathbb{E}\left[\left(X_t - X_{t-1}\right)^{4+\delta}\right] &lt; \infty\)</span> for some <span class="math inline">\(\delta &gt; 0\)</span>.</li>
</ul>
Then, under these conditions we have that <span class="math display">\[\sqrt{T}\left(\widehat{\text{AVar}}_j \left(X_t \right) - \text{AVar}_j \left(X_t \right) \right) \overset{ \mathcal{D} }{\longrightarrow}
\mathcal{N}(0, \sigma^2_T/T),\]</span> where <span class="math inline">\(\sigma^2_T \equiv \sum_{h = -\infty}^{\infty}\text{cov}\left( \left(\bar{X}_{t}^{(j)} - \bar{X}_{t-\tau_j}^{(j)} \right)^2, \left(\bar{X}_{t+h}^{(j)} - \bar{X}_{t+h-\tau_j}^{(j)} \right)^2 \right)\)</span>.
</div>
<p></p>

<div class="proof">
<p> <span class="proof"><em>Proof</em> (Asymptotic Normality of the MOAV Estimator). </span> The proof of the result is direct from the Central Limit Theorem for <span class="math inline">\(\alpha\)</span>-mixing process in Chapter 3. Let <span class="math inline">\(Z_t = X_t - X_{t-1}\)</span>, then since <span class="math inline">\(Z_t\)</span> is stationary with mean zero, then so is <span class="math inline">\((X_t - X_{t-h})\)</span> for all <span class="math inline">\(h \in \mathbb{Z}\)</span>. This directly implies that <span class="math inline">\(\bar{X}_{t}^{(j)} - \bar{X}_{t-\tau_j}^{(j)}\)</span> is also stationary (since it is based on a linear combination of stationary processes) and so is <span class="math inline">\(Y_t \equiv (\bar{X}_{t}^{(j)} - \bar{X}_{t-\tau_j}^{(j)})^2\)</span> (since it is based on a time invariant transformation of a stationary process). And since <span class="math inline">\((Y_t)\)</span> is a borel-measurable function of <span class="math inline">\(Z_t\)</span>, we have <span class="math inline">\((Y_t)\)</span> is also a strong mixing process with mixing coefficient <span class="math inline">\(\alpha^{\ast}(n) \leq \alpha(n)\)</span>, hence <span class="math inline">\(\sum_{n=1}^{\infty} \alpha^{\ast}(n)^{\delta/2 + \delta} &lt; \infty\)</span> for some <span class="math inline">\(\delta &gt; 0\)</span>. Moreover, since <span class="math inline">\((\bar{X}_{t}^{(j)} - \bar{X}_{t-\tau_j}^{(j)})\)</span> is a linear function of <span class="math inline">\(Z_t\)</span>, and <span class="math inline">\(\mathbb{E}\left[Z_t^{4+\delta}\right] &lt; \infty\)</span>, by triangle inequality, we have <span class="math inline">\(\mathbb{E}\left[(\bar{X}_{t}^{(j)} - \bar{X}_{t-\tau_j}^{(j)})^{4+\delta}\right] &lt; \infty\)</span>.</p>
Thus, we can apply CLT for <span class="math inline">\(\alpha\)</span>-mixing process on <span class="math inline">\(Y_t\)</span>, i.e.
<span class="math display">\[\begin{equation*}
    \sqrt{T}\left(\widehat{\text{AVar}}_j \left(X_t \right) - \text{AVar}_j \left(X_t \right) \right) \overset{\mathcal{D}}{\mapsto} \mathcal{N}(0, \sigma^2_T/T),
\end{equation*}\]</span>
where <span class="math inline">\(\sigma^2_T \equiv \sum_{h = -\infty}^{\infty}\text{Cov}\left( \left(\bar{X}_{t}^{(j)} - \bar{X}_{t-\tau_j}^{(j)} \right)^2, \left(\bar{X}_{t+h}^{(j)} - \bar{X}_{t+h-\tau_j}^{(j)} \right)^2 \right)\)</span>, which concludes the proof.
</div>
<p> <br></p>
</div>
<div id="confidence-interval-of-the-moav-estimator" class="section level3">
<h3><span class="header-section-number">4.3.3</span> Confidence Interval of the MOAV Estimator</h3>
Based on the asymptotic normality results (Lemma ), we can construct the <span class="math inline">\(1-\alpha\)</span> confidence intervals for <span class="math inline">\(\widehat{\text{AVar}}_j \left(X_t \right)\)</span> as
<span class="math display">\[\begin{equation*}
    \text{CI}\left(\text{AVar}_j \left(X_t \right)\right) = \left[ \widehat{\text{AVar}}_j \left(X_t \right) \pm z_{1 - \frac{\alpha}{2}} \frac{\sigma_{T}}{T} \right],
\end{equation*}\]</span>
<p>where <span class="math inline">\(z_{1 - \frac{\alpha}{2}} \equiv \boldsymbol{\Phi}^{-1}\left( 1- \frac{\alpha}{2} \right)\)</span> is the <span class="math inline">\((1- \frac{\alpha}{2})\)</span> quantile of a standard normal distribution.</p>
<p>However, the so-called âLong-Run Varianceâ <span class="math inline">\(\sigma^2_{T}\)</span> is usually unknown. Many methods have been proposed to consistently estimate it under mild conditions <span class="citation">(see e.g. Newey and West <a href="#ref-newey1986simple">1986</a>)</span>.</p>

<div class="exercise">
<span id="exr:unnamed-chunk-1" class="exercise"><strong>Remark 4.3  </strong></span>Gaussian-based confidence intervals are often problematic with the AV as the lower limit of CI can very well be negative. We will discuss an alternative method to construct the CI for such statistic later.
</div>
<p></p>
</div>
</div>
<div id="allan-variance-based-estimation" class="section level2">
<h2><span class="header-section-number">4.4</span> Allan Variance-based Estimation</h2>
<div id="allan-variance-log-log-representation" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Allan Variance log-log Representation</h3>
<p>As illustrated in Lemmas <a href="allan-variance-calibration-techniques.html#lem:lemmavpsd">4.1</a> and <a href="allan-variance-calibration-techniques.html#lem:lemmaavtoacf">4.2</a>, the AV depends on the properties of the stochastic process <span class="math inline">\((X_t)\)</span>. We will see that the âlog-logâ representation of the AV is often useful to identify various processes that may compose <span class="math inline">\((X_t)\)</span>.</p>
For example, letâs suppose that <span class="math inline">\(X_t\)</span> is a white noise process. We showed in <a href="allan-variance-calibration-techniques.html#exm:avma1">4.2</a> that the theoretical AV of such process is given as
<span class="math display">\[\begin{equation*}
      \phi_j^2 \equiv \text{AVar}_j(X_t) = \frac{\sigma^2}{\tau_j}.
\end{equation*}\]</span>
<p>Therefore, we have that the Allan Deviation or AD (i.e. <span class="math inline">\(\sqrt{\text{AVar}_j(X_t)}\)</span> or <span class="math inline">\(\phi_j\)</span>) is such that</p>
<span class="math display">\[\begin{equation*}
\text{log}\left( \phi_j \right) = \text{log} \left(\sqrt{\frac{\sigma^2}{\tau_j}}\right) = \text{log} \left(\sigma\right) - \frac{1}{2} \text{log} (\tau_j).
\end{equation*}\]</span>
<p>Thus, the log of the AD is linear in <span class="math inline">\(\tau_j\)</span> with a slope of <span class="math inline">\(-1/2\)</span> and with intercept <span class="math inline">\(\text{log} (\sigma)\)</span> as shown in the following simple simulated example.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load packages</span>
<span class="kw">library</span>(av)      <span class="co"># Package for Allan Variance functions</span>
<span class="kw">library</span>(simts)   <span class="co"># Package for time series simulations</span>

<span class="co"># Simulate white noise</span>
Xt =<span class="st"> </span><span class="kw">gen_gts</span>(<span class="kw">WN</span>(<span class="dt">sigma2 =</span> <span class="dv">1</span>), <span class="dt">n =</span> <span class="dv">10</span>^<span class="dv">5</span>)

<span class="co"># Compute allan variance</span>
av =<span class="st"> </span><span class="kw">avar</span>(Xt)

<span class="co"># Allan Variance log-log Representation</span>
<span class="kw">plot</span>(av)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:exampleAVwhitenoise"></span>
<img src="scis_files/figure-html/exampleAVwhitenoise-1.png" alt="Simulation based on a white noise process with variance as 1 and number of observations as 10^5." width="624" />
<p class="caption">
Figure 4.1: Simulation based on a white noise process with variance as 1 and number of observations as 10^5.
</p>
</div>
<p>Suppose now that <span class="math inline">\((X_t)\)</span> is a composite stochastic process composed of a WN and a RW. For simplicity, we assume that <span class="math inline">\(X_t = Y_t + W_t\)</span> where <span class="math inline">\(Y_t\)</span> is a WN process with variance <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(W_t\)</span> a RW with variance <span class="math inline">\(\gamma^2\)</span>. We already know that</p>
<span class="math display">\[\begin{equation*}
\text{log}\left( \text{AVar}_j(Y_t)  \right) = \text{log} \left(\sigma\right) - \frac{1}{2} \text{log} (\tau_j).
\end{equation*}\]</span>
<p>and it can be shown (using for example Lemma <a href="allan-variance-calibration-techniques.html#lem:lemmavpsd">4.1</a>) that</p>
<span class="math display">\[\begin{equation*}
\text{AVar}_j(W_t) = \frac{1}{3} \gamma^2 \tau_j,
\end{equation*}\]</span>
<p>and therefore we can obtain</p>
<span class="math display">\[\begin{equation*}
\text{log}\left( \sqrt{\text{AVar}_j(W_t) } \right) = \log \left(\sqrt{\frac{1}{3} \gamma^2 \tau_j}\right) = \text{log} \left(\frac{1}{\sqrt{3}} \gamma\right) + \frac{1}{2} \text{log} (\tau_j).
\end{equation*}\]</span>
Thus, the log of the AD of <span class="math inline">\((Z_t)\)</span> is also linear in <span class="math inline">\(\tau_j\)</span> with a slope of <span class="math inline">\(+1/2\)</span>. By Corollary <a href="allan-variance-calibration-techniques.html#cor:coroaddav">4.1</a> we also have that
<span class="math display">\[\begin{equation*}
\text{AVar}_j(X_t) = \text{AVar}_j(Y_t) + \text{AVar}_j(W_t) =  \frac{\sigma^2}{\tau_j} + \frac{1}{3} \gamma^2 \tau_j.
\end{equation*}\]</span>
<p>This result can be shown in the following simulated example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load packages</span>
<span class="kw">library</span>(av)      <span class="co"># Package for Allan Variance functions</span>
<span class="kw">library</span>(simts)   <span class="co"># Package for time series simulations</span>

<span class="co"># Simulate white noise</span>
Xt =<span class="st"> </span><span class="kw">gen_gts</span>(<span class="kw">WN</span>(<span class="dt">sigma2 =</span> <span class="dv">10</span>^<span class="dv">2</span>), <span class="dt">n =</span> <span class="dv">10</span>^<span class="dv">5</span>) +<span class="st"> </span><span class="kw">gen_gts</span>(<span class="kw">RW</span>(<span class="dt">gamma2 =</span> <span class="fl">0.03</span>^<span class="dv">2</span>), <span class="dt">n =</span> <span class="dv">10</span>^<span class="dv">5</span>)

<span class="co"># Compute allan variance</span>
av =<span class="st"> </span><span class="kw">avar</span>(Xt)

<span class="co"># Allan Variance log-log Representation</span>
<span class="kw">plot</span>(av)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:exampleAVwnrw"></span>
<img src="scis_files/figure-html/exampleAVwnrw-1.png" alt="Simulation based on a white noise process with variance as 10^2 and a random walk process with variance 0.03^2 and number of observations as 10^5." width="624" />
<p class="caption">
Figure 4.2: Simulation based on a white noise process with variance as 10^2 and a random walk process with variance 0.03^2 and number of observations as 10^5.
</p>
</div>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-stebler2011constrained">
<p>Stebler, Y., S. Guerrier, J. Skaloud, and M.-P. Victoria-Feser. 2011. âConstrained Expectation-Maximization Algorithm for Stochastic Inertial Error Modeling: Study of Feasibility.â <em>Measurement Science and Technology</em> 22 (8). IOP Publishing: 085204.</p>
</div>
<div id="ref-guerrier2016discussion">
<p>Guerrier, S., R. Molinari, and J. Balamuta. 2016. âDiscussion on Maximum Likelihood-Based Methods for Inertial Sensor Calibration.â <em>IEEE Sensors Journal</em> 16 (14). IEEE: 5522â3.</p>
</div>
<div id="ref-dempster1977maximum">
<p>Dempster, A. P., N. M. Laird, and D. B. Rubin. 1977. âMaximum Likelihood from Incomplete Data via the EM Algorithm.â <em>Journal of the Royal Statistical Society. Series B (Methodological)</em>. JSTOR, 1â38.</p>
</div>
<div id="ref-allan1966statistics">
<p>Allan, D. W. 1966. âStatistics of Atomic Frequency Standards.â <em>Proceedings of the IEEE</em> 54 (2). IEEE: 221â30.</p>
</div>
<div id="ref-elsheimy08av">
<p>El-Sheimy, N., H. Hou, and X. Niu. 2008. âAnalysis and Modeling of Inertial Sensors using Allan Variance.â <em>IEEE Transactions on Instrumentation and Measurement</em> 57 (1). IEEE: 140â49.</p>
</div>
<div id="ref-percival1994long">
<p>Percival, D. B., and P. Guttorp. 1994. âLong-memory processes, the Allan variance and wavelets.â In <em>Wavelet Analysis and Its Applications</em>, 4:325â44. Elsevier.</p>
</div>
<div id="ref-percival2006wavelet">
<p>Percival, D. B., and A. T. Walden. 2006. <em>Wavelet methods for time series analysis</em>. Cambridge university press.</p>
</div>
<div id="ref-greenhall1998spectral">
<p>Greenhall, C. A. 1998. âSpectral Ambiguity of Allan Variance.â <em>IEEE Transactions on Instrumentation and Measurement</em> 47 (3). IEEE: 623â27.</p>
</div>
<div id="ref-zhang2008allan">
<p>Zhang, N. F. 2008. âAllan Variance of Time Series Models for Measurement Data.â <em>Metrologia</em> 45 (5). IOP Publishing: 549.</p>
</div>
<div id="ref-xu2017study">
<p>Xu, H., StÃ©phane Guerrier, Roberto Molinari, and Yuming Zhang. 2017. âA Study of the Allan Variance for Constant-Mean Nonstationary Processes.â <em>IEEE Signal Processing Letters</em> 24 (8). IEEE: 1257â60.</p>
</div>
<div id="ref-percival1995estimation">
<p>Percival, D. P. 1995. âOn Estimation of the Wavelet Variance.â <em>Biometrika</em> 82 (3). Oxford University Press: 619â31.</p>
</div>
<div id="ref-newey1986simple">
<p>Newey, Whitney K, and Kenneth D West. 1986. âA Simple, Positive Semi-Definite, Heteroskedasticity and Autocorrelationconsistent Covariance Matrix.â National Bureau of Economic Research Cambridge, Mass., USA.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="properties-of-statistical-estimators.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="the-generalized-method-of-wavelet-moments.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/03-allanvariance.Rmd",
"text": "Edit"
},
"download": ["scis.pdf", "scis.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
