<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>An Introduction to Inertial Sensors Stochastic Calibration</title>
  <meta name="description" content="TO DO">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="An Introduction to Inertial Sensors Stochastic Calibration" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="TO DO" />
  <meta name="github-repo" content="SMAC-Group/SCIS" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="An Introduction to Inertial Sensors Stochastic Calibration" />
  
  <meta name="twitter:description" content="TO DO" />
  

<meta name="author" content="StÃ©phane Guerrier, Roberto Molinari, Yuming Zhang, Haotian Xu, Gaetan Bakalli, Ahmed Radi and Mucyo Karemera">


<meta name="date" content="2018-06-29">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="timeseries.html">
<link rel="next" href="allan-variance-calibration-techniques.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="timeseries.html"><a href="timeseries.html"><i class="fa fa-check"></i><b>2</b> Introduction to Time Series Analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="timeseries.html"><a href="timeseries.html#time-series"><i class="fa fa-check"></i><b>2.1</b> Time Series</a></li>
<li class="chapter" data-level="2.2" data-path="timeseries.html"><a href="timeseries.html#latent-time-series-processes-and-composite-stochastic-process"><i class="fa fa-check"></i><b>2.2</b> Latent Time Series Processes and Composite Stochastic Process</a></li>
<li class="chapter" data-level="2.3" data-path="timeseries.html"><a href="timeseries.html#dependence-within-time-series"><i class="fa fa-check"></i><b>2.3</b> Dependence within Time Series</a></li>
<li class="chapter" data-level="2.4" data-path="timeseries.html"><a href="timeseries.html#stationarity"><i class="fa fa-check"></i><b>2.4</b> Stationarity</a></li>
<li class="chapter" data-level="2.5" data-path="timeseries.html"><a href="timeseries.html#linear-processes"><i class="fa fa-check"></i><b>2.5</b> Linear Processes</a></li>
<li class="chapter" data-level="2.6" data-path="timeseries.html"><a href="timeseries.html#fundamental-representations-of-time-series"><i class="fa fa-check"></i><b>2.6</b> Fundamental Representations of Time Series</a></li>
<li class="chapter" data-level="2.7" data-path="timeseries.html"><a href="timeseries.html#estimation-problems-with-time-series"><i class="fa fa-check"></i><b>2.7</b> Estimation Problems with Time Series</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html"><i class="fa fa-check"></i><b>3</b> Properties of Statistical Estimators</a><ul>
<li class="chapter" data-level="3.1" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#extremum-estimators"><i class="fa fa-check"></i><b>3.1</b> Extremum Estimators</a></li>
<li class="chapter" data-level="3.2" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#consistency"><i class="fa fa-check"></i><b>3.2</b> Consistency</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html"><i class="fa fa-check"></i><b>4</b> Allan Variance Calibration Techniques</a><ul>
<li class="chapter" data-level="4.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#spectral-ambiguity-of-the-av"><i class="fa fa-check"></i><b>4.1</b> Spectral Ambiguity of the AV</a></li>
<li class="chapter" data-level="4.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#properties-of-the-allan-variance"><i class="fa fa-check"></i><b>4.2</b> Properties of the Allan Variance</a></li>
<li class="chapter" data-level="4.3" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#estimation"><i class="fa fa-check"></i><b>4.3</b> Estimation</a><ul>
<li class="chapter" data-level="4.3.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#consistency-1"><i class="fa fa-check"></i><b>4.3.1</b> Consistency</a></li>
<li class="chapter" data-level="4.3.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#asymptotic-normality"><i class="fa fa-check"></i><b>4.3.2</b> Asymptotic Normality</a></li>
<li class="chapter" data-level="4.3.3" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#confidence-interval-of-the-moav-estimator"><i class="fa fa-check"></i><b>4.3.3</b> Confidence Interval of the MOAV Estimator</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#allan-variance-based-estimation"><i class="fa fa-check"></i><b>4.4</b> Allan variance based estimation</a><ul>
<li class="chapter" data-level="4.4.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#allan-variance-log-log-representation"><i class="fa fa-check"></i><b>4.4.1</b> Allan Variance log-log Representation</a></li>
<li class="chapter" data-level="4.4.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#allan-deviation-of-a-wn-process"><i class="fa fa-check"></i><b>4.4.2</b> Allan Deviation of a WN process</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-generalized-method-of-wavelet-moments.html"><a href="the-generalized-method-of-wavelet-moments.html"><i class="fa fa-check"></i><b>5</b> The Generalized Method of Wavelet Moments</a></li>
<li class="chapter" data-level="6" data-path="extensions.html"><a href="extensions.html"><i class="fa fa-check"></i><b>6</b> Extensions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Inertial Sensors Stochastic Calibration</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="properties-of-statistical-estimators" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Properties of Statistical Estimators</h1>
<p>In this chapter, we will provide a review of the properties of statistical estimators. This chapter is organized with the following outline:</p>
<ul>
<li>Extremum estimators;</li>
<li>Consistency;</li>
<li>Asymptotic normality.</li>
</ul>
<div id="extremum-estimators" class="section level2">
<h2><span class="header-section-number">3.1</span> Extremum Estimators</h2>
<p>In this section, we will introduce a commonly used class of estimators, extremum estimators, and some examples of it.</p>

<div class="definition">
<span id="def:EstreEst" class="definition"><strong>Definition 3.1  (Extremum Estimators)  </strong></span>Many estimators have a common structure, which is often useful to study their asymptotic properties. One structure or framework is the class of estimators that maximize some objective function, referred to as extremum estimators, which can can be defined as follows:
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\theta}} \equiv \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; \hat{Q}_n(\boldsymbol{\theta})
\end{equation*}\]</span>
where <span class="math inline">\(\boldsymbol{\theta}\)</span> and <span class="math inline">\(\boldsymbol{\Theta}\)</span> denote, respectively, the parameter vector of interest and its set of possible values.
</div>
<p></p>
<p>The vast majority of statistical estimators can be represented as extremum estimators. For example, least squares, maximum likelihood or (generalized) method of moment estimators can all be represented as extremum estimators.</p>

<div class="example">
<span id="exm:ExtreEstLSE" class="example"><strong>Example 3.1  (Least Squares Estimator as Extremum Estimator)  </strong></span>Consider the linear model <span class="math inline">\(\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta}_0 + \boldsymbol{\epsilon}\)</span> where <span class="math inline">\(\boldsymbol{X} \in \mathbb{R}^{n \times p}\)</span> is a full rank constant matrix, <span class="math inline">\(\boldsymbol{\beta} \in {\mathcal{B}} \subseteq \mathbb{R}^p\)</span> and <span class="math inline">\(\epsilon_i \overset{iid}{\sim} \mathcal{N}(0, \sigma_\epsilon^2)\)</span>. Let <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> denote the Least Squares Estimator (LSE) of <span class="math inline">\(\boldsymbol{\beta}_0\)</span>, i.e.
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\beta}} = \left(\boldsymbol{X}^T \boldsymbol{X} \right)^{-1} \boldsymbol{X}^T \boldsymbol{y}. 
\end{equation*}\]</span>
This LSE is an extremum estimator since it can be expressed as
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\beta}} = \underset{\boldsymbol{\beta} \in \mathcal{B}}{\text{argmax}} \; -||\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} ||_2^2.
\end{equation*}\]</span>
</div>
<p></p>

<div class="example">
<span id="exm:ExtreEstMLE" class="example"><strong>Example 3.2  (Maximum Likelihood Estimator as Extremum Estimator)  </strong></span>Let <span class="math inline">\(Z_1, \ldots, Z_n\)</span> be an iid sample with pdf <span class="math inline">\(f(z|\boldsymbol{\theta}_0)\)</span>. The Maximum Likelihood Estimator (MLE) is given by
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; \frac{1}{n} \sum_{i = 1}^{n} \text{log}\left[f\left(z_i| \boldsymbol{\theta}\right)\right].
\end{equation*}\]</span>
<p>Here instead of the actual log-likelihood, we are actually using a normalized log-likelihood, which has no impact on the estimator but the normalized form is more convenient to use when we let <span class="math inline">\(n \to \infty\)</span>.</p>
Therefore, the MLE can be seen as an extremum estimator with
<span class="math display">\[\begin{equation*}
\hat{Q}_n(\boldsymbol{\theta}) = \frac{1}{n} \sum_{i = 1}^{n} \text{log}\left[f\left(z_i| \boldsymbol{\theta}\right)\right].
\end{equation*}\]</span>
</div>
<p></p>

<div class="definition">
<span id="def:GMM" class="definition"><strong>Definition 3.2  (Generalized Method of Moments)  </strong></span>Let <span class="math inline">\(Z_1, \ldots, Z_n\)</span> be an iid sample with pdf <span class="math inline">\(f(z|\boldsymbol{\theta}_0)\)</span>. Suppose that there is a moment function vector <span class="math inline">\(\boldsymbol{g}(z | \boldsymbol{\theta})\)</span> such that <span class="math inline">\(\mathbb{E}[\boldsymbol{g} (z | \boldsymbol{\theta}_0)] = 0\)</span>. Then the Generalized Method of Moments (GMM) estimator of <span class="math inline">\(\boldsymbol{\theta}_0\)</span> is defined as
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; - \left[\frac{1}{n} \sum_{i = 1}^n \boldsymbol{g}(z_i | \boldsymbol{\theta}) \right]^T \widehat{\boldsymbol{W}} \left[\frac{1}{n} \sum_{i = 1}^n \boldsymbol{g}(z_i | \boldsymbol{\theta}) \right],
\end{equation*}\]</span>
<p>where <span class="math inline">\(\widehat{\boldsymbol{W}}\)</span> is an positive definite matrix of appropriate dimension.</p>
Alternatively (but equivalently), we can define GMM estimator as
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; - || \hat{\boldsymbol{\mu}} - \boldsymbol{\mu}(\boldsymbol{\theta}) ||_{\widehat{\boldsymbol{W}}}^2,
\end{equation*}\]</span>
where <span class="math inline">\(|| \boldsymbol{x} ||_{\boldsymbol{A}}^2 = \boldsymbol{x}^T \boldsymbol{A} \boldsymbol{x}\)</span>, and where <span class="math inline">\(\hat{\boldsymbol{\mu}}\)</span> and <span class="math inline">\(\boldsymbol{\mu}(\boldsymbol{\theta})\)</span> denote, respectively, the empirical and model based moments. We will use this form of definition of GMM estimator more often.
</div>
<p></p>

<div class="example">
<span id="exm:ExtreEstGMM" class="example"><strong>Example 3.3  (A Simple GMM Estimator as Extremum Estimator)  </strong></span>Let <span class="math inline">\(Z_i \overset{iid}{\sim} \mathcal{N}(\mu_0, \sigma^2_0)\)</span> and <span class="math inline">\(\boldsymbol{\theta}_0 = (\mu_0, \sigma_0^2)^T\)</span>. Suppose we wish to estimate <span class="math inline">\(\boldsymbol{\theta}_0\)</span> by matching the first three empirical moments with their theoretical counterparts. In this case, a reasonable moment function or condition defining a GMM estimator is given by:
<span class="math display">\[\begin{equation*}
\boldsymbol{g} (Z | \boldsymbol{\theta}) = \begin{bmatrix}
Z - \mu\\
Z^2 - \left(\mu^2 + \sigma^2\right)\\
Z^3 - \left(\mu^3 + 3 \mu \sigma^2\right)
\end{bmatrix}.
\end{equation*}\]</span>
Notice that <span class="math inline">\(\frac{1}{n} \sum_{i=1}^n \boldsymbol{g} (Z_i | \boldsymbol{\theta}) = \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta})\)</span>, where <span class="math inline">\(\hat{\boldsymbol{\gamma}}\)</span> and <span class="math inline">\(\boldsymbol{\gamma}(\boldsymbol{\theta})\)</span> denote, respectively, the empirical and model based moments, i.e.
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\gamma}} = \frac{1}{n}  \sum_{i = 1}^n 
\begin{bmatrix}
Z_i\\
Z_i^2\\
Z_i^3\\
\end{bmatrix}, \;\;\;\;
\boldsymbol{\gamma}(\boldsymbol{\theta}) =  \begin{bmatrix}
\mu\\
\mu^2 + \sigma^2\\
\mu^3 + 3 \mu \sigma^2
\end{bmatrix}.
\end{equation*}\]</span>
Therefore we can write the GMM estimator of <span class="math inline">\(\boldsymbol{\theta}_0\)</span> as
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmin}} \; || \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma} (\boldsymbol{\theta}) ||_{\widehat{\boldsymbol{W}}}^2 = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; - || \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma} (\boldsymbol{\theta}) ||_{\widehat{\boldsymbol{W}}}^2 
\end{equation*}\]</span>
</div>
<p></p>
</div>
<div id="consistency" class="section level2">
<h2><span class="header-section-number">3.2</span> Consistency</h2>
<p>In this section, we will introduce one of the most important properties of statistical estimators, consistency. And we will discuss in details about the conditions for consistency of the extremum estimators.</p>

<div class="definition">
<span id="def:consistency" class="definition"><strong>Definition 3.3  (Consistency)  </strong></span>An estimator <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> is said to be consistent or weakly consistent if it converges in probability to <span class="math inline">\(\boldsymbol{\theta}_0\)</span>, i.e.
<span class="math display">\[\begin{equation*}
\underset{n \to \infty}{\text{lim}} \mathbb{P} (||\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0||_2 \geq \epsilon) = 0,
\end{equation*}\]</span>
for all <span class="math inline">\(\epsilon &gt;0\)</span>.
</div>
<p></p>
<p>In Laymanâs term, consistency simply means that if <span class="math inline">\(n\)</span> is large enough, then <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> will be arbitrarily close to <span class="math inline">\(\boldsymbol{\theta}_0\)</span> (i.e.Â inside of an hypersphere of radius <span class="math inline">\(\epsilon\)</span> centered at <span class="math inline">\(\boldsymbol{\theta}_0\)</span>). This also means the procedure (i.e.Â our estimator <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>) based on unlimited data will be able to identify the underlying truth (i.e. <span class="math inline">\(\boldsymbol{\theta}_0\)</span>).</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-1"></span>
<img src="images/consistency.pdf" alt="Interpretation of consistency" width="80%" />
<p class="caption">
Figure 3.1: Interpretation of consistency
</p>
</div>
<p>To show the consistency of estimators, we often make use of the following two important results:</p>

<div class="theorem">
<span id="thm:WLLN" class="theorem"><strong>Theorem 3.1  (Weak Law of Large Number)  </strong></span>Suppose <span class="math inline">\(X_i\)</span> are iid random variables with finite mean <span class="math inline">\(\mu\)</span> (i.e. <span class="math inline">\(\mathbb{E}[X_i] = \mu\)</span>) and finite variance. Let <span class="math inline">\(\bar{X}_n = \frac{1}{n} \sum_{i = 1}^n X_i\)</span>, then <span class="math inline">\(\bar{X}_n \overset{\mathcal{P}}{\mapsto} \mu\)</span>.
</div>
<p></p>

<div class="theorem">
<span id="thm:CMT" class="theorem"><strong>Theorem 3.2  (Continuous Mapping Theorem)  </strong></span>If <span class="math inline">\(Y_n \overset{\mathcal{P}}{\mapsto} \mu\)</span> and <span class="math inline">\(g(\cdot)\)</span> is a continuous function, then <span class="math inline">\(g(Y_n) \overset{\mathcal{P}}{\mapsto} g(\mu)\)</span>.
</div>
<p></p>
<p>Here we present a simple example to prove the consistency of an estimator with the above two results.</p>

<div class="example">
<span id="exm:ConsistencyExpDist" class="example"><strong>Example 3.4  (Consistency in Exponential Distribution)  </strong></span>Suppose we have an iid sample from exponential distribution, i.e. <span class="math inline">\(X_i \overset{iid}{\sim} \text{exp}(\lambda_0),\; \lambda_0 \in \mathbb{R}^+, \; i = 1,..., n\)</span>. So assuming <span class="math inline">\(X \geq 0\)</span>, the density of <span class="math inline">\(X\)</span> is given by
<span class="math display">\[\begin{equation*}
f(x|\lambda) = \lambda \text{exp}\left( - \lambda x \right).
\end{equation*}\]</span>
<p>In this example, we want to show that the MLE for <span class="math inline">\(\lambda_0\)</span> is a consistent estimator of <span class="math inline">\(\lambda_0\)</span>.</p>
First, we want to find the MLE for <span class="math inline">\(\lambda_0\)</span>. The normalized log-likelihood function is given by
<span class="math display">\[\begin{equation*}
\mathcal{L}(\lambda | X_1, ..., X_n) = \text{log}(\lambda) - \lambda \bar{X}_n.
\end{equation*}\]</span>
By solving
<span class="math display">\[\begin{equation*}
\frac{\partial}{\partial \lambda} \; \mathcal{L}(\lambda | X_1, \ldots, X_n) = \frac{1}{\lambda} - \bar{X}_n = 0,
\end{equation*}\]</span>
we obtain <span class="math inline">\(\hat{\lambda} = \frac{1}{\bar{X}_n}\)</span>. We verify that
<span class="math display">\[\begin{equation*}
\frac{\partial^2}{\partial \lambda^2} \; \mathcal{L}(\lambda | X_1, \ldots, X_n) = -\frac{1}{\lambda^2} &lt; 0,
\end{equation*}\]</span>
<p>which implies that <span class="math inline">\(\hat{\lambda}\)</span> is the maxima of <span class="math inline">\(\mathcal{L}(\lambda | X_1, \ldots, X_n)\)</span>. Therefore, the MLE for <span class="math inline">\(\lambda_0\)</span> is <span class="math inline">\(\hat{\lambda} = \frac{1}{\bar{X}_n}\)</span>.</p>
By Weak Law of Large Number, we have <span class="math inline">\(\bar{X}_n \overset{\mathcal{P}}{\mapsto} \mu\)</span>, where is given by
<span class="math display">\[\begin{equation*}
\mu = \mathbb{E}[X_i] = \int_{0}^{\infty} x \lambda_0 \text{exp}\left( - \lambda_0 x \right) dx = \frac{1}{\lambda_0}.
\end{equation*}\]</span>
And also since the function <span class="math inline">\(g(x) = 1/x\)</span> is continuous in <span class="math inline">\(\mathbb{R}^+\)</span>, we obtain by the Continuous Mapping Theorem that <span class="math inline">\(\hat{\lambda} \overset{\mathcal{P}}{\mapsto} \lambda_0\)</span>, which concludes that the MLE for <span class="math inline">\(\lambda_0\)</span> is a consistent estimator of <span class="math inline">\(\lambda_0\)</span> in exponential distribution.
</div>
<p></p>
<p>However, when considering real-life problems, the above approach based on Weak Law of Large Number and Continuous Mapping Theorem is in general not flexible enough. Therefore, we often rely on the results as following.</p>

<div class="theorem">
<p><span id="thm:ConsExtreEst" class="theorem"><strong>Theorem 3.3  (Consistency of Extremum Estimators)  </strong></span>If there is a function <span class="math inline">\({Q}_0 (\boldsymbol{\theta})\)</span> such that:</p>
<p>C1. <span class="math inline">\({Q}_0 (\boldsymbol{\theta})\)</span> is uniquely maximized in <span class="math inline">\(\boldsymbol{\theta}_0\)</span>,</p>
<p>C2. <span class="math inline">\(\boldsymbol{\Theta}\)</span> is compact,</p>
<p>C3. <span class="math inline">\({Q}_0 (\boldsymbol{\theta})\)</span> is continuous in <span class="math inline">\(\boldsymbol{\theta}\)</span>,</p>
<p>C4. <span class="math inline">\(\hat{Q}_n (\boldsymbol{\theta})\)</span> converges uniformly in probability to <span class="math inline">\(Q_0 (\boldsymbol{\theta})\)</span>,</p>
then we have <span class="math inline">\(\hat{\boldsymbol{\theta}} \overset{\mathcal{P}}{\mapsto} \boldsymbol{\theta}_0\)</span>.
</div>
<p></p>

<div class="definition">
<span id="def:compact" class="definition"><strong>Definition 3.4  (Compactness)  </strong></span>We say <span class="math inline">\(\boldsymbol{\Theta}\)</span> is compact if every open cover of <span class="math inline">\(\boldsymbol{\Theta}\)</span> contains a finite subcover. If <span class="math inline">\(\boldsymbol{\Theta}\)</span> is compact, then <span class="math inline">\(\boldsymbol{\Theta}\)</span> is closed (i.e.Â containing all its limit points) and bounded (i.e.Â all its points are within some finite distance of each other).
</div>
<p></p>

<div class="definition">
<span id="def:UnifCont" class="definition"><strong>Definition 3.5  (Uniform Convergence in Probability)  </strong></span><span class="math inline">\(\hat{Q}_n (\boldsymbol{\theta})\)</span> is said to converges uniformly in probability to <span class="math inline">\(Q_0 (\boldsymbol{\theta})\)</span> if <span class="math inline">\(\underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{sup}} \; |\hat{Q}_n (\boldsymbol{\theta}) - {Q}_0 (\boldsymbol{\theta})| \overset{\mathcal{P}}{\mapsto} 0\)</span>.
</div>
<p></p>
<p>Theorem <a href="properties-of-statistical-estimators.html#thm:ConsExtreEst">3.3</a> is an important result as it provides a general approach to prove the consistency of the class of extremum estimators. Notice that in this theorem:</p>
<ul>
<li>Condition (C1) is <strong>substantive</strong> and there are well-known examples where it fails. We will discuss further on how this assumption can (in some cases) be verified in practice.</li>
<li>Condition (C2) is also <strong>substantive</strong> as it requires that there exist some known bounds on the parameters. In practice, this assumption is often neglected although it is in most cases unrealistic to assume it.</li>
<li>Condition (C3) and (C4) are often referred to as <strong>âstandard regularity conditionsâ</strong>. They are typically satisfied. The verification of these conditions will be discussed further later in this section.</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="timeseries.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="allan-variance-calibration-techniques.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/02-estimators.Rmd",
"text": "Edit"
},
"download": ["scis.pdf", "scis.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
