<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>An Introduction to Inertial Sensors Stochastic Calibration</title>
  <meta name="description" content="TO DO">
  <meta name="generator" content="bookdown 0.7.14 and GitBook 2.6.7">

  <meta property="og:title" content="An Introduction to Inertial Sensors Stochastic Calibration" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="TO DO" />
  <meta name="github-repo" content="SMAC-Group/SCIS" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="An Introduction to Inertial Sensors Stochastic Calibration" />
  
  <meta name="twitter:description" content="TO DO" />
  

<meta name="author" content="StÃ©phane Guerrier, Roberto Molinari, Yuming Zhang, Haotian Xu, Gaetan Bakalli, Ahmed Radi and Mucyo Karemera">


<meta name="date" content="2018-07-18">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="timeseries.html">
<link rel="next" href="allan-variance-calibration-techniques.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="timeseries.html"><a href="timeseries.html"><i class="fa fa-check"></i><b>2</b> Introduction to Time Series Analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="timeseries.html"><a href="timeseries.html#time-series"><i class="fa fa-check"></i><b>2.1</b> Time Series</a></li>
<li class="chapter" data-level="2.2" data-path="timeseries.html"><a href="timeseries.html#dependence-within-time-series"><i class="fa fa-check"></i><b>2.2</b> Dependence within Time Series</a></li>
<li class="chapter" data-level="2.3" data-path="timeseries.html"><a href="timeseries.html#stationarity"><i class="fa fa-check"></i><b>2.3</b> Stationarity</a></li>
<li class="chapter" data-level="2.4" data-path="timeseries.html"><a href="timeseries.html#linear-processes"><i class="fa fa-check"></i><b>2.4</b> Linear Processes</a></li>
<li class="chapter" data-level="2.5" data-path="timeseries.html"><a href="timeseries.html#basic-time-series-models"><i class="fa fa-check"></i><b>2.5</b> Basic Time Series Models</a></li>
<li class="chapter" data-level="2.6" data-path="timeseries.html"><a href="timeseries.html#fundamental-representations-of-time-series"><i class="fa fa-check"></i><b>2.6</b> Fundamental Representations of Time Series</a></li>
<li class="chapter" data-level="2.7" data-path="timeseries.html"><a href="timeseries.html#estimation-problems-with-time-series"><i class="fa fa-check"></i><b>2.7</b> Estimation Problems with Time Series</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html"><i class="fa fa-check"></i><b>3</b> Properties of Statistical Estimators</a><ul>
<li class="chapter" data-level="3.1" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#extremum-estimators"><i class="fa fa-check"></i><b>3.1</b> Extremum Estimators</a></li>
<li class="chapter" data-level="3.2" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#consistency"><i class="fa fa-check"></i><b>3.2</b> Consistency</a><ul>
<li class="chapter" data-level="3.2.1" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#consistency-of-extremum-estimators"><i class="fa fa-check"></i><b>3.2.1</b> Consistency of Extremum Estimators</a></li>
<li class="chapter" data-level="3.2.2" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#verification-of-condition-c1"><i class="fa fa-check"></i><b>3.2.2</b> Verification of Condition C1</a></li>
<li class="chapter" data-level="3.2.3" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#verification-of-condition-c4"><i class="fa fa-check"></i><b>3.2.3</b> Verification of Condition C4</a></li>
<li class="chapter" data-level="3.2.4" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#consistency-of-sample-autocovariance-and-autocorrelation-functions"><i class="fa fa-check"></i><b>3.2.4</b> Consistency of Sample AutoCovariance and AutoCorrelation Functions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#asymptotic-normality"><i class="fa fa-check"></i><b>3.3</b> Asymptotic Normality</a><ul>
<li class="chapter" data-level="3.3.1" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#clt-for-iid-random-variables"><i class="fa fa-check"></i><b>3.3.1</b> CLT for iid Random Variables</a></li>
<li class="chapter" data-level="3.3.2" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#clt-for-dependent-processes"><i class="fa fa-check"></i><b>3.3.2</b> CLT for Dependent Processes</a></li>
<li class="chapter" data-level="3.3.3" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#asymptotic-normality-of-sample-autocovariance-and-autocorrelation-functions"><i class="fa fa-check"></i><b>3.3.3</b> Asymptotic Normality of Sample AutoCovariance and AutoCorrelation Functions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html"><i class="fa fa-check"></i><b>4</b> Allan Variance Calibration Techniques</a><ul>
<li class="chapter" data-level="4.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#spectral-ambiguity-of-the-av"><i class="fa fa-check"></i><b>4.1</b> Spectral Ambiguity of the AV</a></li>
<li class="chapter" data-level="4.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#properties-of-the-allan-variance"><i class="fa fa-check"></i><b>4.2</b> Properties of the Allan Variance</a></li>
<li class="chapter" data-level="4.3" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#estimation"><i class="fa fa-check"></i><b>4.3</b> Estimation</a><ul>
<li class="chapter" data-level="4.3.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#consistency-1"><i class="fa fa-check"></i><b>4.3.1</b> Consistency</a></li>
<li class="chapter" data-level="4.3.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#asymptotic-normality-1"><i class="fa fa-check"></i><b>4.3.2</b> Asymptotic Normality</a></li>
<li class="chapter" data-level="4.3.3" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#confidence-interval-of-the-moav-estimator"><i class="fa fa-check"></i><b>4.3.3</b> Confidence Interval of the MOAV Estimator</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#allan-variance-based-estimation"><i class="fa fa-check"></i><b>4.4</b> Allan variance based estimation</a><ul>
<li class="chapter" data-level="4.4.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#allan-variance-log-log-representation"><i class="fa fa-check"></i><b>4.4.1</b> Allan Variance log-log Representation</a></li>
<li class="chapter" data-level="4.4.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#allan-deviation-of-a-wn-process"><i class="fa fa-check"></i><b>4.4.2</b> Allan Deviation of a WN process</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-generalized-method-of-wavelet-moments.html"><a href="the-generalized-method-of-wavelet-moments.html"><i class="fa fa-check"></i><b>5</b> The Generalized Method of Wavelet Moments</a></li>
<li class="chapter" data-level="6" data-path="extensions.html"><a href="extensions.html"><i class="fa fa-check"></i><b>6</b> Extensions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Inertial Sensors Stochastic Calibration</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="properties-of-statistical-estimators" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Properties of Statistical Estimators</h1>
<p>In this chapter, we will provide a review of the properties of statistical estimators. This chapter is organized with the following outline:</p>
<ul>
<li>Extremum estimators;</li>
<li>Consistency;</li>
<li>Asymptotic normality.</li>
</ul>
<div id="extremum-estimators" class="section level2">
<h2><span class="header-section-number">3.1</span> Extremum Estimators</h2>
<p>In this section, we will introduce a commonly used class of estimators, extremum estimators, and some examples of it.</p>

<div class="definition">
<span id="def:EstreEst" class="definition"><strong>Definition 3.1  (Extremum Estimators)  </strong></span>Many estimators have a common structure, which is often useful to study their asymptotic properties. One structure or framework is the class of estimators that maximize some objective function, referred to as extremum estimators, which can can be defined as follows:
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\theta}} \equiv \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; \hat{Q}_n(\boldsymbol{\theta})
\end{equation*}\]</span>
where <span class="math inline">\(\boldsymbol{\theta}\)</span> and <span class="math inline">\(\boldsymbol{\Theta}\)</span> denote, respectively, the parameter vector of interest and its set of possible values.
</div>

<p>The vast majority of statistical estimators can be represented as extremum estimators. For example, least squares, maximum likelihood or (generalized) method of moment estimators can all be represented as extremum estimators.</p>

<div class="example">
<span id="exm:ExtreEstLSE" class="example"><strong>Example 3.1  (Least Squares Estimator as Extremum Estimator)  </strong></span>Consider the linear model <span class="math inline">\(\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta}_0 + \boldsymbol{\epsilon}\)</span> where <span class="math inline">\(\boldsymbol{X} \in \mathbb{R}^{n \times p}\)</span> is a full rank constant matrix, <span class="math inline">\(\boldsymbol{\beta} \in {\mathcal{B}} \subseteq \mathbb{R}^p\)</span> and <span class="math inline">\(\epsilon_i \overset{iid}{\sim} \mathcal{N}(0, \sigma_\epsilon^2)\)</span>. Let <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> denote the Least Squares Estimator (LSE) of <span class="math inline">\(\boldsymbol{\beta}_0\)</span>, i.e.
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\beta}} = \left(\boldsymbol{X}^T \boldsymbol{X} \right)^{-1} \boldsymbol{X}^T \boldsymbol{y}. 
\end{equation*}\]</span>
This LSE is an extremum estimator since it can be expressed as
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\beta}} = \underset{\boldsymbol{\beta} \in \mathcal{B}}{\text{argmax}} \; -||\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} ||_2^2.
\end{equation*}\]</span>
</div>


<div class="example">
<span id="exm:ExtreEstMLE" class="example"><strong>Example 3.2  (Maximum Likelihood Estimator as Extremum Estimator)  </strong></span>Let <span class="math inline">\(Z_1, \ldots, Z_n\)</span> be an iid sample with pdf <span class="math inline">\(f(z|\boldsymbol{\theta}_0)\)</span>. The Maximum Likelihood Estimator (MLE) is given by
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; \frac{1}{n} \sum_{i = 1}^{n} \text{log}\left[f\left(z_i| \boldsymbol{\theta}\right)\right].
\end{equation*}\]</span>
<p>Here instead of the actual log-likelihood, we are actually using a normalized log-likelihood, which has no impact on the estimator but the normalized form is more convenient to use when we let <span class="math inline">\(n \to \infty\)</span>.</p>
Therefore, the MLE can be seen as an extremum estimator with
<span class="math display">\[\begin{equation*}
\hat{Q}_n(\boldsymbol{\theta}) = \frac{1}{n} \sum_{i = 1}^{n} \text{log}\left[f\left(z_i| \boldsymbol{\theta}\right)\right].
\end{equation*}\]</span>
</div>


<div class="definition">
<span id="def:GMM" class="definition"><strong>Definition 3.2  (Generalized Method of Moments)  </strong></span>Let <span class="math inline">\(Z_1, \ldots, Z_n\)</span> be an iid sample with pdf <span class="math inline">\(f(z|\boldsymbol{\theta}_0)\)</span>. Suppose that there is a moment function vector <span class="math inline">\(\boldsymbol{g}(z | \boldsymbol{\theta})\)</span> such that <span class="math inline">\(\mathbb{E}[\boldsymbol{g} (z | \boldsymbol{\theta}_0)] = 0\)</span>. Then the Generalized Method of Moments (GMM) estimator of <span class="math inline">\(\boldsymbol{\theta}_0\)</span> is defined as
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; - \left[\frac{1}{n} \sum_{i = 1}^n \boldsymbol{g}(z_i | \boldsymbol{\theta}) \right]^T \widehat{\boldsymbol{W}} \left[\frac{1}{n} \sum_{i = 1}^n \boldsymbol{g}(z_i | \boldsymbol{\theta}) \right],
\end{equation*}\]</span>
<p>where <span class="math inline">\(\widehat{\boldsymbol{W}}\)</span> is an positive definite matrix of appropriate dimension.</p>
Alternatively (but equivalently), we can define GMM estimator as
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; - || \hat{\boldsymbol{\mu}} - \boldsymbol{\mu}(\boldsymbol{\theta}) ||_{\widehat{\boldsymbol{W}}}^2,
\end{equation*}\]</span>
where <span class="math inline">\(|| \boldsymbol{x} ||_{\boldsymbol{A}}^2 = \boldsymbol{x}^T \boldsymbol{A} \boldsymbol{x}\)</span>, and where <span class="math inline">\(\hat{\boldsymbol{\mu}}\)</span> and <span class="math inline">\(\boldsymbol{\mu}(\boldsymbol{\theta})\)</span> denote, respectively, the empirical and model based moments. We will use this form of definition of GMM estimator more often.
</div>


<div class="example">
<span id="exm:ExtreEstGMM" class="example"><strong>Example 3.3  (A Simple GMM Estimator as Extremum Estimator)  </strong></span>Let <span class="math inline">\(Z_i \overset{iid}{\sim} \mathcal{N}(\mu_0, \sigma^2_0)\)</span> and <span class="math inline">\(\boldsymbol{\theta}_0 = (\mu_0, \sigma_0^2)^T\)</span>. Suppose we wish to estimate <span class="math inline">\(\boldsymbol{\theta}_0\)</span> by matching the first three empirical moments with their theoretical counterparts. In this case, a reasonable moment function or condition defining a GMM estimator is given by:
<span class="math display">\[\begin{equation*}
\boldsymbol{g} (Z | \boldsymbol{\theta}) = \begin{bmatrix}
Z - \mu\\
Z^2 - \left(\mu^2 + \sigma^2\right)\\
Z^3 - \left(\mu^3 + 3 \mu \sigma^2\right)
\end{bmatrix}.
\end{equation*}\]</span>
Notice that <span class="math inline">\(\frac{1}{n} \sum_{i=1}^n \boldsymbol{g} (Z_i | \boldsymbol{\theta}) = \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta})\)</span>, where <span class="math inline">\(\hat{\boldsymbol{\gamma}}\)</span> and <span class="math inline">\(\boldsymbol{\gamma}(\boldsymbol{\theta})\)</span> denote, respectively, the empirical and model based moments, i.e.
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\gamma}} = \frac{1}{n}  \sum_{i = 1}^n 
\begin{bmatrix}
Z_i\\
Z_i^2\\
Z_i^3\\
\end{bmatrix}, \;\;\;\;
\boldsymbol{\gamma}(\boldsymbol{\theta}) =  \begin{bmatrix}
\mu\\
\mu^2 + \sigma^2\\
\mu^3 + 3 \mu \sigma^2
\end{bmatrix}.
\end{equation*}\]</span>
Therefore we can write the GMM estimator of <span class="math inline">\(\boldsymbol{\theta}_0\)</span> as
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmin}} \; || \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma} (\boldsymbol{\theta}) ||_{\widehat{\boldsymbol{W}}}^2 = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; - || \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma} (\boldsymbol{\theta}) ||_{\widehat{\boldsymbol{W}}}^2 
\end{equation*}\]</span>
</div>

</div>
<div id="consistency" class="section level2">
<h2><span class="header-section-number">3.2</span> Consistency</h2>
<p>In this section, we will introduce one of the most important properties of statistical estimators, consistency. And we will discuss in details about the conditions for consistency of the extremum estimators.</p>

<div class="definition">
<span id="def:consistency" class="definition"><strong>Definition 3.3  (Consistency)  </strong></span>An estimator <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> is said to be consistent or weakly consistent if it converges in probability to <span class="math inline">\(\boldsymbol{\theta}_0\)</span>, i.e.
<span class="math display">\[\begin{equation*}
\underset{n \to \infty}{\text{lim}} \mathbb{P} (||\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0||_2 \geq \epsilon) = 0,
\end{equation*}\]</span>
for all <span class="math inline">\(\epsilon &gt;0\)</span>.
</div>

<p>In Laymanâs term, consistency simply means that if <span class="math inline">\(n\)</span> is large enough, then <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> will be arbitrarily close to <span class="math inline">\(\boldsymbol{\theta}_0\)</span> (i.e.Â inside of an hypersphere of radius <span class="math inline">\(\epsilon\)</span> centered at <span class="math inline">\(\boldsymbol{\theta}_0\)</span>). This also means the procedure (i.e.Â our estimator <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>) based on unlimited data will be able to identify the underlying truth (i.e. <span class="math inline">\(\boldsymbol{\theta}_0\)</span>).</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-6"></span>
<img src="images/consistency.pdf" alt="Interpretation of consistency" width="80%" />
<p class="caption">
Figure 3.1: Interpretation of consistency
</p>
</div>
<p>To show the consistency of estimators, we often make use of the following two important results:</p>

<div class="theorem">
<span id="thm:WLLN" class="theorem"><strong>Theorem 3.1  (Weak Law of Large Number)  </strong></span>Suppose <span class="math inline">\(X_i\)</span> are iid random variables with finite mean <span class="math inline">\(\mu\)</span> (i.e. <span class="math inline">\(\mathbb{E}[X_i] = \mu\)</span>) and finite variance. Let <span class="math inline">\(\bar{X}_n = \frac{1}{n} \sum_{i = 1}^n X_i\)</span>, then <span class="math inline">\(\bar{X}_n \overset{\mathcal{P}}{\mapsto} \mu\)</span>.
</div>


<div class="theorem">
<span id="thm:CMT" class="theorem"><strong>Theorem 3.2  (Continuous Mapping Theorem)  </strong></span>If <span class="math inline">\(Y_n \overset{\mathcal{P}}{\mapsto} \mu\)</span> and <span class="math inline">\(g(\cdot)\)</span> is a continuous function, then <span class="math inline">\(g(Y_n) \overset{\mathcal{P}}{\mapsto} g(\mu)\)</span>.
</div>

<p>Here we present a simple example to prove the consistency of an estimator with the above two results.</p>

<div class="example">
<span id="exm:ConsistencyExpDist" class="example"><strong>Example 3.4  (Consistency in Exponential Distribution)  </strong></span>Suppose we have an iid sample from exponential distribution, i.e. <span class="math inline">\(X_i \overset{iid}{\sim} \text{exp}(\lambda_0),\; \lambda_0 \in \mathbb{R}^+, \; i = 1,..., n\)</span>. So assuming <span class="math inline">\(X \geq 0\)</span>, the density of <span class="math inline">\(X\)</span> is given by
<span class="math display">\[\begin{equation*}
f(x|\lambda) = \lambda \text{exp}\left( - \lambda x \right).
\end{equation*}\]</span>
<p>In this example, we want to show that the MLE for <span class="math inline">\(\lambda_0\)</span> is a consistent estimator of <span class="math inline">\(\lambda_0\)</span>.</p>
First, we want to find the MLE for <span class="math inline">\(\lambda_0\)</span>. The normalized log-likelihood function is given by
<span class="math display">\[\begin{equation*}
\mathcal{L}(\lambda | X_1, ..., X_n) = \text{log}(\lambda) - \lambda \bar{X}_n.
\end{equation*}\]</span>
By solving
<span class="math display">\[\begin{equation*}
\frac{\partial}{\partial \lambda} \; \mathcal{L}(\lambda | X_1, \ldots, X_n) = \frac{1}{\lambda} - \bar{X}_n = 0,
\end{equation*}\]</span>
we obtain <span class="math inline">\(\hat{\lambda} = \frac{1}{\bar{X}_n}\)</span>. We verify that
<span class="math display">\[\begin{equation*}
\frac{\partial^2}{\partial \lambda^2} \; \mathcal{L}(\lambda | X_1, \ldots, X_n) = -\frac{1}{\lambda^2} &lt; 0,
\end{equation*}\]</span>
<p>which implies that <span class="math inline">\(\hat{\lambda}\)</span> is the maxima of <span class="math inline">\(\mathcal{L}(\lambda | X_1, \ldots, X_n)\)</span>. Therefore, the MLE for <span class="math inline">\(\lambda_0\)</span> is <span class="math inline">\(\hat{\lambda} = \frac{1}{\bar{X}_n}\)</span>.</p>
By Weak Law of Large Number, we have <span class="math inline">\(\bar{X}_n \overset{\mathcal{P}}{\mapsto} \mu\)</span>, where is given by
<span class="math display">\[\begin{equation*}
\mu = \mathbb{E}[X_i] = \int_{0}^{\infty} x \lambda_0 \text{exp}\left( - \lambda_0 x \right) dx = \frac{1}{\lambda_0}.
\end{equation*}\]</span>
And also since the function <span class="math inline">\(g(x) = 1/x\)</span> is continuous in <span class="math inline">\(\mathbb{R}^+\)</span>, we obtain by the Continuous Mapping Theorem that <span class="math inline">\(\hat{\lambda} \overset{\mathcal{P}}{\mapsto} \lambda_0\)</span>, which concludes that the MLE for <span class="math inline">\(\lambda_0\)</span> is a consistent estimator of <span class="math inline">\(\lambda_0\)</span> in exponential distribution.
</div>

<div id="consistency-of-extremum-estimators" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Consistency of Extremum Estimators</h3>
<p>When considering real-life problems, the above approach based on Weak Law of Large Number and Continuous Mapping Theorem is in general not flexible enough. Therefore, we often rely on the results as following.</p>

<div class="theorem">
<p><span id="thm:ConsExtreEst" class="theorem"><strong>Theorem 3.3  (Consistency of Extremum Estimators)  </strong></span>If there is a function <span class="math inline">\({Q}_0 (\boldsymbol{\theta})\)</span> such that:</p>
<p>C1. <span class="math inline">\({Q}_0 (\boldsymbol{\theta})\)</span> is uniquely maximized in <span class="math inline">\(\boldsymbol{\theta}_0\)</span>,</p>
<p>C2. <span class="math inline">\(\boldsymbol{\Theta}\)</span> is compact,</p>
<p>C3. <span class="math inline">\({Q}_0 (\boldsymbol{\theta})\)</span> is continuous in <span class="math inline">\(\boldsymbol{\theta}\)</span>,</p>
<p>C4. <span class="math inline">\(\hat{Q}_n (\boldsymbol{\theta})\)</span> converges uniformly in probability to <span class="math inline">\(Q_0 (\boldsymbol{\theta})\)</span>,</p>
then we have <span class="math inline">\(\hat{\boldsymbol{\theta}} \overset{\mathcal{P}}{\mapsto} \boldsymbol{\theta}_0\)</span>.
</div>


<div class="definition">
<span id="def:compact" class="definition"><strong>Definition 3.4  (Compactness)  </strong></span>We say <span class="math inline">\(\boldsymbol{\Theta}\)</span> is compact if every open cover of <span class="math inline">\(\boldsymbol{\Theta}\)</span> contains a finite subcover. If <span class="math inline">\(\boldsymbol{\Theta}\)</span> is compact, then <span class="math inline">\(\boldsymbol{\Theta}\)</span> is closed (i.e.Â containing all its limit points) and bounded (i.e.Â all its points are within some finite distance of each other).
</div>


<div class="definition">
<span id="def:UnifCont" class="definition"><strong>Definition 3.5  (Uniform Convergence in Probability)  </strong></span><span class="math inline">\(\hat{Q}_n (\boldsymbol{\theta})\)</span> is said to converges uniformly in probability to <span class="math inline">\(Q_0 (\boldsymbol{\theta})\)</span> if <span class="math inline">\(\underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{sup}} \; |\hat{Q}_n (\boldsymbol{\theta}) - {Q}_0 (\boldsymbol{\theta})| \overset{\mathcal{P}}{\mapsto} 0\)</span>.
</div>

<p>Theorem <a href="properties-of-statistical-estimators.html#thm:ConsExtreEst">3.3</a> is an important result as it provides a general approach to prove the consistency of the class of extremum estimators. Notice that in this theorem:</p>
<ul>
<li>Condition (C1) is <strong>substantive</strong> and there are well-known examples where it fails. We will discuss further on how this assumption can (in some cases) be verified in practice.</li>
<li>Condition (C2) is also <strong>substantive</strong> as it requires that there exist some known bounds on the parameters. In practice, this assumption is often neglected although it is in most cases unrealistic to assume it.</li>
<li>Condition (C3) and (C4) are often referred to as <strong>âstandard regularity conditionsâ</strong>. They are typically satisfied. The verification of these conditions will be discussed further later in this section.</li>
</ul>

<div class="proof">
<p> <span class="proof"><em>Proof</em> (Consistency of Extremum Estimators). </span> Let <span class="math inline">\(\mathcal{G}\)</span> be an <span class="math inline">\(\epsilon\)</span>-neighborhood centered at <span class="math inline">\(\boldsymbol{\theta}_0\)</span> for some <span class="math inline">\(\epsilon &gt; 0\)</span>, i.e. <span class="math inline">\(\mathcal{G} = \{ \boldsymbol{\theta} \in \boldsymbol{\Theta}: || \boldsymbol{\theta} - \boldsymbol{\theta}_0 ||_2 &lt; \epsilon\}\)</span> for some <span class="math inline">\(\epsilon &gt; 0\)</span>.</p>
We want to show <span class="math inline">\(\hat{\boldsymbol{\theta}} \overset{\mathcal{P}}{\mapsto} \boldsymbol{\theta}_0\)</span>, which is equivalent to show
<span class="math display">\[\begin{equation*}
\underset{n \to \infty}{\text{lim}} \mathbb{P}(||\boldsymbol{\theta} - \boldsymbol{\theta}_0 ||_2 \geq \epsilon) = \underset{n \to \infty}{\text{lim}} \mathbb{P}(\hat{\boldsymbol{\theta}} \notin \mathcal{G}) = 0
\end{equation*}\]</span>
<p>Define <span class="math inline">\(\gamma = Q_0(\boldsymbol{\theta}_0) - \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta} \setminus \mathcal{G}}{\text{sup}} Q_0({\boldsymbol{\theta}}) &gt; 0 \;\;\)</span> by condition C1.</p>
<p>So <span class="math inline">\(\hat{\boldsymbol{\theta}} \notin \mathcal{G}\)</span> implies that <span class="math inline">\(Q_0(\hat{\boldsymbol{\theta}}) \leq \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta} \setminus \mathcal{G}}{\text{sup}} Q_0({\boldsymbol{\theta}}) = Q_0(\boldsymbol{\theta}_0) - \gamma\)</span>.</p>
<p>Let <span class="math inline">\(\mathcal{A} = \{ Q_0(\hat{\boldsymbol{\theta}}) \leq Q_0(\boldsymbol{\theta}_0) - \gamma \}\)</span>. So <span class="math inline">\(\underset{n \to \infty}{\text{lim}}\mathbb{P}(\hat{\boldsymbol{\theta}} \notin \mathcal{G}) \leq \underset{n \to \infty}{\text{lim}}\mathbb{P}(\mathcal{A})\)</span>.</p>
Now we define the following events:
<span class="math display">\[\begin{align*}
\mathcal{B} &amp;= \{|\hat{Q_n}(\hat{\boldsymbol{\theta}}) - Q_0(\hat{\boldsymbol{\theta}})| &gt; \gamma/3  \}, \\
\mathcal{C} &amp;= \{|\hat{Q_n}(\boldsymbol{\theta}_0) - Q_0(\boldsymbol{\theta}_0)| &gt; \gamma/3 \}, \\
\mathcal{D} &amp;= \{\underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{sup}} |\hat{Q_n}(\boldsymbol{\theta}) - Q_0(\boldsymbol{\theta})| &gt;\gamma/3 \}.
\end{align*}\]</span>
So we have
<span class="math display">\[\begin{align*}
\mathbb{P} (\mathcal{A}) &amp;\leq \mathbb{P} (\mathcal{A} \cup (\mathcal{B} \cup \mathcal{C})) + \mathbb{P}(\mathcal{B} \cup \mathcal{C}) \\
&amp;= \mathbb{P}(\mathcal{A} \cap \mathcal{B}^c \cap \mathcal{C}^c) + \mathbb{P}(\mathcal{B} \cup \mathcal{C}) \\
&amp;= \mathbb{P}(\emptyset) + \mathbb{P}(\mathcal{B} \cup \mathcal{C}) \\
&amp;= \mathbb{P}(\mathcal{B} \cup \mathcal{C}).
\end{align*}\]</span>
<p>Notice that <span class="math inline">\(\mathcal{A} \cap \mathcal{B}^c \cap \mathcal{C}^c = \emptyset\)</span> because if <span class="math inline">\(\mathcal{A}\)</span>, <span class="math inline">\(\mathcal{B}^c\)</span>, and <span class="math inline">\(\mathcal{C}^c\)</span> hold simultaneously, then</p>
<span class="math display">\[\begin{align*}
\hat{Q_n}(\hat{\boldsymbol{\theta}}) &amp;= \hat{Q_n}(\hat{\boldsymbol{\theta}}) - Q_0(\hat{\boldsymbol{\theta}}) + Q_0(\hat{\boldsymbol{\theta}}) \\
&amp;\leq |\hat{Q_n}(\hat{\boldsymbol{\theta}}) - Q_0(\hat{\boldsymbol{\theta}})| + Q_0(\hat{\boldsymbol{\theta}}) \\
&amp;\leq \gamma/3 + Q_0(\hat{\boldsymbol{\theta}}) \;\;\;\;\;\; \text{since} \;\; \mathcal{B}^c \;\; \text{holds}\\
&amp;\leq Q_0({\boldsymbol{\theta}}_0) - 2\gamma/3 \;\;\;\;\;\; \text{since} \;\; \mathcal{A} \;\; \text{holds}\\
&amp;\leq Q_0({\boldsymbol{\theta}}_0) - \hat{Q_n}(\hat{\boldsymbol{\theta}}_0) + \hat{Q_n}(\hat{\boldsymbol{\theta}}_0) - 2\gamma/3 \\
&amp;\leq |Q_0({\boldsymbol{\theta}}_0) - \hat{Q_n}({\boldsymbol{\theta}}_0)| + \hat{Q_n}({\boldsymbol{\theta}}_0) - 2\gamma/3 \\
&amp;\leq \hat{Q_n}({\boldsymbol{\theta}}_0)  - \gamma/3 \;\;\;\;\;\; \text{since} \;\; \mathcal{C}^c \;\; \text{holds}\\
&amp;&lt; \hat{Q_n}({\boldsymbol{\theta}}_0)
\end{align*}\]</span>
<p>which contradicts that <span class="math inline">\(\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \hat{Q_n}({\boldsymbol{\theta}})\)</span>.</p>
So
<span class="math display">\[\begin{align*}
\mathbb{P}(\mathcal{A}) &amp;= \mathbb{P}(\mathcal{B} \cup \mathcal{C}) \\
&amp;= \mathbb{P}(|\hat{Q_n}(\hat{\boldsymbol{\theta}}) - Q_0(\hat{\boldsymbol{\theta}})| &gt; \gamma/3 \;\; \text{or} \;\; |\hat{Q_n}(\boldsymbol{\theta}_0) - Q_0(\boldsymbol{\theta}_0)| &gt; \gamma/3) \\
&amp;= \mathbb{P} (\underset{\boldsymbol{\theta} \in  \{\hat{\boldsymbol{\theta}}, \boldsymbol{\theta}_0\} }{\text{sup}} |\hat{Q_n}(\boldsymbol{\theta}) - Q_0(\boldsymbol{\theta})| &gt; \gamma/3) \\
&amp; \leq \mathbb{P} (\underset{\boldsymbol{\theta} \in  \boldsymbol{\Theta} }{\text{sup}} |\hat{Q_n}(\boldsymbol{\theta}) - Q_0(\boldsymbol{\theta})| &gt; \gamma/3) \\
&amp;= \mathbb{P}(\mathcal{D}).
\end{align*}\]</span>
So by condition C4, we have
<span class="math display">\[\begin{equation*}
\underset{n \to \infty}{\text{lim}}\mathbb{P}(\hat{\boldsymbol{\theta}} \notin \mathcal{G}) \leq \underset{n \to \infty}{\text{lim}} \mathbb{P}(\mathcal{A}) \leq \underset{n \to \infty}{\text{lim}}\mathbb{P}(\mathcal{D}) = 0.
\end{equation*}\]</span>
Therefore, <span class="math inline">\(\underset{n \to \infty}{\text{lim}}\mathbb{P}(\hat{\boldsymbol{\theta}} \notin \mathcal{G}) = 0\)</span>.
</div>

</div>
<div id="verification-of-condition-c1" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Verification of Condition C1</h3>
<p>In general, the verification of Condition C1 is difficult and is often assumed in the statistical literature. Here we present two results based on <span class="citation">W. K. Newey and McFadden (<a href="#ref-newey1994vlarge">1994</a>)</span> and <span class="citation">Komunjer (<a href="#ref-komunjer2012global">2012</a>)</span> respectively, which allow us to verify the condition C1 for GMM-type estimators. A discussion on the verification of this condition for other estimators can for example be found in Chapter 7 of <span class="citation">Baltagi (<a href="#ref-baltagi2008companion">2008</a>)</span>.</p>

<div class="lemma">
<span id="lem:GMMindentification" class="lemma"><strong>Lemma 3.1  (GMM Identification)  </strong></span>If <span class="math inline">\(\boldsymbol{W} &gt; 0\)</span> (i.e.Â positive definite), where <span class="math inline">\(\boldsymbol{W}\)</span> is such that <span class="math inline">\(\widehat{\boldsymbol{W}} \overset{\mathcal{P}}{\mapsto} \boldsymbol{W}\)</span>, and for <span class="math inline">\(\boldsymbol{g}_0(\boldsymbol{\theta}) = \mathbb{E}[\boldsymbol{g}(z|\boldsymbol{\theta})]\)</span>, we have <span class="math inline">\(\boldsymbol{g}_0(\boldsymbol{\theta}_0)=0\)</span> and <span class="math inline">\(\boldsymbol{g}_0(\boldsymbol{\theta}) \neq 0\)</span> if <span class="math inline">\(\boldsymbol{\theta} \neq \boldsymbol{\theta}_0\)</span>, then <span class="math inline">\(Q_0(\boldsymbol{\theta}) = -\boldsymbol{g}_0(\boldsymbol{\theta})^T \boldsymbol{W} \boldsymbol{g}_0(\boldsymbol{\theta})\)</span> has a unique maximum at <span class="math inline">\(\boldsymbol{\theta}_0\)</span>.
</div>


<div class="proof">
 <span class="proof"><em>Proof</em> (GMM Identification). </span> Since <span class="math inline">\(\boldsymbol{W} &gt;0\)</span> and there exists a unique <span class="math inline">\(\boldsymbol{\theta}_0 \in \boldsymbol{\Theta}\)</span> such that <span class="math inline">\(\boldsymbol{g}_0(\boldsymbol{\theta}) = 0\)</span>, we can say that for all <span class="math inline">\(\boldsymbol{\theta} \in \boldsymbol{\Theta}\)</span>,
<span class="math display">\[\begin{equation*}
Q_0(\boldsymbol{\theta}) = -\boldsymbol{g}_0(\boldsymbol{\theta})^T \boldsymbol{W} \boldsymbol{g}_0(\boldsymbol{\theta}) \leq -\boldsymbol{g}_0(\boldsymbol{\theta}_0) \boldsymbol{W} \boldsymbol{g}_0(\boldsymbol{\theta}_0) = Q_0(\boldsymbol{\theta_0}).
\end{equation*}\]</span>
</div>

<p>Notice that if we write a GMM estimator as in Example <a href="properties-of-statistical-estimators.html#exm:ExtreEstGMM">3.3</a>, then the condition <span class="math inline">\(\boldsymbol{g}_0(\boldsymbol{\theta}) = 0\)</span> if and only if <span class="math inline">\(\boldsymbol{\theta} = \boldsymbol{\theta}_0\)</span> can be replaced by <span class="math inline">\(\boldsymbol{\gamma}(\boldsymbol{\theta}) = \boldsymbol{\gamma}(\boldsymbol{\theta}_0)\)</span> if and only if <span class="math inline">\(\boldsymbol{\theta} = \boldsymbol{\theta}_0\)</span>.</p>
<p>In order to verify the condition in Lemma <a href="properties-of-statistical-estimators.html#lem:GMMindentification">3.1</a> that <span class="math inline">\(\boldsymbol{g}_0(\boldsymbol{\theta}) = 0\)</span> if and only if <span class="math inline">\(\boldsymbol{\theta} = \boldsymbol{\theta}_0\)</span> (or alternatively <span class="math inline">\(\boldsymbol{\gamma}(\boldsymbol{\theta}) = \boldsymbol{\gamma}(\boldsymbol{\theta}_0)\)</span> if and only if <span class="math inline">\(\boldsymbol{\theta} = \boldsymbol{\theta}_0\)</span>), the following theorem based on <span class="citation">Komunjer (<a href="#ref-komunjer2012global">2012</a>)</span> provides us a way.</p>

<div class="theorem">
<p><span id="thm:Homomorphism" class="theorem"><strong>Theorem 3.4  (Homomorphism)  </strong></span>Let <span class="math inline">\(\boldsymbol{\theta} \in \boldsymbol{\Theta} \subset \mathbb{R}^p\)</span>. Let <span class="math inline">\(\boldsymbol{g}^*(\boldsymbol{\theta})\)</span> denote a subset of <span class="math inline">\(p\)</span> elements of <span class="math inline">\(\boldsymbol{g}_0 (\boldsymbol{\theta}) \in \mathbb{R}^q, \; q \geq p\)</span> such that:</p>
<ul>
<li><span class="math inline">\(\boldsymbol{g}^*(\boldsymbol{\theta})\)</span> is in <span class="math inline">\(\mathcal{C}^2\)</span> (i.e. <span class="math inline">\(\boldsymbol{g}^*(\boldsymbol{\theta})\)</span> can be differentiated twice);</li>
<li>For every <span class="math inline">\(\boldsymbol{\theta} \in \boldsymbol{\Theta}\)</span>, <span class="math inline">\(J(\boldsymbol{\theta})\)</span> is non-negative (or alternatively non-positive), where <span class="math inline">\(J(\boldsymbol{\theta}) \equiv \text{det} (\frac{\partial}{\partial \boldsymbol{\theta}^T}\, \boldsymbol{g}^*(\boldsymbol{\theta}) )\)</span>;</li>
<li><span class="math inline">\(||\boldsymbol{g}^*(\boldsymbol{\theta})|| \to \infty\)</span> whenever <span class="math inline">\(|| \boldsymbol{\theta} || \to \infty\)</span>;</li>
<li>For every <span class="math inline">\(s \in \mathbb{R}^p\)</span> the equation <span class="math inline">\(\boldsymbol{g}^*(\boldsymbol{\theta}) = s\)</span> has countably many (possibly zero) solutions in <span class="math inline">\(\boldsymbol{\Theta}\)</span>;</li>
</ul>
then <span class="math inline">\(\boldsymbol{g}^*(\boldsymbol{\theta})\)</span> is a homeomorphism (i.e. <span class="math inline">\(\boldsymbol{g}^*(\boldsymbol{\theta})\)</span> is continuous and one-to-one).
</div>

<ul>
<li><p>A direct consequence of Lemma <a href="properties-of-statistical-estimators.html#lem:GMMindentification">3.1</a> and Theorem <a href="properties-of-statistical-estimators.html#thm:Homomorphism">3.4</a> is that any GMM estimator with <span class="math inline">\(\boldsymbol{W} &gt; 0\)</span> satisfying the conditions of Theorem <a href="properties-of-statistical-estimators.html#thm:Homomorphism">3.4</a> verifies Condition C1 of Theorem <a href="properties-of-statistical-estimators.html#thm:ConsExtreEst">3.3</a>.</p></li>
<li><p>If one can show that <span class="math inline">\(\boldsymbol{g}^c(\boldsymbol{\theta})\)</span> is in <span class="math inline">\(\mathcal{C}\)</span> (where <span class="math inline">\(\boldsymbol{g}^c(\boldsymbol{\theta})\)</span> denotes the element of <span class="math inline">\(\boldsymbol{g}_0(\boldsymbol{\theta})\)</span> that are not in <span class="math inline">\(\boldsymbol{g}^*(\boldsymbol{\theta})\)</span>) then Condition C3 of Theorem <a href="properties-of-statistical-estimators.html#thm:ConsExtreEst">3.3</a> is also verified.</p></li>
<li><p>When considering a GMM estimator of the form used in Example <a href="properties-of-statistical-estimators.html#exm:ExtreEstGMM">3.3</a>, one can simply verify the conditions of Theorem <a href="properties-of-statistical-estimators.html#thm:ConsExtreEst">3.3</a> with <span class="math inline">\(\boldsymbol{g}(\boldsymbol{\theta}) = \boldsymbol{\gamma} (\boldsymbol{\theta})\)</span>.</p></li>
<li><p>For Theorem <a href="properties-of-statistical-estimators.html#thm:Homomorphism">3.4</a>, in <span class="citation">Komunjer (<a href="#ref-komunjer2012global">2012</a>)</span> it is actually assumed that <span class="math inline">\(\boldsymbol{\theta} \in \mathbb{R}^p\)</span> while we assume that <span class="math inline">\(\boldsymbol{\theta} \in \boldsymbol{\Theta}\)</span>. We use this simplification to avoid an overly technical treatment of this topic. In fact, we assume here that there exists a one-to-one function <span class="math inline">\(h(\cdot)\)</span> such that <span class="math inline">\(h \, : \,\mathbb{R}^p \mapsto \boldsymbol{\Theta}\)</span>. This condition is typically verified in practice.</p></li>
</ul>

<div class="example">
<p><span id="exm:exmC1" class="example"><strong>Example 3.5  (An Example to Verify Condition C1)  </strong></span>In this example we revisit Example <a href="properties-of-statistical-estimators.html#exm:ExtreEstGMM">3.3</a>. Let us say that <span class="math inline">\(\widehat{\boldsymbol{W}}\)</span> is such that <span class="math inline">\(\widehat{\boldsymbol{W}} \overset{\mathcal{P}}{\mapsto} \boldsymbol{W} &gt; 0\)</span>.</p>
We have shown that
<span class="math display">\[\begin{equation*}
        \boldsymbol{\gamma}(\boldsymbol{\theta}) =  \begin{bmatrix}
             \mu\\
            \mu^2 + \sigma^2\\
            \mu^3 + 3 \mu \sigma^2
             \end{bmatrix}.
\end{equation*}\]</span>
So we define that
<span class="math display">\[\begin{equation*}
    \boldsymbol{g}^*(\boldsymbol{\theta}) = \begin{bmatrix}
             \mu\\
            \mu^2 + \sigma^2\\
             \end{bmatrix}
             \;\;\;\;\; \text{and} \;\;\;\;\; g^c(\boldsymbol{\theta}) = \begin{bmatrix}
             \mu^3 + 3 \mu \sigma^2\\
             \end{bmatrix}.
\end{equation*}\]</span>
<p>Since the elements of <span class="math inline">\(\boldsymbol{g}^*(\boldsymbol{\theta})\)</span> are polynomial in <span class="math inline">\(\boldsymbol{\Theta}\)</span>, the condition <span class="math inline">\(\boldsymbol{g}^*(\boldsymbol{\theta}) \in \mathcal{C}^2\)</span> is trivially satisfied.</p>
Next, since
<span class="math display">\[\begin{equation*}
        \frac{\partial}{\partial \boldsymbol{\theta}^T}\, \boldsymbol{g}^*(\boldsymbol{\theta}) =   \begin{bmatrix}
             1 &amp; 0\\
            2\mu &amp; 1
             \end{bmatrix},
\end{equation*}\]</span>
<p>we have <span class="math inline">\(J(\boldsymbol{\theta}) \equiv \text{det} (\frac{\partial}{\partial \boldsymbol{\theta}^T}\, \boldsymbol{g}^*(\boldsymbol{\theta}) ) = 1\)</span>.</p>
<p>Finally, the last two conditions of Theorem <a href="properties-of-statistical-estimators.html#thm:Homomorphism">3.4</a> are trivially satisfied since <span class="math inline">\(||\boldsymbol{g}^*(\boldsymbol{\theta})||\)</span> can only diverge if <span class="math inline">\(|| \boldsymbol{\theta} ||\)</span> diverges and since <span class="math inline">\(\boldsymbol{g}^*(\boldsymbol{\theta}) = s\)</span> has (one) countably solutions in <span class="math inline">\(\boldsymbol{\Theta}\)</span>.</p>
Therefore, it follows from Lemma <a href="properties-of-statistical-estimators.html#lem:GMMindentification">3.1</a> and Theorem <a href="properties-of-statistical-estimators.html#thm:Homomorphism">3.4</a> that the function <span class="math inline">\(Q_0(\boldsymbol{\theta}) = - || \boldsymbol{\gamma}(\boldsymbol{\theta}_0) - \boldsymbol{\gamma}(\boldsymbol{\theta}) ||^2_{\boldsymbol{W}}\)</span> is uniquely maximized in <span class="math inline">\(\boldsymbol{\theta}_0\)</span>.
</div>

</div>
<div id="verification-of-condition-c4" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Verification of Condition C4</h3>
<p>In general, the verification of Condition C4 requires pointwise convergence and Lipschitz continuity as specified in the following theorem, which is a slightly adapted version of the Arzela-Ascoli Theorem.</p>

<div class="theorem">
<p><span id="thm:ArzelaAscoli" class="theorem"><strong>Theorem 3.5  (modified Arzela-Ascoli Theorem)  </strong></span>Suppose <span class="math inline">\(\boldsymbol{\Theta}\)</span> is compact. If</p>
<ul>
<li><p>for every <span class="math inline">\(\boldsymbol{\theta} \in \boldsymbol{\Theta}\)</span>, we have <span class="math inline">\(\hat{Q}_n (\boldsymbol{\theta}) \overset{\mathcal{P}}{\mapsto} Q_0 (\boldsymbol{\theta})\)</span>,</p></li>
<li><p><span class="math inline">\(\hat{Q}_n (\boldsymbol{\theta})\)</span> is almost surely Lipschitz continuous, i.e. <span class="math inline">\(\underset{\boldsymbol{\theta}_1, \boldsymbol{\theta}_2 \in \boldsymbol{\Theta}}{\text{sup}} |\hat{Q}_n (\boldsymbol{\theta}_1) - \hat{Q}_n (\boldsymbol{\theta}_2)| \leq H ||\boldsymbol{\theta}_1 - \boldsymbol{\theta}_2||\)</span> where <span class="math inline">\(H\)</span> is bounded almost surely,</p></li>
</ul>
then <span class="math inline">\(\hat{Q}_n (\boldsymbol{\theta})\)</span> converges uniformly in probability to <span class="math inline">\(Q_0 (\boldsymbol{\theta})\)</span>.
</div>

<p>Here we will not discuss how to prove that <span class="math inline">\(\hat{Q}_n (\boldsymbol{\theta})\)</span> is almost surely Lipschitz continuous and assume it for simplicity (more discussion on this topic can for example be found in <span class="citation">W. K. Newey and McFadden (<a href="#ref-newey1994vlarge">1994</a>)</span>). Nevertheless, it worth mentioning that this condition is almost always satisfied in practice and is therefore reasonable to assume for simplicity.</p>

<div class="example">
<span id="exm:exmC4" class="example"><strong>Example 3.6  (An Example to Verify Condition C4)  </strong></span>In this example we revisit Example <a href="properties-of-statistical-estimators.html#exm:ExtreEstGMM">3.3</a>. Since we have <span class="math inline">\(Z_i \overset{iid}{\sim} \mathcal{N}(\mu_0, \sigma^2_0)\)</span>, we let
<span class="math display">\[\begin{equation*}
    \boldsymbol{X}_i \equiv  \begin{bmatrix}
    Z_i\\
    Z_i^2\\
    Z_i^3
    \end{bmatrix}.
\end{equation*}\]</span>
So by the Weak Law of Large Number (Theorem <a href="properties-of-statistical-estimators.html#thm:WLLN">3.1</a>), we have
<span class="math display">\[\begin{equation*}
    \hat{\boldsymbol{\gamma}} = \frac{1}{n} \sum_{i = 1}^n \boldsymbol{X}_i  \overset{\mathcal{P}}{\mapsto} \mathbb{E}[\boldsymbol{X}_i] = \boldsymbol{\gamma}({\boldsymbol{\theta}_0}) \equiv \boldsymbol{\gamma}_0.
\end{equation*}\]</span>
Define
<span class="math display">\[\begin{equation*}
\hat{Q}_n (\boldsymbol{\theta}) = -||\hat{\boldsymbol{\gamma}}- \boldsymbol{\gamma} (\boldsymbol{\theta}) ||_{\widehat{\boldsymbol{W}}}^2
\end{equation*}\]</span>
where <span class="math inline">\(\widehat{\boldsymbol{W}} \overset{\mathcal{P}}{\mapsto} \boldsymbol{W}\)</span>, and also define
<span class="math display">\[\begin{equation*}
Q_0(\boldsymbol{\theta}) = -||\boldsymbol{\gamma}(\boldsymbol{\theta}_0) - \boldsymbol{\gamma}(\boldsymbol{\theta})||_\boldsymbol{W}^2.
\end{equation*}\]</span>
<p>In order to show that <span class="math inline">\(\hat{Q}_n (\boldsymbol{\theta}) \overset{\mathcal{P}}{\mapsto} Q_0(\boldsymbol{\theta})\)</span> for all <span class="math inline">\(\boldsymbol{\theta} \in \boldsymbol{\Theta}\)</span>, we want to show that <span class="math inline">\(|\hat{Q}_n (\boldsymbol{\theta}) - Q_0(\boldsymbol{\theta})| \overset{\mathcal{P}}{\mapsto} 0\)</span>.</p>
<span class="math display">\[\begin{equation*}
    \begin{aligned}
         \hat{Q}_n (\boldsymbol{\theta}) - {Q}_0 (\boldsymbol{\theta}) &amp;\leq |\hat{Q}_n (\boldsymbol{\theta}) - {Q}_0 (\boldsymbol{\theta})| =   \left|\underbrace{ ||\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma} (\boldsymbol{\theta}) ||_{\widehat{\boldsymbol{W}}}^2 - ||\gamma_0 - \boldsymbol{\gamma} (\boldsymbol{\theta}) ||_\boldsymbol{W}^2}_\boldsymbol{A}\right|.
    \end{aligned}
\end{equation*}\]</span>
<p>Without loss of generality, we assume that <span class="math inline">\(\boldsymbol{W}^* = \widehat{\boldsymbol{W}} - \boldsymbol{W}\)</span> and that <span class="math inline">\(\boldsymbol{W}\)</span> is symmetric.</p>
<span class="math display">\[\begin{align*}
\boldsymbol{A} &amp;=  \hat{\boldsymbol{\gamma}}^T \widehat{\boldsymbol{W}}\hat{\boldsymbol{\gamma}} + \boldsymbol{\gamma}(\boldsymbol{\theta})^T \widehat{\boldsymbol{W}} \boldsymbol{\gamma}(\boldsymbol{\theta}) - 2\hat{\boldsymbol{\gamma}}^T\widehat{\boldsymbol{W}}\boldsymbol{\gamma}(\boldsymbol{\theta}) - \boldsymbol{\gamma}_0^T \boldsymbol{W} \boldsymbol{\gamma}_0 - \boldsymbol{\gamma}(\boldsymbol{\theta})^T \boldsymbol{W} \boldsymbol{\gamma}(\boldsymbol{\theta}) + 2\boldsymbol{\gamma}_0^T \boldsymbol{W} \boldsymbol{\gamma}(\boldsymbol{\theta})  \\ 
&amp;= ||\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}_0||_{\widehat{\boldsymbol{W}}}^2 - \boldsymbol{\gamma}_0^T \widehat{\boldsymbol{W}} \boldsymbol{\gamma}_0 + 2\widehat{\boldsymbol{\gamma}}^T \widehat{\boldsymbol{W}} \boldsymbol{\gamma}_0 \\
&amp;+ \boldsymbol{\gamma}(\boldsymbol{\theta})^T \widehat{\boldsymbol{W}} \boldsymbol{\gamma}(\boldsymbol{\theta}) - 2\hat{\boldsymbol{\gamma}}^T\widehat{\boldsymbol{W}}\boldsymbol{\gamma}(\boldsymbol{\theta}) - \boldsymbol{\gamma}_0^T \boldsymbol{W} \boldsymbol{\gamma}_0 - \boldsymbol{\gamma}(\boldsymbol{\theta})^T \boldsymbol{W} \boldsymbol{\gamma}(\boldsymbol{\theta}) + 2\boldsymbol{\gamma}_0^T \boldsymbol{W} \boldsymbol{\gamma}(\boldsymbol{\theta})  \\ 
&amp;= ||\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}_0||_{\widehat{\boldsymbol{W}}}^2 + ||\boldsymbol{\gamma}_0 - \boldsymbol{\gamma}(\boldsymbol{\theta})||_{\boldsymbol{W}^*}^2 \\
&amp;- 2\boldsymbol{\gamma}_0^T\widehat{\boldsymbol{W}}\boldsymbol{\gamma}_0 - 2\hat{\boldsymbol{\gamma}}^T \widehat{\boldsymbol{W}} \boldsymbol{\gamma}(\boldsymbol{\theta}) + 2\boldsymbol{\gamma}_0^T \widehat{\boldsymbol{W}} \boldsymbol{\gamma}(\boldsymbol{\theta}) + 2\hat{\boldsymbol{\gamma}}^T \widehat{\boldsymbol{W}} \boldsymbol{\gamma}_0 \\
&amp;= ||\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}_0||_{\widehat{\boldsymbol{W}}}^2 + ||\boldsymbol{\gamma}_0 - \boldsymbol{\gamma}(\boldsymbol{\theta})||_{\boldsymbol{W}^*}^2 + 2(\boldsymbol{\gamma}_0 - \boldsymbol{\gamma}(\boldsymbol{\theta}))^T \widehat{\boldsymbol{W}} (\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}_0).
\end{align*}\]</span>
So by triangular inequality, we have
<span class="math display">\[\begin{align*}
|\hat{Q}_n (\boldsymbol{\theta}) - {Q}_0 (\boldsymbol{\theta})| &amp;\leq  \left| \underbrace{\|\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}_0)  \|_{\widehat{\boldsymbol{W}}}^2}_{\equiv a_1} \right| + \left| \underbrace{\|\boldsymbol{\gamma}(\boldsymbol{\theta}_0) - \boldsymbol{\gamma} (\boldsymbol{\theta}) \|_{{\boldsymbol{W}}^*}^2}_{\equiv a_2} \right| \\ 
&amp;+ \left| \underbrace{2 \left( \boldsymbol{\gamma}(\boldsymbol{\theta}_0) - \boldsymbol{\gamma} (\boldsymbol{\theta})\right)^T \widehat{\boldsymbol{W}} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}_0)\right)}_{\equiv a_3}\right|. 
\end{align*}\]</span>
<p>Before continuing, suppose that <span class="math inline">\(\boldsymbol{x}\)</span> is a vector and <span class="math inline">\(\boldsymbol{W}\)</span> the above defined matrix, we have that <span class="math inline">\(\boldsymbol{x}^T \boldsymbol{W} \boldsymbol{x} \leq \lambda_1 ||\boldsymbol{x}||^2\)</span>, where <span class="math inline">\(\lambda_1\)</span> is the largest eigenvalue of <span class="math inline">\(\boldsymbol{W}\)</span>. Furthermore, let us also define the Frobenius norm of a matrix as <span class="math inline">\(||\boldsymbol{W}|| = (\sum_i^N \sum_j^J w_{i,j}^2)^{\frac{1}{2}}\)</span> and <span class="math inline">\(||\boldsymbol{W}||^2 = \sigma_{\text{max}} \leq ||\boldsymbol{W}||\)</span>, where <span class="math inline">\(\sigma_{\text{max}}\)</span> is the largest singular value of <span class="math inline">\(||\boldsymbol{W}||\)</span>.</p>
<p>Using those properties, we can investigate the terms in the above equation.</p>
Considering <span class="math inline">\(a_1\)</span> , we have
<span class="math display">\[\begin{align*}
a_1 &amp;\leq ||\boldsymbol{\gamma}_0 - \boldsymbol{\gamma} (\boldsymbol{\theta}) ||^2 \lambda_1  \leq ||\boldsymbol{\gamma}_0 - \boldsymbol{\gamma} (\boldsymbol{\theta}) ||^2 \|\boldsymbol{W}^*|| \\ 
&amp;= \sum^3_{i = 1} \left(\boldsymbol{\gamma}_{0_i} - \boldsymbol{\gamma}_i (\boldsymbol{\theta})\right)^2  \sqrt{\sum^3_{i = 1} \sum^3_{j = 1} \left(\hat{w}_{i,j} - w_{i,j}\right)^2} \\
&amp;\leq 3 \underset{i}{\text{max}}  \left(\boldsymbol{\gamma}_{0_i} - \boldsymbol{\gamma}_i (\boldsymbol{\theta})\right)^2 \cdot 3 \underset{i}{\text{max}} \sqrt{\left(\hat{w}_{i,j} - w_{i,j}\right)^2}
\end{align*}\]</span>
<p>Since <span class="math inline">\(\underset{i}{\text{max}} \left(\boldsymbol{\gamma}_{0_i} - \boldsymbol{\gamma}_i (\boldsymbol{\theta})\right)^2\)</span> is bounded by conditions C1 to C3, and <span class="math inline">\(\underset{i}{\text{max}} \sqrt{\left(\hat{w}_{i,j} - w_{i,j}\right)^2} \overset{\mathcal{P}}{\mapsto} 0\)</span> by <span class="math inline">\(\widehat{\boldsymbol{W}} \overset{\mathcal{P}}{\mapsto} \boldsymbol{W}\)</span>, we show that <span class="math inline">\(a_1 \overset{\mathcal{P}}{\mapsto} 0\)</span>.</p>
<p>Similarly, we can also show that <span class="math inline">\(a_2 \overset{\mathcal{P}}{\mapsto} 0\)</span> using the fact that he sample moments converge to the population ones.</p>
<p>Finally, considering the previous results on <span class="math inline">\(a_1\)</span> and <span class="math inline">\(a_2\)</span>, we can see that the last term <span class="math inline">\(a_3\)</span> also tends to 0. Therefore, we can say that for all <span class="math inline">\(\boldsymbol{\theta} \in \boldsymbol{\Theta}\)</span>,</p>
<span class="math display">\[\begin{equation*}
|\hat{Q}_n (\boldsymbol{\theta}) - Q_0(\boldsymbol{\theta})| \overset{\mathcal{P}}{\mapsto} 0.
\end{equation*}\]</span>
</div>

<p>In general, showing that <span class="math inline">\(\hat{Q}_n (\boldsymbol{\theta}) \overset{\mathcal{P}}{\mapsto} Q_0(\boldsymbol{\theta})\)</span> for all <span class="math inline">\(\boldsymbol{\theta} \in \boldsymbol{\Theta}\)</span> for every <span class="math inline">\(\boldsymbol{\theta} \in \boldsymbol{\Theta}\)</span> is generally done using the Weak Law of Large Number (i.e.Â Theorem <a href="properties-of-statistical-estimators.html#thm:WLLN">3.1</a>). However, when <span class="math inline">\(X_i\)</span> which are not iid random variables, Theorem <a href="properties-of-statistical-estimators.html#thm:WLLN">3.1</a>) cannot be applied. The following theorem (taken from Proposition 7.5 of <span class="citation">Hamilton (<a href="#ref-hamilton1994time">1994</a>)</span>) generalizes this result for (weak) stationary processes with absolutely summable covariance structure.</p>

<div class="theorem">
<span id="thm:WLLNdep" class="theorem"><strong>Theorem 3.6  (Weak Law of Large Number for Dependent Process)  </strong></span>Suppose <span class="math inline">\((X_t)\)</span> is a (weak) stationary process with absolutely summable autocovariance structure, then
<span class="math display">\[\begin{equation*}
\bar{X}_T \overset{\mathcal{P}}{\mapsto} \mathbb{E}[X_t].
\end{equation*}\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof</em> (Weak Law of Large Number for Dependent Process). </span> Since <span class="math inline">\(\bar{X}_n - \mu\)</span> has mean zero, its variance is
<span class="math display">\[\begin{equation*}
\mathbb{E}[(\bar{X}_n - \mu)^2] = 1/n\sum_{h=-\infty}^{\infty} \gamma(h) \leq 1/n\sum_{h=-\infty}^{\infty} |\gamma(h)| \leq C/n.
\end{equation*}\]</span>
By Chebychevâs inequality, for all <span class="math inline">\(\epsilon &gt; 0\)</span>,
<span class="math display">\[\begin{equation*}
\mathbb{P} \left(| \bar{X}_n - \mu | \geq \epsilon \right) \leq \frac{\mathbb{E}[(\bar{X}_n - \mu)^2]}{\epsilon^2} \leq \frac{C}{\epsilon^2n},
\end{equation*}\]</span>
so that
<span class="math display">\[\begin{equation*}
\underset{n \to \infty}{\text{lim}} \mathbb{P} \left(| \bar{X}_n - \mu | \geq \epsilon \right) \to 0,
\end{equation*}\]</span>
which concludes the proof.
</div>


<div class="example">
<span id="exm:ConsistSampleMean" class="example"><strong>Example 3.7  (Consistency of Sample Mean)  </strong></span>Consider a stationary AR1 process and suppose we want to study whether its sample mean converges in probability to its expected value. This time we consider a non-zero mean AR1, i.e.
<span class="math display">\[\begin{equation*}
    \left(X_t - \mu\right) = \phi \left(X_{t-1} - \mu\right) + Z_t,
\end{equation*}\]</span>
where <span class="math inline">\(\mu = \mathbb{E}[X_t]\)</span>, <span class="math inline">\(Z_t \overset{iid}{\sim} \mathcal{N} (0, \nu^2)\)</span> and <span class="math inline">\(\nu^2 &lt; \infty\)</span>. This process can also be written as a linear process:
<span class="math display">\[\begin{equation*}
    X_i - \mu = \sum_{k=0}^{\infty}\phi^k Z_{i-k}.
\end{equation*}\]</span>
Since the process is stationary for <span class="math inline">\(|\phi| &lt; 1\)</span>, we have
<span class="math display">\[\begin{equation*}
\gamma_h = \frac{\nu^2 \phi^{|h|}}{1-\phi^2},
\end{equation*}\]</span>
<p>for <span class="math inline">\(h \in \mathbb{Z}\)</span>.</p>
Then we have
<span class="math display">\[\begin{equation*}
\sum_{h = -\infty}^{\infty} |\gamma_k| = \sum_{h = -\infty}^{\infty} \frac{\nu^2 |\phi|^{|h|}}{1-\phi^2} &lt; 2 \lim_{n\to\infty}\frac{\nu^2 (1-|\phi|^{n+1})}{(1-\phi^2)(1-|\phi|)} = \frac{2\nu^2 }{(1-\phi^2)(1-|\phi|)} &lt; \infty,
\end{equation*}\]</span>
implying that the process has an absolutely summable covariance structure. Therefore, by applying Theorem <a href="properties-of-statistical-estimators.html#thm:WLLNdep">3.6</a>, we can verify that
<span class="math display">\[\begin{equation*}
    \bar{X}_T = \frac{1}{T} \sum_{t = 1}^T \, X_t \overset{\mathcal{P}}{\mapsto} \mathbb{E}[X_t] = \mu.
\end{equation*}\]</span>
</div>

</div>
<div id="consistency-of-sample-autocovariance-and-autocorrelation-functions" class="section level3">
<h3><span class="header-section-number">3.2.4</span> Consistency of Sample AutoCovariance and AutoCorrelation Functions</h3>
<p>In Chapter 2, we have defined the sample autocovariance andautocorrelation functions. In the following, we will show that these estimators are both consistent.</p>

<div class="corollary">
<span id="cor:ConsistACVACF" class="corollary"><strong>Corollary 3.1  (Consistency of Sample AutoCovariance and AutoCorrelation Functions)  </strong></span>Let <span class="math inline">\((X_t)\)</span> be such that <span class="math inline">\((X_t)\)</span> is weakly stationary and <span class="math inline">\((X_t^2)\)</span> has an absolutely summable covariance structure, then for all <span class="math inline">\(|h| &lt; \infty\)</span>, we have
<span class="math display">\[\begin{align*}
\hat{\gamma}(h) &amp;\overset{\mathcal{P}}{\mapsto} \gamma(h),\\
\hat{\rho}(h) &amp;\overset{\mathcal{P}}{\mapsto} \rho(h).
\end{align*}\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof</em> (Consistency of Sample AutoCovariance and AutoCorrelation Functions). </span> <span class="math inline">\((X_t^2)\)</span> has an absolutely summable covariance structure implies that both <span class="math inline">\((X_t)\)</span> and <span class="math inline">\((X_tX_{t-h})\)</span> for all <span class="math inline">\(h \in \mathbb{Z}\)</span> have absolutely summable covariance structures. Under the conditions that <span class="math inline">\((X_t)\)</span> is weakly stationary and has absolutely summable covariance structure, we have <span class="math inline">\(\bar{X}_T \overset{\mathcal{P}}{\mapsto} \mu\)</span>. Let
<span class="math display">\[\begin{equation*}
\tilde{\gamma}(h) = \frac{1}{T} \sum_{t = h+1}^{T} \left(X_{t} - \mu\right) \left(X_{t-h} - \mu\right).
\end{equation*}\]</span>
Since <span class="math inline">\((\left(X_{t} - \mu\right) \left(X_{t-h} - \mu\right))\)</span> is stationary and has absolutely summable covariance structure, we have <span class="math inline">\(\tilde{\gamma}(h) \overset{\mathcal{P}}{\mapsto} \gamma(h)\)</span>. We can also show that <span class="math inline">\(\sqrt{T} \left(\tilde{\gamma}(h) - \hat{\gamma}(h) \right) = o_p(1)\)</span>. Therefore, we obtain
<span class="math display">\[\begin{equation*}
\hat{\gamma}(h) \overset{\mathcal{P}}{\mapsto} \gamma(h).
\end{equation*}\]</span>
Similarly, we can show that
<span class="math display">\[\begin{equation*}
\left( \hat{\gamma}(0), \hat{\gamma}(h) \right)^T \overset{\mathcal{P}}{\mapsto} \left( \gamma(0), \gamma(h) \right)^T.
\end{equation*}\]</span>
Then by Theorem <a href="properties-of-statistical-estimators.html#thm:CMT">3.2</a>, we have
<span class="math display">\[\begin{equation*}
\hat{\rho}(h) \overset{\mathcal{P}}{\mapsto} \rho(h),
\end{equation*}\]</span>
which concludes the proof.
</div>

</div>
</div>
<div id="asymptotic-normality" class="section level2">
<h2><span class="header-section-number">3.3</span> Asymptotic Normality</h2>
<p>The Central Limit Theorem (CLT) takes one step further than the Law of Large Number. It identifies the limiting distribution of the (properly scaled) sum of random variables as the normal distribution, which allows us to do the statistical inference (confidence interval and hypothesis testing). The scale will tell us how fast this approximation converges to the normal distribution.</p>
<div id="clt-for-iid-random-variables" class="section level3">
<h3><span class="header-section-number">3.3.1</span> CLT for iid Random Variables</h3>

<div class="theorem">
<span id="thm:CLT" class="theorem"><strong>Theorem 3.7  (CLT for iid sequences)  </strong></span>Suppose <span class="math inline">\(X_i\)</span> are iid random variables with <span class="math inline">\(\mathbb{E}[X_i] = \mu\)</span> and <span class="math inline">\(\text{Var}(X_i) = \sigma^2 &lt; \infty\)</span>. Let <span class="math inline">\(\bar{X}_n = \frac{1}{n} \sum_{i = 1}^n X_i\)</span>, then <span class="math inline">\(\sqrt{n}\left(\bar{X}_n - \mu\right) \overset{\mathcal{D}}{\mapsto} \mathcal{N}(0, \sigma^2)\)</span>.
</div>


<div class="theorem">
<span id="thm:CLTmulti" class="theorem"><strong>Theorem 3.8  (CLT for iid multivariate sequences)  </strong></span>Suppose <span class="math inline">\(\boldsymbol{X}_i\)</span> are iid random vectors with <span class="math inline">\(\mathbb{E}[\boldsymbol{X}_i] = \boldsymbol{\mu} \in \mathbb{R}^d\)</span> and <span class="math inline">\(\boldsymbol{\text{Cov}}(\boldsymbol{X}_i) = \boldsymbol{\Sigma} \in \mathbb{R}^{d \times d}\)</span>. Then, we have <span class="math inline">\(\sqrt{n}\left(\bar{\boldsymbol{X}}_n - \boldsymbol{\mu}\right) \overset{\mathcal{D}}{\mapsto} \mathcal{N}(\boldsymbol{0}, \boldsymbol{\Sigma})\)</span>.
</div>


<div class="theorem">
<p><span id="thm:slutsky" class="theorem"><strong>Theorem 3.9  (Slutskyâs Theorem)  </strong></span>Let <span class="math inline">\(X_n\)</span>, <span class="math inline">\(Y_n\)</span> and <span class="math inline">\(X\)</span> be random variables. If <span class="math inline">\(X_n \overset{\mathcal{D}}{\mapsto} X\)</span> and <span class="math inline">\(Y_n \overset{\mathcal{P}}{\mapsto} c\)</span> for some constant <span class="math inline">\(c\)</span>, then</p>
<ul>
<li><span class="math inline">\(X_n + Y_n \overset{\mathcal{D}}{\mapsto} X+c\)</span>,</li>
<li><span class="math inline">\(X_n Y_n \overset{\mathcal{D}}{\mapsto} cX\)</span>,</li>
<li><span class="math inline">\(X_n / Y_n \overset{\mathcal{D}}{\mapsto} X/c\)</span> if <span class="math inline">\(c \neq 0\)</span>.
</div>
</li>
</ul>
<p>General results on the asymptotic normality of extremum estimators can be found in <span class="citation">W. K. Newey and McFadden (<a href="#ref-newey1994vlarge">1994</a>)</span>. In this section, we will only restrict our attention to a simple example of GMM estimators as in Example <a href="properties-of-statistical-estimators.html#exm:ExtreEstGMM">3.3</a>. We will see that the asymptotic normality of extremum estimators is implied by combining the following results and techniques:</p>
<ul>
<li>Consistency of <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>,</li>
<li>Central Limit Theorem (see Theorem <a href="properties-of-statistical-estimators.html#thm:CLTmulti">3.8</a>),</li>
<li>Slutskyâs Theorem (see Theorem <a href="properties-of-statistical-estimators.html#thm:slutsky">3.9</a>),</li>
<li>Taylor expansions.</li>
</ul>

<div class="example">
<p><span id="exm:exmAsympNormality" class="example"><strong>Example 3.8  (An Example to Prove Asymptotic Normality of Extremum Estimators)  </strong></span>In this example we revisit Example <a href="properties-of-statistical-estimators.html#exm:ExtreEstGMM">3.3</a>. So we have</p>
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \hat{Q}_n(\boldsymbol{\theta}) = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} -||\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}) ||_\widehat{\boldsymbol{W}}^2
\end{equation*}\]</span>
where
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\gamma}} = \frac{1}{n}  \sum_{i = 1}^n 
\begin{bmatrix}
Z_i\\
Z_i^2\\
Z_i^3\\
\end{bmatrix}, \;\;\;\;
\boldsymbol{\gamma}(\boldsymbol{\theta}) =  \begin{bmatrix}
\mu\\
\mu^2 + \sigma^2\\
\mu^3 + 3 \mu \sigma^2
\end{bmatrix}.
\end{equation*}\]</span>
So by CLT (Theorem <a href="properties-of-statistical-estimators.html#thm:CLTmulti">3.8</a>), we have
<span class="math display">\[\begin{equation*}
\sqrt{n}\left(\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}_0)\right)  = \frac{1}{\sqrt{n}}  \sum_{i = 1}^n 
    \begin{bmatrix}
      Z_i - \mu_0\\
     Z_i^2 - \mu_0^2 - \sigma_0^2\\
     Z_i^3 - \mu_0^3 - 3 \mu_0 \sigma_0^2\\
     \end{bmatrix} \overset{\mathcal{D}}{\mapsto} \mathcal{N}(\boldsymbol{0}, \boldsymbol{\Sigma}),
\end{equation*}\]</span>
<p>where <span class="math inline">\(\boldsymbol{\Sigma} = \text{Cov}\left( \begin{bmatrix}  Z_i &amp; Z_i^2&amp; Z_i^3\\  \end{bmatrix}^T \right)\)</span>.</p>
Since <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> maximizes <span class="math inline">\(\hat{Q}_n(\boldsymbol{\theta})\)</span>, we have
<span class="math display">\[\begin{equation*}
\frac{\partial \hat{Q}_n(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \bigg\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}} = \frac{\partial}{\partial \boldsymbol{\theta}} \left(\left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}) \right) \right)^T \widehat{\boldsymbol{W}} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}) \right) \bigg\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}} = \boldsymbol{0}.
\end{equation*}\]</span>
Using Taylor expansion for <span class="math inline">\(\boldsymbol{\gamma}(\hat{\boldsymbol{\theta}})\)</span> around the true <span class="math inline">\(\boldsymbol{\theta}_0\)</span>, we obtain
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\hat{\boldsymbol{\theta}})  = \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}_0) + \frac{\partial \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}) \right)}{\partial \boldsymbol{\theta}}\bigg\rvert_{\boldsymbol{\theta} = \boldsymbol{\theta}_0} \left( \hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0 \right) + o_p(1).
\end{equation*}\]</span>
<p>Under certain regularity conditions, we have (using Theorem <a href="properties-of-statistical-estimators.html#thm:CMT">3.2</a>) that</p>
<span class="math display">\[\begin{equation*}
\frac{\partial}{\partial \boldsymbol{\theta}} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}) \right)\bigg\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}} = - \frac{\partial}{\partial \boldsymbol{\theta}}  \boldsymbol{\gamma}(\boldsymbol{\theta}) \bigg\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}}  \overset{\mathcal{P}}{\mapsto} - \frac{\partial}{\partial \boldsymbol{\theta}}  \boldsymbol{\gamma}(\boldsymbol{\theta}) \bigg\rvert_{\boldsymbol{\theta} = \boldsymbol{\theta}_0}.
\end{equation*}\]</span>
<p>So under certain regularity conditions, using the above equations and Slutskyâs Theorem, we can obtain that</p>
<span class="math display">\[\begin{equation*}
\sqrt{n}\left( \hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0 \right) \overset{\mathcal{D}}{\mapsto}  \mathcal{N}\left(\boldsymbol{0}, \boldsymbol{D}^T \boldsymbol{\Sigma} \boldsymbol{D}\right),
\end{equation*}\]</span>
<p>where</p>
<span class="math display">\[\begin{equation*}
\boldsymbol{D} = \left[ ||\frac{\partial}{\partial \boldsymbol{\theta}} \left(  \boldsymbol{\gamma}(\boldsymbol{\theta})\right)||_{\boldsymbol{W}}\rvert_{\boldsymbol{\theta} = \boldsymbol{\theta}_0} \right]^{-1} \left(\frac{\partial}{\partial \boldsymbol{\theta}}  \boldsymbol{\gamma}(\boldsymbol{\theta})\bigg\rvert_{\boldsymbol{\theta} = \boldsymbol{\theta}_0} \right)^T \boldsymbol{W}
\end{equation*}\]</span>
<p>due to the fact that</p>
<span class="math display">\[\begin{equation*}
\begin{aligned}
&amp;\sqrt{n}\left( \hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0 \right) 
        = \overbrace{-\left[ \left(\frac{\partial}{\partial \boldsymbol{\theta}} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}) \right)\bigg\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}} \right)^T \widehat{\boldsymbol{W}} \left( \frac{\partial}{\partial \boldsymbol{\theta}} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}) \right)\bigg\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}} \right) \right]^{-1}}^{ \overset{p}{\to} \left[ ||\frac{\partial}{\partial \boldsymbol{\theta}} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta})\right)||_{\boldsymbol{W}}\rvert_{\boldsymbol{\theta} = \boldsymbol{\theta}_0} \right]^{-1}}\\
&amp;\underbrace{\left(\frac{\partial}{\partial \boldsymbol{\theta}} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}) \right)\bigg\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}} \right)^T \widehat{\boldsymbol{W}}}_{\overset{p}{\to} \left(\frac{\partial}{\partial \boldsymbol{\theta}} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta})\right)\rvert_{\boldsymbol{\theta} = \boldsymbol{\theta}_0} \right)^T \boldsymbol{W}} \sqrt{n} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}_0) \right) + o_p(1).
\end{aligned}
\end{equation*}\]</span>
</div>

</div>
<div id="clt-for-dependent-processes" class="section level3">
<h3><span class="header-section-number">3.3.2</span> CLT for Dependent Processes</h3>
<p>For a dependent process, the validity of CLT requires the process to be âmixingâ or âasymptotically independentâ. Suppose two events <span class="math inline">\(G\)</span> and <span class="math inline">\(H\)</span> are independent, then <span class="math display">\[|\mathbb{P}(G \cap H) - \mathbb{P}(G) \mathbb{P}(H)| = 0.\]</span></p>
<p>Based on this idea, many dependence measures have been developed. The most commonly used <span class="math inline">\(\alpha\)</span>-mixing coefficient is one of these dependence measures which can be easily verified for certain stochastic processes.</p>

<div class="definition">
<p><span id="def:mixcoef" class="definition"><strong>Definition 3.6  (Mixing Coefficients)  </strong></span>For a stochastic process <span class="math inline">\(\{ X_i \}_{i \in \mathbb{Z}}\)</span>, we define the strong- or <span class="math inline">\(\alpha\)</span>-mixing coefficients as</p>
<span class="math display">\[\begin{equation*}
\alpha(t_1, t_2) = \text{sup}\{ |\mathbb{P}(A \cap B) - \mathbb{P}(A)\mathbb{P}(B)|: \; A\in\mathcal{F}_{-\infty}^{t_1}, B\in\mathcal{F}_{t_2}^{\infty} \},
\end{equation*}\]</span>
where <span class="math inline">\(\mathcal{F}_{-\infty}^{t_1} = \sigma(X_{-\infty}, \dots, X_{t_1})\)</span> and <span class="math inline">\(\mathcal{F}_{t_2}^{\infty} = \sigma(X_{t_2}, \dots, X_{\infty})\)</span> are <span class="math inline">\(\sigma\)</span>-algebras generated by corresponding random variables.
</div>


<div class="exercise">
<span id="exr:MixCoefRemark" class="exercise"><strong>Remark 3.1  (Mixing Coefficients)  </strong></span>If the process is stationary, then <span class="math inline">\(\alpha(t_1, t_2) = \alpha(t_2, t_1) = \alpha(|t_1-t_2|) \equiv \alpha(\tau)\)</span>. If <span class="math inline">\(\alpha(\tau) \to 0\)</span> as <span class="math inline">\(\tau \to \infty\)</span>, then the process is strong-mixing or <span class="math inline">\(\alpha\)</span>-mixing.
</div>


<div class="theorem">
<p><span id="thm:CLTalpha" class="theorem"><strong>Theorem 3.10  (Central Limit Theorem and Alpha-Mixing)  </strong></span>Let <span class="math inline">\((X_t)\)</span> be a strictly stationary process with <span class="math inline">\(\mathbb{E}[X_t] =0\)</span>. <span class="math inline">\(S_n \equiv \sum_{t=1}^{n}X_t\)</span> is the partial sum process with <span class="math inline">\(\sigma_n^2 \equiv \text{Var}(S_n)\)</span>. Suppose <span class="math inline">\((X_t)\)</span> is <span class="math inline">\(\alpha\)</span>-mixing, and that for <span class="math inline">\(\delta &gt; 0\)</span>, we have</p>
<span class="math display">\[\begin{equation*}
\mathbb{E}\left[ |X_t|^{2+\delta} \right] \leq \infty, \mbox{ and } \sum_{n = 0}^{\infty} \alpha(n)^{\delta/2 + \delta} \leq \infty.
\end{equation*}\]</span>
Then
<span class="math display">\[\begin{equation*}
\underset{n \to \infty}{\text{lim}}\frac{\sigma_n^2}{n} = \mathbb{E}\left[ |X_t|^{2} \right] + 2\sum_{k=1}^{\infty}\mathbb{E}\left[ X_1X_k \right] \equiv \sigma^2.
\end{equation*}\]</span>
If <span class="math inline">\(\sigma^2 &gt; 0\)</span>, <span class="math inline">\((X_t)\)</span> obeys both the Central Limit Theorem with variance <span class="math inline">\(\sigma^2\)</span>, and the functional Central Limit Theorem.
</div>


<div class="exercise">
<span id="exr:ImpAlphaMix" class="exercise"><strong>Remark 3.2  (Implication of Alpha-Mixing)  </strong></span>If a process <span class="math inline">\((X_t)\)</span> is <span class="math inline">\(\alpha\)</span>-mixing, then its covariance structure is absolutely summable.
</div>

</div>
<div id="asymptotic-normality-of-sample-autocovariance-and-autocorrelation-functions" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Asymptotic Normality of Sample AutoCovariance and AutoCorrelation Functions</h3>
<p>If <span class="math inline">\((X_t)\)</span> is a white noise, then <span class="math inline">\(\hat{\rho}(h)\)</span> should be equal to 0 if <span class="math inline">\(h \neq 0\)</span>. In practice, this is of course not the case due the estimation error of <span class="math inline">\(\hat{\rho}(h)\)</span>. The next result gives us a way to assess whether the data comes from a completely random series or whether correlations are statistically significant at some lags.</p>

<div class="theorem">
<span id="thm:DistACF" class="theorem"><strong>Theorem 3.11  (Distribution of Sample ACF in iid Case)  </strong></span>If <span class="math inline">\((X_t)\)</span> is white noise (with finite variance) and <span class="math inline">\(h = 1, ..., H\)</span> where <span class="math inline">\(H\)</span> is fixed but arbitrary, then we have that
<span class="math display">\[\begin{equation*}
\sqrt{T} \left(\hat{\rho}(h) - {\rho}(h)\right) \overset{\mathcal{D}}{\mapsto}  \mathcal{N}\left(0, 1\right).
\end{equation*}\]</span>
</div>

<p>The proof of Theorem <a href="properties-of-statistical-estimators.html#thm:DistACF">3.11</a> is straightforward from the CLT and Delta method. It is therefore omitted here but can for example be found in <span class="citation">Hamilton (<a href="#ref-hamilton1994time">1994</a>)</span>.</p>
<p>Theorem <a href="properties-of-statistical-estimators.html#thm:DistACF">3.11</a> implies that an approximate confidence interval for <span class="math inline">\(\hat{\rho}(h)\)</span> (in the iid case) is given by <span class="math display">\[\text{CI}({\rho}(h), \alpha) = \hat{\rho}(h) \pm \frac{z_{1-\frac{\alpha}{2}} }{\sqrt{T}}\]</span> for <span class="math inline">\(0 &lt; h &lt; k &lt; \infty\)</span> and where <span class="math inline">\(z_{1- \frac{\alpha}{2}} \equiv \boldsymbol{\Phi}^{-1}\left( 1- \frac{\alpha}{2} \right)\)</span> is the <span class="math inline">\((1- \frac{\alpha}{2})\)</span> quantile of a standard normal distribution. Typically, for <span class="math inline">\(\alpha = 0.05\)</span> one would consider the following confidence interval: <span class="math display">\[\text{CI}({\rho}(h), 0.05) = \hat{\rho}(h) \pm \frac{2}{\sqrt{T}}.\]</span></p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-newey1994vlarge">
<p>Newey, W. K., and D. McFadden. 1994. âLarge Sample Estimation and Hypothesis Testing, V in Handbook of Econometrics.â In. Vol. 4. Elsevier, Amsterdam.</p>
</div>
<div id="ref-komunjer2012global">
<p>Komunjer, I. 2012. âGlobal Identification in Nonlinear Models with Moment Restrictions.â <em>Econometric Theory</em> 28 (4): 719.</p>
</div>
<div id="ref-baltagi2008companion">
<p>Baltagi, Badi H. 2008. <em>A Companion to Theoretical Econometrics</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-hamilton1994time">
<p>Hamilton, J. D. 1994. <em>Time Series Analysis</em>. Vol. 2. Princeton university press Princeton.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="timeseries.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="allan-variance-calibration-techniques.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/02-estimators.Rmd",
"text": "Edit"
},
"download": ["scis.pdf", "scis.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
