<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>An Introduction to Inertial Sensors Stochastic Calibration</title>
  <meta name="description" content="TO DO">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="An Introduction to Inertial Sensors Stochastic Calibration" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="TO DO" />
  <meta name="github-repo" content="SMAC-Group/SCIS" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="An Introduction to Inertial Sensors Stochastic Calibration" />
  
  <meta name="twitter:description" content="TO DO" />
  

<meta name="author" content="Stéphane Guerrier, Roberto Molinari, Yuming Zhang, Haotian Xu, Gaetan Bakalli, Ahmed Radi and Mucyo Karemera">


<meta name="date" content="2018-07-01">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="timeseries.html">
<link rel="next" href="allan-variance-calibration-techniques.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="timeseries.html"><a href="timeseries.html"><i class="fa fa-check"></i><b>2</b> Introduction to Time Series Analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="timeseries.html"><a href="timeseries.html#time-series"><i class="fa fa-check"></i><b>2.1</b> Time Series</a></li>
<li class="chapter" data-level="2.2" data-path="timeseries.html"><a href="timeseries.html#latent-time-series-processes-and-composite-stochastic-process"><i class="fa fa-check"></i><b>2.2</b> Latent Time Series Processes and Composite Stochastic Process</a></li>
<li class="chapter" data-level="2.3" data-path="timeseries.html"><a href="timeseries.html#dependence-within-time-series"><i class="fa fa-check"></i><b>2.3</b> Dependence within Time Series</a></li>
<li class="chapter" data-level="2.4" data-path="timeseries.html"><a href="timeseries.html#stationarity"><i class="fa fa-check"></i><b>2.4</b> Stationarity</a></li>
<li class="chapter" data-level="2.5" data-path="timeseries.html"><a href="timeseries.html#linear-processes"><i class="fa fa-check"></i><b>2.5</b> Linear Processes</a></li>
<li class="chapter" data-level="2.6" data-path="timeseries.html"><a href="timeseries.html#fundamental-representations-of-time-series"><i class="fa fa-check"></i><b>2.6</b> Fundamental Representations of Time Series</a></li>
<li class="chapter" data-level="2.7" data-path="timeseries.html"><a href="timeseries.html#estimation-problems-with-time-series"><i class="fa fa-check"></i><b>2.7</b> Estimation Problems with Time Series</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html"><i class="fa fa-check"></i><b>3</b> Properties of Statistical Estimators</a><ul>
<li class="chapter" data-level="3.1" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#extremum-estimators"><i class="fa fa-check"></i><b>3.1</b> Extremum Estimators</a></li>
<li class="chapter" data-level="3.2" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#consistency"><i class="fa fa-check"></i><b>3.2</b> Consistency</a><ul>
<li class="chapter" data-level="3.2.1" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#consistency-of-extremum-estimators"><i class="fa fa-check"></i><b>3.2.1</b> Consistency of Extremum Estimators</a></li>
<li class="chapter" data-level="3.2.2" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#verification-of-condition-c1"><i class="fa fa-check"></i><b>3.2.2</b> Verification of Condition C1</a></li>
<li class="chapter" data-level="3.2.3" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#verification-of-condition-c4"><i class="fa fa-check"></i><b>3.2.3</b> Verification of Condition C4</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html"><i class="fa fa-check"></i><b>4</b> Allan Variance Calibration Techniques</a><ul>
<li class="chapter" data-level="4.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#spectral-ambiguity-of-the-av"><i class="fa fa-check"></i><b>4.1</b> Spectral Ambiguity of the AV</a></li>
<li class="chapter" data-level="4.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#properties-of-the-allan-variance"><i class="fa fa-check"></i><b>4.2</b> Properties of the Allan Variance</a></li>
<li class="chapter" data-level="4.3" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#estimation"><i class="fa fa-check"></i><b>4.3</b> Estimation</a><ul>
<li class="chapter" data-level="4.3.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#consistency-1"><i class="fa fa-check"></i><b>4.3.1</b> Consistency</a></li>
<li class="chapter" data-level="4.3.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#asymptotic-normality"><i class="fa fa-check"></i><b>4.3.2</b> Asymptotic Normality</a></li>
<li class="chapter" data-level="4.3.3" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#confidence-interval-of-the-moav-estimator"><i class="fa fa-check"></i><b>4.3.3</b> Confidence Interval of the MOAV Estimator</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#allan-variance-based-estimation"><i class="fa fa-check"></i><b>4.4</b> Allan variance based estimation</a><ul>
<li class="chapter" data-level="4.4.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#allan-variance-log-log-representation"><i class="fa fa-check"></i><b>4.4.1</b> Allan Variance log-log Representation</a></li>
<li class="chapter" data-level="4.4.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#allan-deviation-of-a-wn-process"><i class="fa fa-check"></i><b>4.4.2</b> Allan Deviation of a WN process</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-generalized-method-of-wavelet-moments.html"><a href="the-generalized-method-of-wavelet-moments.html"><i class="fa fa-check"></i><b>5</b> The Generalized Method of Wavelet Moments</a></li>
<li class="chapter" data-level="6" data-path="extensions.html"><a href="extensions.html"><i class="fa fa-check"></i><b>6</b> Extensions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Inertial Sensors Stochastic Calibration</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="properties-of-statistical-estimators" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Properties of Statistical Estimators</h1>
<p>In this chapter, we will provide a review of the properties of statistical estimators. This chapter is organized with the following outline:</p>
<ul>
<li>Extremum estimators;</li>
<li>Consistency;</li>
<li>Asymptotic normality.</li>
</ul>
<div id="extremum-estimators" class="section level2">
<h2><span class="header-section-number">3.1</span> Extremum Estimators</h2>
<p>In this section, we will introduce a commonly used class of estimators, extremum estimators, and some examples of it.</p>

<div class="definition">
<span id="def:EstreEst" class="definition"><strong>Definition 3.1  (Extremum Estimators)  </strong></span>Many estimators have a common structure, which is often useful to study their asymptotic properties. One structure or framework is the class of estimators that maximize some objective function, referred to as extremum estimators, which can can be defined as follows:
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\theta}} \equiv \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; \hat{Q}_n(\boldsymbol{\theta})
\end{equation*}\]</span>
where <span class="math inline">\(\boldsymbol{\theta}\)</span> and <span class="math inline">\(\boldsymbol{\Theta}\)</span> denote, respectively, the parameter vector of interest and its set of possible values.
</div>
<p></p>
<p>The vast majority of statistical estimators can be represented as extremum estimators. For example, least squares, maximum likelihood or (generalized) method of moment estimators can all be represented as extremum estimators.</p>

<div class="example">
<span id="exm:ExtreEstLSE" class="example"><strong>Example 3.1  (Least Squares Estimator as Extremum Estimator)  </strong></span>Consider the linear model <span class="math inline">\(\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta}_0 + \boldsymbol{\epsilon}\)</span> where <span class="math inline">\(\boldsymbol{X} \in \mathbb{R}^{n \times p}\)</span> is a full rank constant matrix, <span class="math inline">\(\boldsymbol{\beta} \in {\mathcal{B}} \subseteq \mathbb{R}^p\)</span> and <span class="math inline">\(\epsilon_i \overset{iid}{\sim} \mathcal{N}(0, \sigma_\epsilon^2)\)</span>. Let <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> denote the Least Squares Estimator (LSE) of <span class="math inline">\(\boldsymbol{\beta}_0\)</span>, i.e.
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\beta}} = \left(\boldsymbol{X}^T \boldsymbol{X} \right)^{-1} \boldsymbol{X}^T \boldsymbol{y}. 
\end{equation*}\]</span>
This LSE is an extremum estimator since it can be expressed as
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\beta}} = \underset{\boldsymbol{\beta} \in \mathcal{B}}{\text{argmax}} \; -||\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} ||_2^2.
\end{equation*}\]</span>
</div>
<p></p>

<div class="example">
<span id="exm:ExtreEstMLE" class="example"><strong>Example 3.2  (Maximum Likelihood Estimator as Extremum Estimator)  </strong></span>Let <span class="math inline">\(Z_1, \ldots, Z_n\)</span> be an iid sample with pdf <span class="math inline">\(f(z|\boldsymbol{\theta}_0)\)</span>. The Maximum Likelihood Estimator (MLE) is given by
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; \frac{1}{n} \sum_{i = 1}^{n} \text{log}\left[f\left(z_i| \boldsymbol{\theta}\right)\right].
\end{equation*}\]</span>
<p>Here instead of the actual log-likelihood, we are actually using a normalized log-likelihood, which has no impact on the estimator but the normalized form is more convenient to use when we let <span class="math inline">\(n \to \infty\)</span>.</p>
Therefore, the MLE can be seen as an extremum estimator with
<span class="math display">\[\begin{equation*}
\hat{Q}_n(\boldsymbol{\theta}) = \frac{1}{n} \sum_{i = 1}^{n} \text{log}\left[f\left(z_i| \boldsymbol{\theta}\right)\right].
\end{equation*}\]</span>
</div>
<p></p>

<div class="definition">
<span id="def:GMM" class="definition"><strong>Definition 3.2  (Generalized Method of Moments)  </strong></span>Let <span class="math inline">\(Z_1, \ldots, Z_n\)</span> be an iid sample with pdf <span class="math inline">\(f(z|\boldsymbol{\theta}_0)\)</span>. Suppose that there is a moment function vector <span class="math inline">\(\boldsymbol{g}(z | \boldsymbol{\theta})\)</span> such that <span class="math inline">\(\mathbb{E}[\boldsymbol{g} (z | \boldsymbol{\theta}_0)] = 0\)</span>. Then the Generalized Method of Moments (GMM) estimator of <span class="math inline">\(\boldsymbol{\theta}_0\)</span> is defined as
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; - \left[\frac{1}{n} \sum_{i = 1}^n \boldsymbol{g}(z_i | \boldsymbol{\theta}) \right]^T \widehat{\boldsymbol{W}} \left[\frac{1}{n} \sum_{i = 1}^n \boldsymbol{g}(z_i | \boldsymbol{\theta}) \right],
\end{equation*}\]</span>
<p>where <span class="math inline">\(\widehat{\boldsymbol{W}}\)</span> is an positive definite matrix of appropriate dimension.</p>
Alternatively (but equivalently), we can define GMM estimator as
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; - || \hat{\boldsymbol{\mu}} - \boldsymbol{\mu}(\boldsymbol{\theta}) ||_{\widehat{\boldsymbol{W}}}^2,
\end{equation*}\]</span>
where <span class="math inline">\(|| \boldsymbol{x} ||_{\boldsymbol{A}}^2 = \boldsymbol{x}^T \boldsymbol{A} \boldsymbol{x}\)</span>, and where <span class="math inline">\(\hat{\boldsymbol{\mu}}\)</span> and <span class="math inline">\(\boldsymbol{\mu}(\boldsymbol{\theta})\)</span> denote, respectively, the empirical and model based moments. We will use this form of definition of GMM estimator more often.
</div>
<p></p>

<div class="example">
<span id="exm:ExtreEstGMM" class="example"><strong>Example 3.3  (A Simple GMM Estimator as Extremum Estimator)  </strong></span>Let <span class="math inline">\(Z_i \overset{iid}{\sim} \mathcal{N}(\mu_0, \sigma^2_0)\)</span> and <span class="math inline">\(\boldsymbol{\theta}_0 = (\mu_0, \sigma_0^2)^T\)</span>. Suppose we wish to estimate <span class="math inline">\(\boldsymbol{\theta}_0\)</span> by matching the first three empirical moments with their theoretical counterparts. In this case, a reasonable moment function or condition defining a GMM estimator is given by:
<span class="math display">\[\begin{equation*}
\boldsymbol{g} (Z | \boldsymbol{\theta}) = \begin{bmatrix}
Z - \mu\\
Z^2 - \left(\mu^2 + \sigma^2\right)\\
Z^3 - \left(\mu^3 + 3 \mu \sigma^2\right)
\end{bmatrix}.
\end{equation*}\]</span>
Notice that <span class="math inline">\(\frac{1}{n} \sum_{i=1}^n \boldsymbol{g} (Z_i | \boldsymbol{\theta}) = \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta})\)</span>, where <span class="math inline">\(\hat{\boldsymbol{\gamma}}\)</span> and <span class="math inline">\(\boldsymbol{\gamma}(\boldsymbol{\theta})\)</span> denote, respectively, the empirical and model based moments, i.e.
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\gamma}} = \frac{1}{n}  \sum_{i = 1}^n 
\begin{bmatrix}
Z_i\\
Z_i^2\\
Z_i^3\\
\end{bmatrix}, \;\;\;\;
\boldsymbol{\gamma}(\boldsymbol{\theta}) =  \begin{bmatrix}
\mu\\
\mu^2 + \sigma^2\\
\mu^3 + 3 \mu \sigma^2
\end{bmatrix}.
\end{equation*}\]</span>
Therefore we can write the GMM estimator of <span class="math inline">\(\boldsymbol{\theta}_0\)</span> as
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmin}} \; || \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma} (\boldsymbol{\theta}) ||_{\widehat{\boldsymbol{W}}}^2 = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; - || \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma} (\boldsymbol{\theta}) ||_{\widehat{\boldsymbol{W}}}^2 
\end{equation*}\]</span>
</div>
<p></p>
</div>
<div id="consistency" class="section level2">
<h2><span class="header-section-number">3.2</span> Consistency</h2>
<p>In this section, we will introduce one of the most important properties of statistical estimators, consistency. And we will discuss in details about the conditions for consistency of the extremum estimators.</p>

<div class="definition">
<span id="def:consistency" class="definition"><strong>Definition 3.3  (Consistency)  </strong></span>An estimator <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> is said to be consistent or weakly consistent if it converges in probability to <span class="math inline">\(\boldsymbol{\theta}_0\)</span>, i.e.
<span class="math display">\[\begin{equation*}
\underset{n \to \infty}{\text{lim}} \mathbb{P} (||\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0||_2 \geq \epsilon) = 0,
\end{equation*}\]</span>
for all <span class="math inline">\(\epsilon &gt;0\)</span>.
</div>
<p></p>
<p>In Layman’s term, consistency simply means that if <span class="math inline">\(n\)</span> is large enough, then <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> will be arbitrarily close to <span class="math inline">\(\boldsymbol{\theta}_0\)</span> (i.e. inside of an hypersphere of radius <span class="math inline">\(\epsilon\)</span> centered at <span class="math inline">\(\boldsymbol{\theta}_0\)</span>). This also means the procedure (i.e. our estimator <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>) based on unlimited data will be able to identify the underlying truth (i.e. <span class="math inline">\(\boldsymbol{\theta}_0\)</span>).</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-1"></span>
<img src="images/consistency.pdf" alt="Interpretation of consistency" width="80%" />
<p class="caption">
Figure 3.1: Interpretation of consistency
</p>
</div>
<p>To show the consistency of estimators, we often make use of the following two important results:</p>

<div class="theorem">
<span id="thm:WLLN" class="theorem"><strong>Theorem 3.1  (Weak Law of Large Number)  </strong></span>Suppose <span class="math inline">\(X_i\)</span> are iid random variables with finite mean <span class="math inline">\(\mu\)</span> (i.e. <span class="math inline">\(\mathbb{E}[X_i] = \mu\)</span>) and finite variance. Let <span class="math inline">\(\bar{X}_n = \frac{1}{n} \sum_{i = 1}^n X_i\)</span>, then <span class="math inline">\(\bar{X}_n \overset{\mathcal{P}}{\mapsto} \mu\)</span>.
</div>
<p></p>

<div class="theorem">
<span id="thm:CMT" class="theorem"><strong>Theorem 3.2  (Continuous Mapping Theorem)  </strong></span>If <span class="math inline">\(Y_n \overset{\mathcal{P}}{\mapsto} \mu\)</span> and <span class="math inline">\(g(\cdot)\)</span> is a continuous function, then <span class="math inline">\(g(Y_n) \overset{\mathcal{P}}{\mapsto} g(\mu)\)</span>.
</div>
<p></p>
<p>Here we present a simple example to prove the consistency of an estimator with the above two results.</p>

<div class="example">
<span id="exm:ConsistencyExpDist" class="example"><strong>Example 3.4  (Consistency in Exponential Distribution)  </strong></span>Suppose we have an iid sample from exponential distribution, i.e. <span class="math inline">\(X_i \overset{iid}{\sim} \text{exp}(\lambda_0),\; \lambda_0 \in \mathbb{R}^+, \; i = 1,..., n\)</span>. So assuming <span class="math inline">\(X \geq 0\)</span>, the density of <span class="math inline">\(X\)</span> is given by
<span class="math display">\[\begin{equation*}
f(x|\lambda) = \lambda \text{exp}\left( - \lambda x \right).
\end{equation*}\]</span>
<p>In this example, we want to show that the MLE for <span class="math inline">\(\lambda_0\)</span> is a consistent estimator of <span class="math inline">\(\lambda_0\)</span>.</p>
First, we want to find the MLE for <span class="math inline">\(\lambda_0\)</span>. The normalized log-likelihood function is given by
<span class="math display">\[\begin{equation*}
\mathcal{L}(\lambda | X_1, ..., X_n) = \text{log}(\lambda) - \lambda \bar{X}_n.
\end{equation*}\]</span>
By solving
<span class="math display">\[\begin{equation*}
\frac{\partial}{\partial \lambda} \; \mathcal{L}(\lambda | X_1, \ldots, X_n) = \frac{1}{\lambda} - \bar{X}_n = 0,
\end{equation*}\]</span>
we obtain <span class="math inline">\(\hat{\lambda} = \frac{1}{\bar{X}_n}\)</span>. We verify that
<span class="math display">\[\begin{equation*}
\frac{\partial^2}{\partial \lambda^2} \; \mathcal{L}(\lambda | X_1, \ldots, X_n) = -\frac{1}{\lambda^2} &lt; 0,
\end{equation*}\]</span>
<p>which implies that <span class="math inline">\(\hat{\lambda}\)</span> is the maxima of <span class="math inline">\(\mathcal{L}(\lambda | X_1, \ldots, X_n)\)</span>. Therefore, the MLE for <span class="math inline">\(\lambda_0\)</span> is <span class="math inline">\(\hat{\lambda} = \frac{1}{\bar{X}_n}\)</span>.</p>
By Weak Law of Large Number, we have <span class="math inline">\(\bar{X}_n \overset{\mathcal{P}}{\mapsto} \mu\)</span>, where is given by
<span class="math display">\[\begin{equation*}
\mu = \mathbb{E}[X_i] = \int_{0}^{\infty} x \lambda_0 \text{exp}\left( - \lambda_0 x \right) dx = \frac{1}{\lambda_0}.
\end{equation*}\]</span>
And also since the function <span class="math inline">\(g(x) = 1/x\)</span> is continuous in <span class="math inline">\(\mathbb{R}^+\)</span>, we obtain by the Continuous Mapping Theorem that <span class="math inline">\(\hat{\lambda} \overset{\mathcal{P}}{\mapsto} \lambda_0\)</span>, which concludes that the MLE for <span class="math inline">\(\lambda_0\)</span> is a consistent estimator of <span class="math inline">\(\lambda_0\)</span> in exponential distribution.
</div>
<p></p>
<div id="consistency-of-extremum-estimators" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Consistency of Extremum Estimators</h3>
<p>When considering real-life problems, the above approach based on Weak Law of Large Number and Continuous Mapping Theorem is in general not flexible enough. Therefore, we often rely on the results as following.</p>

<div class="theorem">
<p><span id="thm:ConsExtreEst" class="theorem"><strong>Theorem 3.3  (Consistency of Extremum Estimators)  </strong></span>If there is a function <span class="math inline">\({Q}_0 (\boldsymbol{\theta})\)</span> such that:</p>
<p>C1. <span class="math inline">\({Q}_0 (\boldsymbol{\theta})\)</span> is uniquely maximized in <span class="math inline">\(\boldsymbol{\theta}_0\)</span>,</p>
<p>C2. <span class="math inline">\(\boldsymbol{\Theta}\)</span> is compact,</p>
<p>C3. <span class="math inline">\({Q}_0 (\boldsymbol{\theta})\)</span> is continuous in <span class="math inline">\(\boldsymbol{\theta}\)</span>,</p>
<p>C4. <span class="math inline">\(\hat{Q}_n (\boldsymbol{\theta})\)</span> converges uniformly in probability to <span class="math inline">\(Q_0 (\boldsymbol{\theta})\)</span>,</p>
then we have <span class="math inline">\(\hat{\boldsymbol{\theta}} \overset{\mathcal{P}}{\mapsto} \boldsymbol{\theta}_0\)</span>.
</div>
<p></p>

<div class="definition">
<span id="def:compact" class="definition"><strong>Definition 3.4  (Compactness)  </strong></span>We say <span class="math inline">\(\boldsymbol{\Theta}\)</span> is compact if every open cover of <span class="math inline">\(\boldsymbol{\Theta}\)</span> contains a finite subcover. If <span class="math inline">\(\boldsymbol{\Theta}\)</span> is compact, then <span class="math inline">\(\boldsymbol{\Theta}\)</span> is closed (i.e. containing all its limit points) and bounded (i.e. all its points are within some finite distance of each other).
</div>
<p></p>

<div class="definition">
<span id="def:UnifCont" class="definition"><strong>Definition 3.5  (Uniform Convergence in Probability)  </strong></span><span class="math inline">\(\hat{Q}_n (\boldsymbol{\theta})\)</span> is said to converges uniformly in probability to <span class="math inline">\(Q_0 (\boldsymbol{\theta})\)</span> if <span class="math inline">\(\underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{sup}} \; |\hat{Q}_n (\boldsymbol{\theta}) - {Q}_0 (\boldsymbol{\theta})| \overset{\mathcal{P}}{\mapsto} 0\)</span>.
</div>
<p></p>
<p>Theorem <a href="properties-of-statistical-estimators.html#thm:ConsExtreEst">3.3</a> is an important result as it provides a general approach to prove the consistency of the class of extremum estimators. Notice that in this theorem:</p>
<ul>
<li>Condition (C1) is <strong>substantive</strong> and there are well-known examples where it fails. We will discuss further on how this assumption can (in some cases) be verified in practice.</li>
<li>Condition (C2) is also <strong>substantive</strong> as it requires that there exist some known bounds on the parameters. In practice, this assumption is often neglected although it is in most cases unrealistic to assume it.</li>
<li>Condition (C3) and (C4) are often referred to as <strong>“standard regularity conditions”</strong>. They are typically satisfied. The verification of these conditions will be discussed further later in this section.</li>
</ul>

<div class="proof">
<p> <span class="proof"><em>Proof</em> (Consistency of Extremum Estimators). </span> Let <span class="math inline">\(\mathcal{G}\)</span> be an <span class="math inline">\(\epsilon\)</span>-neighborhood centered at <span class="math inline">\(\boldsymbol{\theta}_0\)</span> for some <span class="math inline">\(\epsilon &gt; 0\)</span>, i.e. <span class="math inline">\(\mathcal{G} = \{ \boldsymbol{\theta} \in \boldsymbol{\Theta}: || \boldsymbol{\theta} - \boldsymbol{\theta}_0 ||_2 &lt; \epsilon\}\)</span> for some <span class="math inline">\(\epsilon &gt; 0\)</span>.</p>
We want to show <span class="math inline">\(\hat{\boldsymbol{\theta}} \overset{\mathcal{P}}{\mapsto} \boldsymbol{\theta}_0\)</span>, which is equivalent to show
<span class="math display">\[\begin{equation*}
\underset{n \to \infty}{\text{lim}} \mathbb{P}(||\boldsymbol{\theta} - \boldsymbol{\theta}_0 ||_2 \geq \epsilon) = \underset{n \to \infty}{\text{lim}} \mathbb{P}(\hat{\boldsymbol{\theta}} \notin \mathcal{G}) = 0
\end{equation*}\]</span>
<p>Define <span class="math inline">\(\gamma = Q_0(\boldsymbol{\theta}_0) - \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta} \setminus \mathcal{G}}{\text{sup}} Q_0({\boldsymbol{\theta}}) &gt; 0 \;\;\)</span> by condition C1.</p>
<p>So <span class="math inline">\(\hat{\boldsymbol{\theta}} \notin \mathcal{G}\)</span> implies that <span class="math inline">\(Q_0(\hat{\boldsymbol{\theta}}) \leq \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta} \setminus \mathcal{G}}{\text{sup}} Q_0({\boldsymbol{\theta}}) = Q_0(\boldsymbol{\theta}_0) - \gamma\)</span>.</p>
<p>Let <span class="math inline">\(\mathcal{A} = \{ Q_0(\hat{\boldsymbol{\theta}}) \leq Q_0(\boldsymbol{\theta}_0) - \gamma \}\)</span>. So <span class="math inline">\(\underset{n \to \infty}{\text{lim}}\mathbb{P}(\hat{\boldsymbol{\theta}} \notin \mathcal{G}) \leq \underset{n \to \infty}{\text{lim}}\mathbb{P}(\mathcal{A})\)</span>.</p>
Now we define the following events:
<span class="math display">\[\begin{align*}
\mathcal{B} &amp;= \{|\hat{Q_n}(\hat{\boldsymbol{\theta}}) - Q_0(\hat{\boldsymbol{\theta}})| &gt; \gamma/3  \}, \\
\mathcal{C} &amp;= \{|\hat{Q_n}(\boldsymbol{\theta}_0) - Q_0(\boldsymbol{\theta}_0)| &gt; \gamma/3 \}, \\
\mathcal{D} &amp;= \{\underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{sup}} |\hat{Q_n}(\boldsymbol{\theta}) - Q_0(\boldsymbol{\theta})| &gt;\gamma/3 \}.
\end{align*}\]</span>
So we have
<span class="math display">\[\begin{align*}
\mathbb{P} (\mathcal{A}) &amp;\leq \mathbb{P} (\mathcal{A} \cup (\mathcal{B} \cup \mathcal{C})) + \mathbb{P}(\mathcal{B} \cup \mathcal{C}) \\
&amp;= \mathbb{P}(\mathcal{A} \cap \mathcal{B}^c \cap \mathcal{C}^c) + \mathbb{P}(\mathcal{B} \cup \mathcal{C}) \\
&amp;= \mathbb{P}(\emptyset) + \mathbb{P}(\mathcal{B} \cup \mathcal{C}) \\
&amp;= \mathbb{P}(\mathcal{B} \cup \mathcal{C}).
\end{align*}\]</span>
<p>Notice that <span class="math inline">\(\mathcal{A} \cap \mathcal{B}^c \cap \mathcal{C}^c = \emptyset\)</span> because if <span class="math inline">\(\mathcal{A}\)</span>, <span class="math inline">\(\mathcal{B}^c\)</span>, and <span class="math inline">\(\mathcal{C}^c\)</span> hold simultaneously, then</p>
<span class="math display">\[\begin{align*}
\hat{Q_n}(\hat{\boldsymbol{\theta}}) &amp;= \hat{Q_n}(\hat{\boldsymbol{\theta}}) - Q_0(\hat{\boldsymbol{\theta}}) + Q_0(\hat{\boldsymbol{\theta}}) \\
&amp;\leq |\hat{Q_n}(\hat{\boldsymbol{\theta}}) - Q_0(\hat{\boldsymbol{\theta}})| + Q_0(\hat{\boldsymbol{\theta}}) \\
&amp;\leq \gamma/3 + Q_0(\hat{\boldsymbol{\theta}}) \;\;\;\;\;\; \text{since} \;\; \mathcal{B}^c \;\; \text{holds}\\
&amp;\leq Q_0({\boldsymbol{\theta}}_0) - 2\gamma/3 \;\;\;\;\;\; \text{since} \;\; \mathcal{A} \;\; \text{holds}\\
&amp;\leq Q_0({\boldsymbol{\theta}}_0) - \hat{Q_n}(\hat{\boldsymbol{\theta}}_0) + \hat{Q_n}(\hat{\boldsymbol{\theta}}_0) - 2\gamma/3 \\
&amp;\leq |Q_0({\boldsymbol{\theta}}_0) - \hat{Q_n}({\boldsymbol{\theta}}_0)| + \hat{Q_n}({\boldsymbol{\theta}}_0) - 2\gamma/3 \\
&amp;\leq \hat{Q_n}({\boldsymbol{\theta}}_0)  - \gamma/3 \;\;\;\;\;\; \text{since} \;\; \mathcal{C}^c \;\; \text{holds}\\
&amp;&lt; \hat{Q_n}({\boldsymbol{\theta}}_0)
\end{align*}\]</span>
<p>which contradicts that <span class="math inline">\(\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \hat{Q_n}({\boldsymbol{\theta}})\)</span>.</p>
So
<span class="math display">\[\begin{align*}
\mathbb{P}(\mathcal{A}) &amp;= \mathbb{P}(\mathcal{B} \cup \mathcal{C}) \\
&amp;= \mathbb{P}(|\hat{Q_n}(\hat{\boldsymbol{\theta}}) - Q_0(\hat{\boldsymbol{\theta}})| &gt; \gamma/3 \;\; \text{or} \;\; |\hat{Q_n}(\boldsymbol{\theta}_0) - Q_0(\boldsymbol{\theta}_0)| &gt; \gamma/3) \\
&amp;= \mathbb{P} (\underset{\boldsymbol{\theta} \in  \{\hat{\boldsymbol{\theta}}, \boldsymbol{\theta}_0\} }{\text{sup}} |\hat{Q_n}(\boldsymbol{\theta}) - Q_0(\boldsymbol{\theta})| &gt; \gamma/3) \\
&amp; \leq \mathbb{P} (\underset{\boldsymbol{\theta} \in  \boldsymbol{\Theta} }{\text{sup}} |\hat{Q_n}(\boldsymbol{\theta}) - Q_0(\boldsymbol{\theta})| &gt; \gamma/3) \\
&amp;= \mathbb{P}(\mathcal{D}).
\end{align*}\]</span>
So by condition C4, we have
<span class="math display">\[\begin{equation*}
\underset{n \to \infty}{\text{lim}}\mathbb{P}(\hat{\boldsymbol{\theta}} \notin \mathcal{G}) \leq \underset{n \to \infty}{\text{lim}} \mathbb{P}(\mathcal{A}) \leq \underset{n \to \infty}{\text{lim}}\mathbb{P}(\mathcal{D}) = 0.
\end{equation*}\]</span>
Therefore, <span class="math inline">\(\underset{n \to \infty}{\text{lim}}\mathbb{P}(\hat{\boldsymbol{\theta}} \notin \mathcal{G}) = 0\)</span>.
</div>
<p></p>
</div>
<div id="verification-of-condition-c1" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Verification of Condition C1</h3>
<p>In general, the verification of Condition C1 is difficult and is often assumed in the statistical literature. Here we present two results based on <span class="citation">Newey and McFadden (<a href="#ref-newey1994vlarge">1994</a>)</span> and <span class="citation">Komunjer (<a href="#ref-komunjer2012global">2012</a>)</span> respectively, which allow us to verify the condition C1 for GMM-type estimators. A discussion on the verification of this condition for other estimators can for example be found in Chapter 7 of <span class="citation">Baltagi (<a href="#ref-baltagi2008companion">2008</a>)</span>.</p>

<div class="lemma">
<span id="lem:GMMindentification" class="lemma"><strong>Lemma 3.1  (GMM Identification)  </strong></span>If <span class="math inline">\(\boldsymbol{W} &gt; 0\)</span> (i.e. positive definite), where <span class="math inline">\(\boldsymbol{W}\)</span> is such that <span class="math inline">\(\widehat{\boldsymbol{W}} \overset{\mathcal{P}}{\mapsto} \boldsymbol{W}\)</span>, and for <span class="math inline">\(\boldsymbol{g}_0(\boldsymbol{\theta}) = \mathbb{E}[\boldsymbol{g}(z|\boldsymbol{\theta})]\)</span>, we have <span class="math inline">\(\boldsymbol{g}_0(\boldsymbol{\theta}_0)=0\)</span> and <span class="math inline">\(\boldsymbol{g}_0(\boldsymbol{\theta}) \neq 0\)</span> if <span class="math inline">\(\boldsymbol{\theta} \neq \boldsymbol{\theta}_0\)</span>, then <span class="math inline">\(Q_0(\boldsymbol{\theta}) = -\boldsymbol{g}_0(\boldsymbol{\theta})^T \boldsymbol{W} \boldsymbol{g}_0(\boldsymbol{\theta})\)</span> has a unique maximum at <span class="math inline">\(\boldsymbol{\theta}_0\)</span>.
</div>
<p></p>

<div class="proof">
 <span class="proof"><em>Proof</em> (GMM Identification). </span> Since <span class="math inline">\(\boldsymbol{W} &gt;0\)</span> and there exists a unique <span class="math inline">\(\boldsymbol{\theta}_0 \in \boldsymbol{\Theta}\)</span> such that <span class="math inline">\(\boldsymbol{g}_0(\boldsymbol{\theta}) = 0\)</span>, we can say that for all <span class="math inline">\(\boldsymbol{\theta} \in \boldsymbol{\Theta}\)</span>,
<span class="math display">\[\begin{equation*}
Q_0(\boldsymbol{\theta}) = -\boldsymbol{g}_0(\boldsymbol{\theta})^T \boldsymbol{W} \boldsymbol{g}_0(\boldsymbol{\theta}) \leq -\boldsymbol{g}_0(\boldsymbol{\theta}_0) \boldsymbol{W} \boldsymbol{g}_0(\boldsymbol{\theta}_0) = Q_0(\boldsymbol{\theta_0}).
\end{equation*}\]</span>
</div>
<p></p>
<p>Notice that if we write a GMM estimator as in Example <a href="properties-of-statistical-estimators.html#exm:ExtreEstGMM">3.3</a>, then the condition <span class="math inline">\(\boldsymbol{g}_0(\boldsymbol{\theta}) = 0\)</span> if and only if <span class="math inline">\(\boldsymbol{\theta} = \boldsymbol{\theta}_0\)</span> can be replaced by <span class="math inline">\(\boldsymbol{\gamma}(\boldsymbol{\theta}) = \boldsymbol{\gamma}(\boldsymbol{\theta}_0)\)</span> if and only if <span class="math inline">\(\boldsymbol{\theta} = \boldsymbol{\theta}_0\)</span>.</p>
<p>In order to verify the condition in Lemma <a href="properties-of-statistical-estimators.html#lem:GMMindentification">3.1</a> that <span class="math inline">\(\boldsymbol{g}_0(\boldsymbol{\theta}) = 0\)</span> if and only if <span class="math inline">\(\boldsymbol{\theta} = \boldsymbol{\theta}_0\)</span> (or alternatively <span class="math inline">\(\boldsymbol{\gamma}(\boldsymbol{\theta}) = \boldsymbol{\gamma}(\boldsymbol{\theta}_0)\)</span> if and only if <span class="math inline">\(\boldsymbol{\theta} = \boldsymbol{\theta}_0\)</span>), the following theorem based on <span class="citation">Komunjer (<a href="#ref-komunjer2012global">2012</a>)</span> provides us a way.</p>

<div class="theorem">
<p><span id="thm:Homomorphism" class="theorem"><strong>Theorem 3.4  (Homomorphism)  </strong></span>Let <span class="math inline">\(\boldsymbol{\theta} \in \boldsymbol{\Theta} \subset \mathbb{R}^p\)</span>. Let <span class="math inline">\(\boldsymbol{g}^*(\boldsymbol{\theta})\)</span> denote a subset of <span class="math inline">\(p\)</span> elements of <span class="math inline">\(\boldsymbol{g}_0 (\boldsymbol{\theta}) \in \mathbb{R}^q, \; q \geq p\)</span> such that:</p>
<ul>
<li><span class="math inline">\(\boldsymbol{g}^*(\boldsymbol{\theta})\)</span> is in <span class="math inline">\(\mathcal{C}^2\)</span> (i.e. <span class="math inline">\(\boldsymbol{g}^*(\boldsymbol{\theta})\)</span> can be differentiated twice);</li>
<li>For every <span class="math inline">\(\boldsymbol{\theta} \in \boldsymbol{\Theta}\)</span>, <span class="math inline">\(J(\boldsymbol{\theta})\)</span> is non-negative (or alternatively non-positive), where <span class="math inline">\(J(\boldsymbol{\theta}) \equiv \text{det} (\frac{\partial}{\partial \boldsymbol{\theta}^T}\, \boldsymbol{g}^*(\boldsymbol{\theta}) )\)</span>;</li>
<li><span class="math inline">\(||\boldsymbol{g}^*(\boldsymbol{\theta})|| \to \infty\)</span> whenever <span class="math inline">\(|| \boldsymbol{\theta} || \to \infty\)</span>;</li>
<li>For every <span class="math inline">\(s \in \mathbb{R}^p\)</span> the equation <span class="math inline">\(\boldsymbol{g}^*(\boldsymbol{\theta}) = s\)</span> has countably many (possibly zero) solutions in <span class="math inline">\(\boldsymbol{\Theta}\)</span>;</li>
</ul>
then <span class="math inline">\(\boldsymbol{g}^*(\boldsymbol{\theta})\)</span> is a homeomorphism (i.e. <span class="math inline">\(\boldsymbol{g}^*(\boldsymbol{\theta})\)</span> is continuous and one-to-one).
</div>
<p></p>
<ul>
<li><p>A direct consequence of Lemma <a href="properties-of-statistical-estimators.html#lem:GMMindentification">3.1</a> and Theorem <a href="properties-of-statistical-estimators.html#thm:Homomorphism">3.4</a> is that any GMM estimator with <span class="math inline">\(\boldsymbol{W} &gt; 0\)</span> satisfying the conditions of Theorem <a href="properties-of-statistical-estimators.html#thm:Homomorphism">3.4</a> verifies Condition C1 of Theorem <a href="properties-of-statistical-estimators.html#thm:ConsExtreEst">3.3</a>.</p></li>
<li><p>If one can show that <span class="math inline">\(\boldsymbol{g}^c(\boldsymbol{\theta})\)</span> is in <span class="math inline">\(\mathcal{C}\)</span> (where <span class="math inline">\(\boldsymbol{g}^c(\boldsymbol{\theta})\)</span> denotes the element of <span class="math inline">\(\boldsymbol{g}_0(\boldsymbol{\theta})\)</span> that are not in <span class="math inline">\(\boldsymbol{g}^*(\boldsymbol{\theta})\)</span>) then Condition C3 of Theorem <a href="properties-of-statistical-estimators.html#thm:ConsExtreEst">3.3</a> is also verified.</p></li>
<li><p>When considering a GMM estimator of the form used in Example <a href="properties-of-statistical-estimators.html#exm:ExtreEstGMM">3.3</a>, one can simply verify the conditions of Theorem <a href="properties-of-statistical-estimators.html#thm:ConsExtreEst">3.3</a> with <span class="math inline">\(\boldsymbol{g}(\boldsymbol{\theta}) = \boldsymbol{\gamma} (\boldsymbol{\theta})\)</span>.</p></li>
<li><p>For Theorem <a href="properties-of-statistical-estimators.html#thm:Homomorphism">3.4</a>, in <span class="citation">Komunjer (<a href="#ref-komunjer2012global">2012</a>)</span> it is actually assumed that <span class="math inline">\(\boldsymbol{\theta} \in \mathbb{R}^p\)</span> while we assume that <span class="math inline">\(\boldsymbol{\theta} \in \boldsymbol{\Theta}\)</span>. We use this simplification to avoid an overly technical treatment of this topic. In fact, we assume here that there exists a one-to-one function <span class="math inline">\(h(\cdot)\)</span> such that <span class="math inline">\(h \, : \,\mathbb{R}^p \mapsto \boldsymbol{\Theta}\)</span>. This condition is typically verified in practice.</p></li>
</ul>

<div class="example">
<p><span id="exm:exmC1" class="example"><strong>Example 3.5  (An Example to Verify Condition C1)  </strong></span>In this example we revisit Example <a href="properties-of-statistical-estimators.html#exm:ExtreEstGMM">3.3</a>. Let us say that <span class="math inline">\(\widehat{\boldsymbol{W}}\)</span> is such that <span class="math inline">\(\widehat{\boldsymbol{W}} \overset{\mathcal{P}}{\mapsto} \boldsymbol{W} &gt; 0\)</span>.</p>
We have shown that
<span class="math display">\[\begin{equation*}
        \boldsymbol{\gamma}(\boldsymbol{\theta}) =  \begin{bmatrix}
             \mu\\
            \mu^2 + \sigma^2\\
            \mu^3 + 3 \mu \sigma^2
             \end{bmatrix}.
\end{equation*}\]</span>
So we define that
<span class="math display">\[\begin{equation*}
    \boldsymbol{g}^*(\boldsymbol{\theta}) = \begin{bmatrix}
             \mu\\
            \mu^2 + \sigma^2\\
             \end{bmatrix}
             \;\;\;\;\; \text{and} \;\;\;\;\; g^c(\boldsymbol{\theta}) = \begin{bmatrix}
             \mu^3 + 3 \mu \sigma^2\\
             \end{bmatrix}.
\end{equation*}\]</span>
<p>Since the elements of <span class="math inline">\(\boldsymbol{g}^*(\boldsymbol{\theta})\)</span> are polynomial in <span class="math inline">\(\boldsymbol{\Theta}\)</span>, the condition <span class="math inline">\(\boldsymbol{g}^*(\boldsymbol{\theta}) \in \mathcal{C}^2\)</span> is trivially satisfied.</p>
Next, since
<span class="math display">\[\begin{equation*}
        \frac{\partial}{\partial \boldsymbol{\theta}^T}\, \boldsymbol{g}^*(\boldsymbol{\theta}) =   \begin{bmatrix}
             1 &amp; 0\\
            2\mu &amp; 1
             \end{bmatrix},
\end{equation*}\]</span>
<p>we have <span class="math inline">\(J(\boldsymbol{\theta}) \equiv \text{det} (\frac{\partial}{\partial \boldsymbol{\theta}^T}\, \boldsymbol{g}^*(\boldsymbol{\theta}) ) = 1\)</span>.</p>
<p>Finally, the last two conditions of Theorem <a href="properties-of-statistical-estimators.html#thm:Homomorphism">3.4</a> are trivially satisfied since <span class="math inline">\(||\boldsymbol{g}^*(\boldsymbol{\theta})||\)</span> can only diverge if <span class="math inline">\(|| \boldsymbol{\theta} ||\)</span> diverges and since <span class="math inline">\(\boldsymbol{g}^*(\boldsymbol{\theta}) = s\)</span> has (one) countably solutions in <span class="math inline">\(\boldsymbol{\Theta}\)</span>.</p>
Therefore, it follows from Lemma <a href="properties-of-statistical-estimators.html#lem:GMMindentification">3.1</a> and Theorem <a href="properties-of-statistical-estimators.html#thm:Homomorphism">3.4</a> that the function <span class="math inline">\(Q_0(\boldsymbol{\theta}) = - || \boldsymbol{\gamma}(\boldsymbol{\theta}_0) - \boldsymbol{\gamma}(\boldsymbol{\theta}) ||^2_{\boldsymbol{W}}\)</span> is uniquely maximized in <span class="math inline">\(\boldsymbol{\theta}_0\)</span>.
</div>
<p></p>
</div>
<div id="verification-of-condition-c4" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Verification of Condition C4</h3>
<p>In general, the verification of Condition C4 requires pointwise convergence and Lipschitz continuity as specified in the following theorem, which is a slightly adapted version of the Arzela-Ascoli Theorem.</p>

<div class="theorem">
<p><span id="thm:ArzelaAscoli" class="theorem"><strong>Theorem 3.5  (modified Arzela-Ascoli Theorem)  </strong></span>Suppose <span class="math inline">\(\boldsymbol{\Theta}\)</span> is compact. If</p>
<ul>
<li><p>for every <span class="math inline">\(\boldsymbol{\theta} \in \boldsymbol{\Theta}\)</span>, we have <span class="math inline">\(\hat{Q}_n (\boldsymbol{\theta}) \overset{\mathcal{P}}{\mapsto} Q_0 (\boldsymbol{\theta})\)</span>,</p></li>
<li><p><span class="math inline">\(\hat{Q}_n (\boldsymbol{\theta})\)</span> is almost surely Lipschitz continuous, i.e. <span class="math inline">\(\underset{\boldsymbol{\theta}_1, \boldsymbol{\theta}_2 \in \boldsymbol{\Theta}}{\text{sup}} |\hat{Q}_n (\boldsymbol{\theta}_1) - \hat{Q}_n (\boldsymbol{\theta}_2)| \leq H ||\boldsymbol{\theta}_1 - \boldsymbol{\theta}_2||\)</span> where <span class="math inline">\(H\)</span> is bounded almost surely,</p></li>
</ul>
then <span class="math inline">\(\hat{Q}_n (\boldsymbol{\theta})\)</span> converges uniformly in probability to <span class="math inline">\(Q_0 (\boldsymbol{\theta})\)</span>.
</div>
<p></p>
<p>Here we will not discuss how to prove that <span class="math inline">\(\hat{Q}_n (\boldsymbol{\theta})\)</span> is almost surely Lipschitz continuous and assume it for simplicity (more discussion on this topic can for example be found in <span class="citation">Newey and McFadden (<a href="#ref-newey1994vlarge">1994</a>)</span>). Nevertheless, it worth mentioning that this condition is almost always satisfied in practice and is therefore reasonable to assume for simplicity.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-newey1994vlarge">
<p>Newey, W. K., and D. McFadden. 1994. “Large Sample Estimation and Hypothesis Testing, V in Handbook of Econometrics.” In. Vol. 4. Elsevier, Amsterdam.</p>
</div>
<div id="ref-komunjer2012global">
<p>Komunjer, I. 2012. “Global Identification in Nonlinear Models with Moment Restrictions.” <em>Econometric Theory</em> 28 (4): 719.</p>
</div>
<div id="ref-baltagi2008companion">
<p>Baltagi, Badi H. 2008. <em>A Companion to Theoretical Econometrics</em>. John Wiley &amp; Sons.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="timeseries.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="allan-variance-calibration-techniques.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/02-estimators.Rmd",
"text": "Edit"
},
"download": ["scis.pdf", "scis.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
