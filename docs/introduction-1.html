<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>An Introduction to Inertial Sensors Stochastic Calibration</title>
  <meta name="description" content="TO DO">
  <meta name="generator" content="bookdown 0.7.10 and GitBook 2.6.7">

  <meta property="og:title" content="An Introduction to Inertial Sensors Stochastic Calibration" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="TO DO" />
  <meta name="github-repo" content="SMAC-Group/SCIS" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="An Introduction to Inertial Sensors Stochastic Calibration" />
  
  <meta name="twitter:description" content="TO DO" />
  

<meta name="author" content="StÃ©phane Guerrier, Roberto Molinari, Yuming Zhang, Haotian Xu, Gaetan Bakalli, Ahmed Radi and Mucyo Karemera">


<meta name="date" content="2018-07-17">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="references.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="timeseries.html"><a href="timeseries.html"><i class="fa fa-check"></i><b>2</b> Introduction to Time Series Analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="timeseries.html"><a href="timeseries.html#time-series"><i class="fa fa-check"></i><b>2.1</b> Time Series</a></li>
<li class="chapter" data-level="2.2" data-path="timeseries.html"><a href="timeseries.html#dependence-within-time-series"><i class="fa fa-check"></i><b>2.2</b> Dependence within Time Series</a></li>
<li class="chapter" data-level="2.3" data-path="timeseries.html"><a href="timeseries.html#stationarity"><i class="fa fa-check"></i><b>2.3</b> Stationarity</a></li>
<li class="chapter" data-level="2.4" data-path="timeseries.html"><a href="timeseries.html#linear-processes"><i class="fa fa-check"></i><b>2.4</b> Linear Processes</a></li>
<li class="chapter" data-level="2.5" data-path="timeseries.html"><a href="timeseries.html#basic-time-series-models"><i class="fa fa-check"></i><b>2.5</b> Basic Time Series Models</a></li>
<li class="chapter" data-level="2.6" data-path="timeseries.html"><a href="timeseries.html#fundamental-representations-of-time-series"><i class="fa fa-check"></i><b>2.6</b> Fundamental Representations of Time Series</a></li>
<li class="chapter" data-level="2.7" data-path="timeseries.html"><a href="timeseries.html#estimation-problems-with-time-series"><i class="fa fa-check"></i><b>2.7</b> Estimation Problems with Time Series</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html"><i class="fa fa-check"></i><b>3</b> Properties of Statistical Estimators</a><ul>
<li class="chapter" data-level="3.1" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#extremum-estimators"><i class="fa fa-check"></i><b>3.1</b> Extremum Estimators</a></li>
<li class="chapter" data-level="3.2" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#consistency"><i class="fa fa-check"></i><b>3.2</b> Consistency</a><ul>
<li class="chapter" data-level="3.2.1" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#consistency-of-extremum-estimators"><i class="fa fa-check"></i><b>3.2.1</b> Consistency of Extremum Estimators</a></li>
<li class="chapter" data-level="3.2.2" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#verification-of-condition-c1"><i class="fa fa-check"></i><b>3.2.2</b> Verification of Condition C1</a></li>
<li class="chapter" data-level="3.2.3" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#verification-of-condition-c4"><i class="fa fa-check"></i><b>3.2.3</b> Verification of Condition C4</a></li>
<li class="chapter" data-level="3.2.4" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#consistency-of-sample-autocovariance-and-autocorrelation-functions"><i class="fa fa-check"></i><b>3.2.4</b> Consistency of Sample AutoCovariance and AutoCorrelation Functions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#asymptotic-normality"><i class="fa fa-check"></i><b>3.3</b> Asymptotic Normality</a><ul>
<li class="chapter" data-level="3.3.1" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#clt-for-iid-random-variables"><i class="fa fa-check"></i><b>3.3.1</b> CLT for iid Random Variables</a></li>
<li class="chapter" data-level="3.3.2" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#clt-for-dependent-processes"><i class="fa fa-check"></i><b>3.3.2</b> CLT for Dependent Processes</a></li>
<li class="chapter" data-level="3.3.3" data-path="properties-of-statistical-estimators.html"><a href="properties-of-statistical-estimators.html#asymptotic-normality-of-sample-autocovariance-and-autocorrelation-functions"><i class="fa fa-check"></i><b>3.3.3</b> Asymptotic Normality of Sample AutoCovariance and AutoCorrelation Functions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html"><i class="fa fa-check"></i><b>4</b> Allan Variance Calibration Techniques</a><ul>
<li class="chapter" data-level="4.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#spectral-ambiguity-of-the-av"><i class="fa fa-check"></i><b>4.1</b> Spectral Ambiguity of the AV</a></li>
<li class="chapter" data-level="4.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#properties-of-the-allan-variance"><i class="fa fa-check"></i><b>4.2</b> Properties of the Allan Variance</a></li>
<li class="chapter" data-level="4.3" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#estimation"><i class="fa fa-check"></i><b>4.3</b> Estimation</a><ul>
<li class="chapter" data-level="4.3.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#consistency-1"><i class="fa fa-check"></i><b>4.3.1</b> Consistency</a></li>
<li class="chapter" data-level="4.3.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#asymptotic-normality-1"><i class="fa fa-check"></i><b>4.3.2</b> Asymptotic Normality</a></li>
<li class="chapter" data-level="4.3.3" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#confidence-interval-of-the-moav-estimator"><i class="fa fa-check"></i><b>4.3.3</b> Confidence Interval of the MOAV Estimator</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#allan-variance-based-estimation"><i class="fa fa-check"></i><b>4.4</b> Allan variance based estimation</a><ul>
<li class="chapter" data-level="4.4.1" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#allan-variance-log-log-representation"><i class="fa fa-check"></i><b>4.4.1</b> Allan Variance log-log Representation</a></li>
<li class="chapter" data-level="4.4.2" data-path="allan-variance-calibration-techniques.html"><a href="allan-variance-calibration-techniques.html#allan-deviation-of-a-wn-process"><i class="fa fa-check"></i><b>4.4.2</b> Allan Deviation of a WN process</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-generalized-method-of-wavelet-moments.html"><a href="the-generalized-method-of-wavelet-moments.html"><i class="fa fa-check"></i><b>5</b> The Generalized Method of Wavelet Moments</a></li>
<li class="chapter" data-level="6" data-path="extensions.html"><a href="extensions.html"><i class="fa fa-check"></i><b>6</b> Extensions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="7" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>7</b> Introduction</a><ul>
<li class="chapter" data-level="7.1" data-path="introduction-1.html"><a href="introduction-1.html#time-series-1"><i class="fa fa-check"></i><b>7.1</b> Time Series</a></li>
<li class="chapter" data-level="7.2" data-path="introduction-1.html"><a href="introduction-1.html#eda"><i class="fa fa-check"></i><b>7.2</b> Exploratory Data Analysis for Time Series</a></li>
<li class="chapter" data-level="7.3" data-path="introduction-1.html"><a href="introduction-1.html#basicmodels"><i class="fa fa-check"></i><b>7.3</b> Basic Time Series Models</a><ul>
<li class="chapter" data-level="7.3.1" data-path="introduction-1.html"><a href="introduction-1.html#wn"><i class="fa fa-check"></i><b>7.3.1</b> White noise processes</a></li>
<li class="chapter" data-level="7.3.2" data-path="introduction-1.html"><a href="introduction-1.html#rw"><i class="fa fa-check"></i><b>7.3.2</b> Random Walk Processes</a></li>
<li class="chapter" data-level="7.3.3" data-path="introduction-1.html"><a href="introduction-1.html#ar1"><i class="fa fa-check"></i><b>7.3.3</b> Autoregressive Process of Order 1</a></li>
<li class="chapter" data-level="7.3.4" data-path="introduction-1.html"><a href="introduction-1.html#ma1"><i class="fa fa-check"></i><b>7.3.4</b> Moving Average Process of Order 1</a></li>
<li class="chapter" data-level="7.3.5" data-path="introduction-1.html"><a href="introduction-1.html#drift"><i class="fa fa-check"></i><b>7.3.5</b> Linear Drift</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="introduction-1.html"><a href="introduction-1.html#lts"><i class="fa fa-check"></i><b>7.4</b> Composite Stochastic Processes</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Inertial Sensors Stochastic Calibration</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction-1" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Introduction</h1>
<!--
> "One damned thing after another." ~ R. A. Fisher
>
-->
<blockquote>
<p>â<em>PrÃ©voir consiste Ã  projeter dans lâavenir ce quâon a perÃ§u dans le passÃ©.</em>â â Henri Bergson</p>
</blockquote>
<!--
>
> *HÃ¢tons-nous; le temps fuit, et nous entraÃ®ne avec soi : le moment oÃ¹ je parle est dÃ©jÃ  loin de moi*. Nicolas Boileau
>
-->
<p>After reading this chapter you will be able to:</p>
<ul>
<li>Describe what a <em>time series</em> is.</li>
<li>Perform exploratory data analysis on time series data.</li>
<li>Evaluate different characteristics of a time series.</li>
<li>Classify basic time series models through equations and plots.</li>
<li>Manipulate a time series equation using <em>backsubstitution</em>.</li>
</ul>
<div id="time-series-1" class="section level2">
<h2><span class="header-section-number">7.1</span> Time Series</h2>
<p>Generally speaking a <em>time series</em> (or stochastic process) corresponds to set of
ârepeatedâ observations of the same variable such as price of a financial asset
or temperature in a given location. In terms of notation a time series is often
written as</p>
<p><span class="math display">\[\left(X_1, X_2, ..., X_n \right) \;\;\; \text{ or } \;\;\; \left(X_t\right)_{t = 1,...,n}.\]</span></p>
<p>The time index <span class="math inline">\(t\)</span> is contained within either the set of reals, <span class="math inline">\(\mathbb{R}\)</span>, or
integers, <span class="math inline">\(\mathbb{N}\)</span>. When <span class="math inline">\(t \in \mathbb{R}\)</span>, the time series becomes a
<em>continuous-time</em> stochastic process such a Brownian motion, a model used to
represent the random movement of particles within a suspended liquid or gas,
or an ElectroCardioGram (ECG) signal, which corresponds to the palpitations of
the heart. However, within this text, we will limit ourselves to the cases where
<span class="math inline">\(t \in \mathbb{N}\)</span>, better known as <em>discrete-time</em> processes.
<em>Discrete-time</em> processes are where a variable is measured sequentially at fixed
and equally spaced intervals in time.
This implies that we will have two assumptions:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(t\)</span> is not random, e.g.Â the time at which each observation is measured is known, and</li>
<li>the time between two consecutive observations is constant.</li>
</ol>
<p>Moreover, the term âtime seriesâ can also represent a probability model for a
set of observations. For example, one of the fundamental probability models used
in time series analysis is called a <em>white noise</em> process and is defined as</p>
<p><span class="math display">\[W_t \mathop \sim \limits^{iid} N(0, \sigma^2).\]</span></p>
<p>This statement simply means that <span class="math inline">\((W_t)\)</span> is normally distributed and independent
over time. This model may appear to be dull but we will soon see it as a crucial
component to constructing more complex models. Unlike the white noise process,
time series are typically <em>not</em> independent over time. Suppose that the
temperature in Champaign is unusually low, then it is reasonable to assume
that tomorrowâs temperature will also be low. Indeed, such behavior would
suggest the existence of a dependency over time. The time series methods we will
discuss in this text consists of parametric models used to characterize
(or at least approximate) the joint distribution of <span class="math inline">\((X_t)\)</span>. Often, time series
models can be decomposed into two components, the first of which is what we call
a <em>signal</em>, say <span class="math inline">\((Y_t)\)</span>, and the second component is a <em>noise</em>, say <span class="math inline">\((W_t)\)</span>,
leading to the model</p>
<p><span class="math display">\[X_t = Y_t + W_t.\]</span></p>
<p>Typically, we have <span class="math inline">\(\mathbb{E}[Y_t] \neq 0\)</span> while <span class="math inline">\(\mathbb{E}[W_t] = 0\)</span> (although we may have
<span class="math inline">\(\mathbb{E}[W_t | W_{t-1}, ..., W_1] \neq 0\)</span>). Such models impose some parametric
structure which represents a convenient and flexible way of studying time series
as well as a means to evaluate <em>future</em> values of the series through forecasting.
As we will see, predicting future values is one of the main aspects of time
series analysis. However, making predictions is often a daunting task or as
famously stated by Nils Bohr:</p>
<blockquote>
<p>â<em>Prediction is very difficult, especially about the future.</em>â</p>
</blockquote>
<p>There are plenty of examples of predictions that turned out to be completely
erroneous. For example, three days before the 1929 crash, Irving Fisher,
Professor of Economics at Yale University, famously predicted:</p>
<blockquote>
<p>â<em>Stock prices have reached what looks like a permanently high plateau</em>â.</p>
</blockquote>
<p>Another example is given by Thomas Watson, president of IBM, who said in 1943:</p>
<blockquote>
<p>â<em>I think there is a world market for maybe five computers.</em>â</p>
</blockquote>
</div>
<div id="eda" class="section level2">
<h2><span class="header-section-number">7.2</span> Exploratory Data Analysis for Time Series</h2>
<p>When dealing with relatively small time series (e.g.Â a few thousands), it is
often useful to look at a graph of the original data. A graph can be
an informative tool for âdetectingâ some features of a time series such as trends and
the presence of outliers.</p>
<p>Indeed, a trend is typically assumed to be present in a time series when the
data exhibit some form of long term increase or decrease or combination of
increases or decreases. Such trends could be linear or non-linear and represent
an important part of the âsignalâ of a model. Here are a few examples of
non-linear trends:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Seasonal trends</strong> (periodic): These are the cyclical patterns which repeat
after a fixed/regular time period. This could be due to business cycles
(e.g.Â bust/recession, recovery).</p></li>
<li><p><strong>Non-seasonal trends</strong> (periodic): These patterns cannot be associated to
seasonal variation and can for example be due to an external variable such as,
for example, the impact of economic indicators on stock returns. Note that such
trends are often hard to detect based on a graphical analysis of the data.</p></li>
<li><p><strong>âOtherâ trends</strong>: These trends have typically no regular patterns and are
over a segment of time, known as a âwindowâ, that change the statistical
properties of a time series. A common example of such trends is given by the
vibrations observed before, during and after an earthquake.</p></li>
</ol>

<div class="example">
<span id="exm:jjquarterly" class="example"><strong>Example 7.1  </strong></span>A traditional example of a time series is the quarterly earnings of
the company Johnson and Johson. In the figure below, we present these earnings
between 1960 and 1980.
</div>

<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load data</span>
<span class="kw">data</span>(jj, <span class="dt">package =</span> <span class="st">&quot;astsa&quot;</span>)

<span class="co"># Construct gts object</span>
jj =<span class="st"> </span><span class="kw">gts</span>(jj, <span class="dt">start =</span> <span class="dv">1960</span>, <span class="dt">freq =</span> <span class="dv">4</span>, <span class="dt">unit_ts =</span> <span class="st">&quot;$&quot;</span>, <span class="dt">name_ts =</span> <span class="st">&quot;Earnings&quot;</span>, 
         <span class="dt">data_name =</span> <span class="st">&quot;Johnson and Johnson Quarterly Earnings&quot;</span>)

<span class="co"># Plot time series</span>
<span class="kw">plot</span>(jj)</code></pre>
<p><img src="scis_files/figure-html/example_JJ-1.png" width="672" /></p>
<p>One trait that the graph makes evident is that the data contains a non-linear
increasing trend as well as a yearly seasonal component. In addition, one can
note that the <em>variability</em> of the data seems to increase with time. Being able
to make such observations provides important information to select suitable
models for the data.</p>
<p>Moreover, when observing ârawâ time series data it is also interesting to
evaluate if some of the following phenomena occur:</p>
<ol style="list-style-type: decimal">
<li><strong>Change in Mean:</strong> Does the mean of the process shift over time?</li>
<li><strong>Change in Variance:</strong> Does the variance of the process evolve with time?</li>
<li><strong>Change in State:</strong> Does the time series appear to change between âstatesâ
having distinct statistical properties?</li>
<li><strong>Outliers</strong> Does the time series contain some âextremeâ observations?
(Note that this is typically difficult to assess visually.)</li>
</ol>

<div class="example">
<span id="exm:earthquake" class="example"><strong>Example 7.2  </strong></span>In the figure below, we present an example of displacement recorded
during an earthquake as well as an explosion.
</div>

<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># TO DO!</span></code></pre>
<p>From the graph, it can be observed that the statistical properties of the time
series appear to change over time. For instance, the variance of the time series
shifts at around <span class="math inline">\(t = 1150\)</span> for both series.
The shift in variance also opens âwindowsâ where there appear to be distinct
states. In the case of the explosion data, this is particularly relevant around
<span class="math inline">\(t = 50, \cdots, 250\)</span> and then again from <span class="math inline">\(t = 1200, \cdots, 1500\)</span>. Even within
these windows, there are âspikesâ that could be considered as outliers most
notably around <span class="math inline">\(t = 1200\)</span> in the explosion series.</p>
<p>Extreme observations or outliers are commonly observed in real time series data, this is illustrated in the following example.</p>

<div class="example">
<span id="exm:percipitation" class="example"><strong>Example 7.3  </strong></span>We consider here a data set coming from the domain of hydrology. The data
concerns monthly precipitation (in mm) over a certain period of time (1907 to
1972) and is interesting for scientists in order to study water cycles. The
data are presented in the graph below:
</div>

<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load hydro dataset</span>
<span class="kw">data</span>(<span class="st">&quot;hydro&quot;</span>)

<span class="co"># Simulate based on data</span>
hydro =<span class="st"> </span><span class="kw">gts</span>(<span class="kw">as.vector</span>(hydro), <span class="dt">start =</span> <span class="dv">1907</span>, <span class="dt">freq =</span> <span class="dv">12</span>, <span class="dt">unit_ts =</span> <span class="st">&quot;in.&quot;</span>, 
            <span class="dt">name_ts =</span> <span class="st">&quot;Precipitation&quot;</span>, <span class="dt">data_name =</span> <span class="st">&quot;Hydrology data&quot;</span>)

<span class="co"># Plot hydro </span>
<span class="kw">plot</span>(hydro)</code></pre>
<p><img src="scis_files/figure-html/example_hydro-1.png" width="672" /></p>
<p>Next, we consider an example coming from high-frequency finance to illustrate
the limitations our current framework.</p>

<div class="example">
<span id="exm:stocks-starbucks" class="example"><strong>Example 7.4  </strong></span>The figure below presents the returns or price innovations
(i.e.Â the changes in price from one observation to the
next) for Starbuckâs stock on July 1, 2011 for about 150 seconds (left
panel) and about 400 minutes (right panel).
</div>

<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># TO DO add Duke&#39;s code here!</span></code></pre>
<p>It can be observed on the left panel that observations are not equally spaced.
Indeed, in high-frequency data the intervals between two points are typically not
constant and are, even worse, random variables. This implies that the time when
a new observation will be available is in general unknown. On the right panel,
one can observe that the variability of the data seems to change during the
course of the trading day. Such a phenomenon is well known in the finance
community since a lot of variation occurs at the start (and the end) of the
day while the middle of the day is associated with small changes.
Moreover, clear extreme observations can also be noted in this graph at
around 11:00.</p>
<p>Finally, let us consider the limitations of a direct graphical representation of
a time series when the sample size is large. Indeed, due to visual limitations,
a direct plotting of the data will probably result in an uninformative
aggregation of points between which it is unable to distinguish anything. This is
illustrated in the following example.</p>

<div class="example">
<span id="exm:large-imu" class="example"><strong>Example 7.5  </strong></span>We consider here the data coming from the calibration procedure of
an Inertial Measurement Unit (IMU) which, in general terms, is used to enhance
navigation precision or reconstruct three dimensional movements (see
e.g. <a href="https://www.youtube.com/watch?v=htoBvSq8jLA">link</a>). These sensors are
used in a very wide range of applications such as robotics, virtual reality,
vehicle stability control, human and animal motion capture and so forth (see e.g. <a href="https://www.youtube.com/watch?v=g4tgtPA54_Y">link</a>).
The signals coming from these instruments are measured at
high frequencies over a long time and are often characterized by linear trends
and numerous underlying stochastic processes.
</div>

<p>The code below retrieves some data from an IMU and plots it directly:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load IMU data</span>
<span class="kw">data</span>(imu6, <span class="dt">package =</span> <span class="st">&quot;imudata&quot;</span>)

<span class="co"># Construct gst object</span>
Xt =<span class="st"> </span><span class="kw">gts</span>(imu6[,<span class="dv">1</span>], <span class="dt">data_name =</span> <span class="st">&quot;Gyroscope data&quot;</span>, <span class="dt">unit_time =</span> <span class="st">&quot;hour&quot;</span>, 
         <span class="dt">freq =</span> <span class="dv">100</span><span class="op">*</span><span class="dv">60</span><span class="op">*</span><span class="dv">60</span>, <span class="dt">name_ts =</span> <span class="st">&quot;Angular rate&quot;</span>, 
         <span class="dt">unit_ts =</span> <span class="kw">bquote</span>(rad<span class="op">^</span><span class="dv">2</span><span class="op">/</span>s<span class="op">^</span><span class="dv">2</span>))

<span class="co"># Plot time series</span>
<span class="kw">plot</span>(Xt)</code></pre>
<p><img src="scis_files/figure-html/example_IMU-1.png" width="672" /></p>
<p>Although a linear trend and other processes are present in this signal
(time series), it is practically impossible to understand or guess anything
from the plot.</p>
</div>
<div id="basicmodels" class="section level2">
<h2><span class="header-section-number">7.3</span> Basic Time Series Models</h2>
<p>In this section, we introduce some simple time series models. Before doing so
it is useful to define <span class="math inline">\(\Omega_t\)</span> as all the information avaiable up to time
<span class="math inline">\(t-1\)</span>, i.e.</p>
<p><span class="math display">\[\Omega_t = \left(X_{t-1}, X_{t-2}, ..., X_0 \right).\]</span></p>
<p>As we will see this compact notation is quite useful.</p>
<div id="wn" class="section level3">
<h3><span class="header-section-number">7.3.1</span> White noise processes</h3>
<p>The building block for most time series models is the Gaussian white noise
process, which can be defined as</p>
<p><span class="math display">\[{W_t}\mathop \sim \limits^{iid} N\left( {0,\sigma _w^2} \right).\]</span></p>
<p>This definition implies that:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\mathbb{E}[W_t | \Omega_t] = 0\)</span> for all <span class="math inline">\(t\)</span>,</li>
<li><span class="math inline">\(\cov\left(W_t, W_{t-h} \right) = \boldsymbol{1}_{h = 0} \; \sigma^2\)</span> for
all <span class="math inline">\(t, h\)</span>.</li>
</ol>
<p>Therefore, in this process there is an absence of temporal (or serial)
dependence and is homoskedastic (i.e.Â it has a constant variance).
White noise can be generalized into two sorts of processes: <em>weak</em> and <em>strong</em>. The process <span class="math inline">\((W_t)\)</span> is
a weak white noise if</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\mathbb{E}[W_t] = 0\)</span> for all <span class="math inline">\(t\)</span>,</li>
<li><span class="math inline">\(\var\left(W_t\right) = \sigma^2\)</span> for all <span class="math inline">\(t\)</span>,</li>
<li><span class="math inline">\(\cov \left(W_t, W_{t-h}\right) = 0\)</span>, for all <span class="math inline">\(t\)</span>, and for all <span class="math inline">\(h \neq 0\)</span>.</li>
</ol>
<p>Note that this definition does not imply that <span class="math inline">\(W_t\)</span> and <span class="math inline">\(W_{t-h}\)</span> are
independent (for <span class="math inline">\(h \neq 0\)</span>) but simply uncorrelated.
However, the notion of independence is used to define a <em>strong</em> white noise as</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\mathbb{E}[W_t] = 0\)</span> and <span class="math inline">\(\var(W_t) = \sigma^2 &lt; \infty\)</span>, for all <span class="math inline">\(t\)</span>,</li>
<li><span class="math inline">\(F(W_t) = F(W_{t-h})\)</span>, for all <span class="math inline">\(t,h\)</span> (where <span class="math inline">\(F(W_t)\)</span> denotes the distribution of <span class="math inline">\(W_t\)</span>),</li>
<li><span class="math inline">\(W_t\)</span> and <span class="math inline">\(W_{t-h}\)</span> are independent for all <span class="math inline">\(t\)</span> and for all <span class="math inline">\(h \neq 0\)</span>.</li>
</ol>
<p>It is clear from these definitions that if a process is a strong white noise
it is also a weak white noise. However, the converse is not true as shown in
the following example:</p>

<div class="example">
<p><span id="exm:weaknotstrong" class="example"><strong>Example 7.6  </strong></span>
Let <span class="math inline">\(Y_t \mathop \sim F_{t+2}\)</span>, where <span class="math inline">\(F_{t+2}\)</span> denotes
a Student distribution with <span class="math inline">\(t+2\)</span> degrees of freedom. Assuming the
sequence <span class="math inline">\((Y_1, \ldots, Y_n)\)</span> to be independent, we
let <span class="math inline">\(X_t = \sqrt{\frac{t}{t+2}} Y_t\)</span>. Then, the process <span class="math inline">\((X_t)\)</span> is obviously
not a strong white noise as the distribution of <span class="math inline">\(X_t\)</span> changes with <span class="math inline">\(t\)</span>. However,
this process is a weak white noise since we have:</p>
<ul>
<li><span class="math inline">\(\mathbb{E}[X_t] = \sqrt{\frac{t}{t+2}} \mathbb{E}[Y_t] = 0\)</span> for all <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(\var(X_t) = \frac{t}{t+2} \var(Y_t) = \frac{t}{t+2} \frac{t+2}{t} = 1\)</span> for all <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(\cov(X_t, X_{t+h}) = 0\)</span> (by independence), for all <span class="math inline">\(t\)</span>, and for all <span class="math inline">\(h \neq 0\)</span>.</li>
</ul>
</div>

<p>The code below presents an example of how to simulate a Gaussian white noise process.</p>
<pre class="sourceCode r"><code class="sourceCode r">n =<span class="st"> </span><span class="dv">1000</span>                               <span class="co"># process length</span>
sigma2 =<span class="st"> </span><span class="dv">1</span>                             <span class="co"># process variance</span>
Xt =<span class="st"> </span><span class="kw">gen_gts</span>(n, <span class="kw">WN</span>(<span class="dt">sigma2 =</span> sigma2))
<span class="kw">plot</span>(Xt)</code></pre>
<p><img src="scis_files/figure-html/example_WN-1.png" width="672" /></p>
</div>
<div id="rw" class="section level3">
<h3><span class="header-section-number">7.3.2</span> Random Walk Processes</h3>
<p>The term <em>random walk</em> was first introduced by Karl Pearson in the early
nineteen-hundreds. Regarding white noise, there exist a large range of random walk
processes. For example, one of the simplest forms of a random walk process can be
explained as follows: suppose that you are walking on campus and your
next step can either be to your left, your right, forward or backward
(each with equal probability). Two realizations of such processes are
represented below:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">5</span>)
<span class="kw">RW2dimension</span>(<span class="dt">steps =</span> <span class="dv">10</span><span class="op">^</span><span class="dv">2</span>)</code></pre>
<p><img src="scis_files/figure-html/RW2d-1.png" width="528" style="display: block; margin: auto;" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">RW2dimension</span>(<span class="dt">steps =</span> <span class="dv">10</span><span class="op">^</span><span class="dv">4</span>)</code></pre>
<p><img src="scis_files/figure-html/RW2d-2.png" width="528" style="display: block; margin: auto;" /></p>
<p>Such processes inspired Karl Pearsonâs famous quote that</p>
<blockquote>
<p>â<em>the most likely place to find a drunken walker is somewhere near his starting point.</em>â</p>
</blockquote>
<p>Empirical evidence of this phenomenon is not too hard to find on a Friday night. In this text, we only consider one very specific form of
random walk, namely the Gaussian random walk which can be defined as:</p>
<p><span class="math display">\[X_t = X_{t-1} + W_t,\]</span></p>
<p>where <span class="math inline">\(W_t\)</span> is a Gaussian white noise process with initial condition <span class="math inline">\(X_0 = c\)</span>.
(Typically <span class="math inline">\(c = 0\)</span>.) This process can be expressed differently by
<em>backsubstitution</em> as follows:</p>
<p><span class="math display">\[\begin{aligned}
  {X_t} &amp;= {X_{t - 1}} + {W_t} \\
   &amp;= \left( {{X_{t - 2}} + {W_{t - 1}}} \right) + {W_t} \\
   &amp;= \vdots \\
  {X_t} &amp;= \sum\limits_{i = 1}^t {{W_i}} + X_0 =  \sum\limits_{i = 1}^t {{W_i}} + c \\ 
\end{aligned} \]</span></p>
<p>The code below presents an example of how to simulate a such process.</p>
<pre class="sourceCode r"><code class="sourceCode r">n =<span class="st"> </span><span class="dv">1000</span>                               <span class="co"># process length</span>
gamma2 =<span class="st"> </span><span class="dv">1</span>                             <span class="co"># innovation variance</span>
Xt =<span class="st"> </span><span class="kw">gen_gts</span>(n, <span class="kw">RW</span>(<span class="dt">gamma2 =</span> gamma2))
<span class="kw">plot</span>(Xt)</code></pre>
<p><img src="scis_files/figure-html/example_RW-1.png" width="672" /></p>
</div>
<div id="ar1" class="section level3">
<h3><span class="header-section-number">7.3.3</span> Autoregressive Process of Order 1</h3>
<p>An autoregressive process of order 1 or AR(1) is a generalization of both
the white noise and random walk processes which are both themselves special
cases of an AR(1). A (Gaussian) AR(1) process can be defined as</p>
<p><span class="math display">\[{X_t} = {\phi}{X_{t - 1}} + {W_t},\]</span></p>
<p>where <span class="math inline">\(W_t\)</span> is a Gaussian white noise. Clearly, an AR(1) with <span class="math inline">\(\phi = 0\)</span> is
a Gaussian white noise and when <span class="math inline">\(\phi = 1\)</span> the process becomes a random walk.</p>

<div class="remark">
<p> <span class="remark"><em>Remark. </em></span> We generally assume that an AR(1), as well as other time series
models, have zero mean. The reason for this assumption is only to simplfy the
notation but it is easy to consider an AR(1) process around an
arbitrary mean <span class="math inline">\(\mu\)</span>, i.e.</p>
<p><span class="math display">\[\left(X_t - \mu\right) = \phi \left(X_{t-1} - \mu \right) + W_t,\]</span></p>
<p>which is of course equivalent to</p>
<p><span class="math display">\[X_t = \left(1 - \phi \right) \mu + \phi X_{t-1} + W_t.\]</span></p>
Thus, we will generally only work with zero mean processes since adding means
is simple.
</div>


<div class="remark">
<p> <span class="remark"><em>Remark. </em></span> An AR(1) is in fact a linear combination of past realisations of
the white noise <span class="math inline">\(W_t\)</span> process. Indeed, we have</p>
<p><span class="math display">\[\begin{aligned}
 {X_t} &amp;= {\phi_t}{X_{t - 1}} + {W_t} 
   = {\phi}\left( {{\phi}{X_{t - 2}} + {W_{t - 1}}} \right) + {W_t} \\
   &amp;= \phi^2{X_{t - 2}} + {\phi}{W_{t - 1}} + {W_t} 
   = {\phi^t}{X_0} + \sum\limits_{i = 0}^{t - 1} {\phi^i{W_{t - i}}}.
\end{aligned}\]</span></p>
<p>Under the assumption of infinite past (i.e. <span class="math inline">\(t \in \mathbb{Z}\)</span>) and <span class="math inline">\(|\phi| &lt; 1\)</span>,
we obtain</p>
<p><span class="math display">\[X_t = \sum\limits_{i = 0}^{\infty} {\phi^i {W_{t - i}}},\]</span></p>
since <span class="math inline">\(\operatorname{lim}_{i \to \infty} \; {\phi^i}{X_{t-i}} = 0\)</span>.
</div>

<p>The code below presents an example of how an AR(1) can be simulated.</p>
<pre class="sourceCode r"><code class="sourceCode r">n =<span class="st"> </span><span class="dv">1000</span>                              <span class="co"># process length</span>
phi =<span class="st"> </span><span class="fl">0.5</span>                             <span class="co"># phi parameter</span>
sigma2 =<span class="st"> </span><span class="dv">1</span>                            <span class="co"># innovation variance</span>
Xt =<span class="st"> </span><span class="kw">gen_gts</span>(n, <span class="kw">AR1</span>(<span class="dt">phi =</span> phi, <span class="dt">sigma2 =</span> sigma2))
<span class="kw">plot</span>(Xt)</code></pre>
<p><img src="scis_files/figure-html/example_AR1-1.png" width="672" /></p>
</div>
<div id="ma1" class="section level3">
<h3><span class="header-section-number">7.3.4</span> Moving Average Process of Order 1</h3>
<p>As we have seen in the previous example, an AR(1) can be expressed as a linear
combination of all past observations of <span class="math inline">\((W_t)\)</span> while the next process, called
a moving average process of order 1 or MA(1), is (in some sense) a âtruncatedâ
version of an AR(1). It is defined as</p>
<span class="math display">\[\begin{equation} 
  X_t = \theta W_{t-1} + W_t,
\end{equation}\]</span>
<p>where (again) <span class="math inline">\(W_t\)</span> denotes a Gaussian white noise process. An example on how to
generate an MA(1) is given below:</p>
<pre class="sourceCode r"><code class="sourceCode r">n =<span class="st"> </span><span class="dv">1000</span>                              <span class="co"># process length</span>
sigma2 =<span class="st"> </span><span class="dv">1</span>                            <span class="co"># innovation variance</span>
theta =<span class="st"> </span><span class="fl">0.5</span>                           <span class="co"># theta parameter</span>
Xt =<span class="st"> </span><span class="kw">gen_gts</span>(n, <span class="kw">MA1</span>(<span class="dt">theta =</span> theta, <span class="dt">sigma2 =</span> sigma2))
<span class="kw">plot</span>(Xt)</code></pre>
<p><img src="scis_files/figure-html/example_MA1-1.png" width="672" /></p>
</div>
<div id="drift" class="section level3">
<h3><span class="header-section-number">7.3.5</span> Linear Drift</h3>
<p>A linear drift is a very simple deterministic time series model which can be
expressed as</p>
<p><span class="math display">\[X_t = X_{t-1} + \omega, \]</span></p>
<p>where <span class="math inline">\(\omega\)</span> is a constant and with the initial condition <span class="math inline">\(X_0 = c\)</span>, where <span class="math inline">\(c\)</span> is an
arbitrary constant (typically zero). This process can be expressed in a more
familiar form as follows:</p>
<p><span class="math display">\[
  {X_t} = {X_{t - 1}} + \omega 
   = \left( {{X_{t - 2}} + \omega} \right) + \omega 
   = t{\omega} + c  \]</span></p>
<p>Therefore, a (linear) drift corresponds to a simple linear model with slope
<span class="math inline">\(\omega\)</span> and intercept <span class="math inline">\(c\)</span>.</p>
<p>A drift can simply be generated using the code below:</p>
<pre class="sourceCode r"><code class="sourceCode r">n =<span class="st"> </span><span class="dv">100</span>                               <span class="co"># process length</span>
omega =<span class="st"> </span><span class="fl">0.5</span>                           <span class="co"># slope parameter</span>
Xt =<span class="st"> </span><span class="kw">gen_gts</span>(n, <span class="kw">DR</span>(<span class="dt">omega =</span> omega))
<span class="kw">plot</span>(Xt)</code></pre>
<p><img src="scis_files/figure-html/example_Drift-1.png" width="672" /></p>
</div>
</div>
<div id="lts" class="section level2">
<h2><span class="header-section-number">7.4</span> Composite Stochastic Processes</h2>
<p>A composite stochastic process can be defined as the sum of underlying
(or latent) stochastic processes. In this text, we will use the term
<em>latent time series</em> as a synomym for composite stochastic processes.
A simple example of such a process is given by</p>
<p><span class="math display">\[\begin{aligned}
Y_t &amp;= Y_{t-1} + W_t + \delta\\
X_t &amp;= Y_t + Z_t,
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(W_t\)</span> and <span class="math inline">\(Z_t\)</span> are two independent Gaussian white noise processes.
This model is often used as a first tool to approximate the number of
individuals in the context ecological population dynamics.
For example, suppose we want to study the population of Chamois in the Swiss Alps.
Let <span class="math inline">\(Y_t\)</span> denote the âtrueâ number of individuals in this population at time <span class="math inline">\(t\)</span>.
It is reasonable that <span class="math inline">\(Y_t\)</span> is (approximately) the population at the previous
time <span class="math inline">\(t-1\)</span> (e.g the previous year) plus a random variation and a drift.
This random variation is due to the natural randomness in ecological population
dynamics and reflects changes such as the number of predators, the abundance
of food, or weather conditions.
On the other hand, ecological <em>drift</em> is often of particular interest for ecologists as
it can be used to determine the âlongâ term trends of the population
(e.g.Â if the population is increasing, decreasing, or stable).
Of course, <span class="math inline">\(Y_t\)</span> (the number of individauls) is typically unknown and we observe
a noisy version of it, denoted as <span class="math inline">\(X_t\)</span>.
This process corresponds to the true population plus a measurement error since
some individuals may not be observed while others may have been counted several
times.
Interestingly, this process can clearly be expressed as a
<em>latent time series model</em> (or composite stochastic process) as follows:</p>
<p><span class="math display">\[\begin{aligned}
R_t &amp;= R_{t-1} + W_t \\
S_t &amp;= \delta t \\
X_t &amp;= R_t + S_t + Z_t,
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(R_t\)</span>, <span class="math inline">\(S_t\)</span> and <span class="math inline">\(Z_t\)</span> denote, respectively, a random walk,
a drift, and a white noise. The code below can be used to simulate such data:</p>
<pre class="sourceCode r"><code class="sourceCode r">n =<span class="st"> </span><span class="dv">1000</span>                                <span class="co"># process length</span>
delta =<span class="st"> </span><span class="fl">0.005</span>                           <span class="co"># delta parameter (drift)</span>
sigma2 =<span class="st"> </span><span class="dv">10</span>                             <span class="co"># variance parameter (white noise)</span>
gamma2 =<span class="st"> </span><span class="fl">0.1</span>                            <span class="co"># innovation variance (random walk)</span>
model =<span class="st"> </span><span class="kw">WN</span>(<span class="dt">sigma2 =</span> sigma2) <span class="op">+</span><span class="st"> </span><span class="kw">RW</span>(<span class="dt">gamma2 =</span> gamma2) <span class="op">+</span><span class="st"> </span><span class="kw">DR</span>(<span class="dt">omega =</span> delta)
Xt =<span class="st"> </span><span class="kw">gen_lts</span>(n, model)
<span class="kw">plot</span>(Xt)</code></pre>
<p><img src="scis_files/figure-html/example_composite-1.png" width="672" /></p>
<p>In the above graph, the first three plots represent the latent (unobserved)
processes (i.e.Â white noise, random walk, and drift) and the last one represents
the sum of the three (i.e. <span class="math inline">\((X_t)\)</span>).</p>
<p>Let us consider a real example where these latent processes are useful to
describe (and predict) the behavior of economic variables such as Personal
Saving Rates (PSR). A process that is used for these settings is the
ârandom-walk-plus-noiseâ model, meaning that the data can be explained by a
random walk process in addition to which we observe some other process (e.g.
a white noise model, an autoregressive model such as an AR(1), etc.). The PSR
taken from the Federal Reserve of St.Â Louis from January 1, 1959, to May 1,
2015, is presented in the following plot:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load savingrt dataset</span>
<span class="kw">data</span>(<span class="st">&quot;savingrt&quot;</span>)
<span class="co"># Simulate based on data</span>
savingrt =<span class="st"> </span><span class="kw">gts</span>(<span class="kw">as.vector</span>(savingrt), <span class="dt">start =</span> <span class="dv">1959</span>, <span class="dt">freq =</span> <span class="dv">12</span>, <span class="dt">unit_ts =</span> <span class="st">&quot;%&quot;</span>, 
            <span class="dt">name_ts =</span> <span class="st">&quot;Saving Rates&quot;</span>, <span class="dt">data_name =</span> <span class="st">&quot;US Personal Saving Rates&quot;</span>)
<span class="co"># Plot savingrt simulation</span>
<span class="kw">plot</span>(savingrt)</code></pre>
<p><img src="scis_files/figure-html/example_PSR-1.png" width="672" /></p>
<p>It can be observed that the mean of this process seems to vary over time,
suggesting that a random walk can indeed be considered as a possible model
to explain this data. In addition, aside from some âspikesâ and occasional
sudden changes, the observations appear to gradually change from one time point
to the other, suggesting that some other form of dependence between them could
exist.</p>

<div id="refs" class="references">
<div>
<p>Allan, D. W. 1966. âStatistics of Atomic Frequency Standards.â <em>Proceedings of the IEEE</em> 54 (2). IEEE:221â30.</p>
</div>
<div>
<p>Baltagi, Badi H. 2008. <em>A Companion to Theoretical Econometrics</em>. John Wiley &amp; Sons.</p>
</div>
<div>
<p>Efron, B., and R. J. Tibshirani. 1994. <em>An Introduction to the Bootstrap</em>. CRC press.</p>
</div>
<div>
<p>El-Sheimy, N., H. Hou, and X. Niu. 2008. âAnalysis and Modeling of Inertial Sensors using Allan Variance.â <em>IEEE Transactions on Instrumentation and Measurement</em> 57 (1). IEEE:140â49.</p>
</div>
<div>
<p>Granger, C. W. J. 1969. âInvestigating Causal Relations by Econometric Models and Cross-Spectral Methods.â <em>Econometrica: Journal of the Econometric Society</em>. JSTOR, 424â38.</p>
</div>
<div>
<p>Greenhall, C. A. 1998. âSpectral Ambiguity of Allan Variance.â <em>IEEE Transactions on Instrumentation and Measurement</em> 47 (3). IEEE:623â27.</p>
</div>
<div>
<p>Hamilton, J. D. 1994. <em>Time Series Analysis</em>. Vol. 2. Princeton university press Princeton.</p>
</div>
<div>
<p>Komunjer, I. 2012. âGlobal Identification in Nonlinear Models with Moment Restrictions.â <em>Econometric Theory</em> 28 (4):719.</p>
</div>
<div>
<p>Newey, Whitney K, and Kenneth D West. 1986. âA Simple, Positive Semi-Definite, Heteroskedasticity and Autocorrelationconsistent Covariance Matrix.â National Bureau of Economic Research Cambridge, Mass., USA.</p>
</div>
<div>
<p>Newey, W. K., and D. McFadden. 1994. âLarge Sample Estimation and Hypothesis Testing, V in Handbook of Econometrics.â In. Vol. 4. Elsevier, Amsterdam.</p>
</div>
<div>
<p>Percival, D. B., and P. Guttorp. 1994. âLong-memory processes, the Allan variance and wavelets.â In <em>Wavelet Analysis and Its Applications</em>, 4:325â44. Elsevier.</p>
</div>
<div>
<p>Percival, D. B., and A. T. Walden. 2006. <em>Wavelet methods for time series analysis</em>. Cambridge university press.</p>
</div>
<div>
<p>Percival, D. P. 1995. âOn Estimation of the Wavelet Variance.â <em>Biometrika</em> 82 (3). Oxford University Press:619â31.</p>
</div>
<div>
<p>Xu, H., StÃ©phane Guerrier, Roberto Molinari, and Yuming Zhang. 2017. âA Study of the Allan Variance for Constant-Mean Nonstationary Processes.â <em>IEEE Signal Processing Letters</em> 24 (8). IEEE:1257â60.</p>
</div>
<div>
<p>Zhang, N. F. 2008. âAllan Variance of Time Series Models for Measurement Data.â <em>Metrologia</em> 45 (5). IOP Publishing:549.</p>
</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="references.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/07-intro.Rmd",
"text": "Edit"
},
"download": ["scis.pdf", "scis.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
