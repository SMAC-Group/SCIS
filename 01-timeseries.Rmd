# Introduction to Time Series Analysis {#timeseries}

In this chapter, we will provide an introduction to time series analysis. This chapter is organized with the following outline:

- Definition and descriptive analysis of time series;
- Commonly used time series processes;
- Dependence within time series;
- Concept of stationarity;
- Fundamental representation of time series. 

## Time Series
```{definition, ts, name = "Time Series"}
A time series is a stochastic process (i.e. a sequence of random variables (r.v.)), defined on a common probability space denoted as $(X_t)_{t = 1,...,T}$ (i.e. \{$X_1$, $X_2$, ...., $X_T$\}). Note that the time $t$ is not continuous and belongs to discrete index sets. Therefore, we implicitly assume that

- $t$ is not random, i.e. the time at which each observation is measured is known, and 
- the time between two consecutive observations is constant.
```

When recording values of a time series over an extended period of time, it is usually difficult to discern any trend or pattern of the time series by simply looking at the values. However, when these data points are displayed on a plot with time on x-axis and $X_t$ on y-axis, some features of the time series jump out. So it is often useful to understand a time series process by performing a descriptive analysis, especially when we have data of small or moderate size. 

When we perform descriptive analysis, we usually check the following in the time series data/graph:

- Trends
    - Seasonal (e.g. business cycles)
    - Non-seasonal (e.g. impact of economic indicators on stock returns)
    - Local fluctuation (e.g. vibrations observed before, during and after an earthquake)
- Changes in the statistical properties
    - Mean (e.g. economic crisis)
    - Variance (e.g. earnings)
    - States (e.g. bear/bull in finance)
- Model deviations (e.g. outliers)

```{example, exampleJJ, name = "Johnson and Johnson Quarterly Earnings"}
A traditional example of a time series is the quarterly earnings of the company Johnson and Johson. In the graph below, we present these earnings between 1960 and 1980.
```

```{r, fig.height = 5, fig.width = 6.5, cache = TRUE, fig.align='center', fig.cap='Johnson and Johnson Quarterly Earnings'}
# Load data
data(jj, package = "astsa")
# Construct gts object
jj = gts(jj, start = 1960, freq = 4)
# Plot time series
plot(jj, main = "Johnson and Johnson Quarterly Earnings",
     ylab = "Quarterly Earnings per Share ($)") 
```

As we can see from the graph, the data contains a non-linear increasing trend as well as a yearly seasonal component. In addition, we can notice that the variability of the data seems to increase with time. By plotting the time series graph for this data, we can discern some important features of the data, which is helpful for us to conduct further analysis such as selecting suitable models for this data.

```{example, examplePrecipitation, name = "Monthly Precipitation Data"}
Now we consider another data set coming from the domain of hydrology. The data records monthly precipitation (in mm) over a certain period of time (1907 to 1972) and is interesting for scientists in order to study water cycles. The data are presented in the graph below:
```

```{r, fig.height = 5, fig.width = 6.5, cache = TRUE, fig.align='center', fig.cap='Monthly Precipitation Data'}
# Load data
data(hydro, package = "simts")
# Construct gts object
hydro = gts(hydro, start = 1907, freq = 12)
# Plot data
plot(hydro, main = "Monthly Precipitation Data",
     ylab = "Mean Monthly Precipitation (mm)")
```

From the above time series graph, we can observe some extreme observations (i.e. outliers) in this data more easily than if we simply look at all the values of this data. 

```{example, exampleInertialSensor, name = "Inertial Sensor Data"}
Now we consider the data coming from the calibration procedure of an Inertial Measurement Unit (IMU). The signals coming from an IMU are usually measured at high frequencies over a long time and are often characterized by linear trends and numerous underlying stochastic processes. We present the time series graph of some data from an IMU below.
```

```{r, fig.height = 5, fig.width = 6.5, cache = TRUE, fig.align='center', fig.cap='Inertial Sensor Data'}
# Load data
data(imu6, package = "imudata")
# Construct gts object
imu = gts(imu6[,1], freq = 100*60*60)
# Plot data
plot(imu, main = "Inertial Sensor Data",
     ylab = expression(paste("Error ", (rad/s^2))))
```

As we can see from the graph, although a linear trend and other processes are present in this data, it is practically impossible to discern any feature of the data based on the time series graph. In general, the descriptive analysis in classical time series analysis is not appropriate for the analysis of inertial sensors as these data are usually very large in order to perform parameter estimation.


## Commonly Used Time Series Processes
We first introduce some latent time series processes that are commonly used, especially in the calibration procedure of inertial sensors.

```{definition, WN, name = "Gaussian White Noise"}
The Gaussian White Noise (WN) process with parameter $\sigma^2 \in \mathbb{R}^+$ is defined as 
\begin{equation*}
		X_t  \overset{iid}{\sim} \mathcal{N}\left(0, \sigma^2 \right)
\end{equation*}
where "iid" stands for "independent and identically distributed".
```

```{definition, QN, name = "Quantization Noise"}
The Quantization Noise (QN) process with parameter $Q^2 \in \mathbb{R}^+$ is a process with Power Spectral Density (PSD) of the form 
\begin{equation*}
		S_{X}(f) = 4 Q^2 \sin^2 \left( \frac{\pi f}{\Delta t} \right) \Delta t, \;\; f < \frac{\Delta t}{2}.
\end{equation*}
```

```{definition, DR, name = "Drift"}
The Drift (DR) process with parameter $\omega \in \Omega$, where $\Omega$ is either $\mathbb{R}^+$ or $\mathbb{R}^-$, is defined as 
\begin{equation*}
X_t = \omega t.
\end{equation*}
```

```{definition, RW, name = "Random Walk"}
The Random Walk (RW) process with parameter $\gamma^2 \in \mathbb{R}^+$ is defined as 
\begin{equation*}
X_t = X_{t-1} + \epsilon_t \;\; \text{where}\;\; \epsilon_t  \overset{iid}{\sim} \mathcal{N}\left(0, \gamma^2 \right)\;\; \text{and}\;\; X_0 = 0.
\end{equation*}
```

```{definition, AR, name = "Auto-Regressive"}
The Auto-Regressive Process of Order 1 (AR1) process with parameter $\phi \in (-1, +1)$ and $\upsilon^2 \in \mathbb{R}^+$ is defined as 
\begin{equation*}
X_t = \phi X_{t-1} + Z_t, \;\;\; Z_t \overset{iid}{\sim} \mathcal{N}(0,\upsilon^2).
\end{equation*}
```

```{definition, GM, name = "Gauss Markov"}
The Gauss Markov Process of Order 1 (GM) process with parameter $\beta \in \mathbb{R}$ and $\sigma_G^2 \in \mathbb{R}^+$ is defined as 
\begin{equation*}
      X_t = \exp(-\beta \Delta_t) X_{t-1} + Z_t, \;\;\; 
      Z_t \overset{iid}{\sim} \mathcal{N}(0,\sigma^2_{G}(1-\exp(-2\beta\Delta t)))
\end{equation*}
where $\Delta t$ denotes the time between $X_t$ and $X_{t-1}$.
```

```{exercise, GMandAR1, name = "GM and AR1"}
A GM process is a one-to-one reparametrization of an AR1 process. In the following, we will only discuss AR1 processes but all results remain valid for GM processes.
```

```{definition, CompStocProc, name = "Composite Stochastic Processes"}
A composite stochastic process is a sum of latent processes. We implicitly assume that these latent processes are independent.
```

```{example, exampleCompStocProc, name = "2*AR1 + WN"}
The composite stochastic process of "2*AR1 + WN" is given as
\begin{align}
Y_t &= \phi_1 Y_{t-1} + Z_t, \;\;\; Z_t \overset{iid}{\sim} \mathcal{N}(0,\upsilon_1^2),\\
W_t &= \phi_2 W_{t-1} + U_t, \;\;\; U_t \overset{iid}{\sim} \mathcal{N}(0,\upsilon_2^2),\\
Q_t &\overset{iid}{\sim} \mathcal{N}(0,\sigma^2),\\
X_t &= Y_t + W_t + Q_t,
\end{align}
where $Y_t$, $W_t$ and $Q_t$ are independent and only $(X_t)$ is observed.
```