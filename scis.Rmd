--- 
title: "An Introduction to Inertial Sensors Stochastic Calibration"
author: "Stéphane Guerrier, Roberto Molinari, Yuming Zhang, Haotian Xu, Gaetan Bakalli, Ahmed Radi and Mucyo Karemera"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [biblio.bib, book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: SMAC-Group/SCIS
description: "TO DO"
---
--- 
title: "An Introduction to Inertial Sensors Stochastic Calibration"
author: "Stéphane Guerrier, Roberto Molinari, Yuming Zhang, Haotian Xu, Gaetan Bakalli, Ahmed Radi and Mucyo Karemera"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [biblio.bib, book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: SMAC-Group/SCIS
description: "TO DO"
---

# Introduction

TO DO (Introduction to the text)

<!--chapter:end:index.Rmd-->

# Introduction to Time Series Analysis {#timeseries}

In this chapter we give an introduction to time series analysis. For this purpose, it is organized in the following order:

- Definition and descriptive analysis of time series;
- Dependence within time series (and fundamental representations);
- Stationarity of time series;
- Basic time series models;
- Linear processes;
- Latent (or composite) stochastic processes;
- Estimation problems with time series.

## Time Series
```{definition, ts, name = "Time Series"}
A time series is a stochastic process (i.e. a sequence of random variables) defined on a common probability space. Let us denote this time-indexed stochastic process (time series) as $(X_t)_{t = 1,...,T}$, i.e. ($X_1$, $X_2$, ...., $X_T$), where the time $t$ belongs to the discrete index set. Therefore, we implicitly assume that

- $t$ is non-random, i.e. the time at which each observation is measured is known, and 
- the time between two consecutive observations is constant (i.e. sampling occurs at regular intervals).
```

As for any data analysis procedure, the first step consists in representing the data in such a way as to highlight any important features or information that should be taken into account for the following statistical analysis. For time series, a typical first step is representing the observations over time (where the latter is represented on the x-axis and values of $X_t$ on the y-axis). This can be considered as the first step for the **descriptive analysis** of a time series, especially when their length is moderate. 

With this in mind, when performing a descriptive analysis of a time series it is customary to check the following aspects:

- Trends
    - Seasonal (e.g. business cycles)
    - Non-seasonal (e.g. impact of economic indicators on stock returns)
    - Local fluctuations (e.g. vibrations observed before, during and after an earthquake)
- Changes in the statistical properties
    - Mean (e.g. economic crisis)
    - Variance (e.g. earnings)
    - States (e.g. bear/bull in finance)
- Model deviations (e.g. outliers)

In order to give an idea of what the above characteristics imply for a time series, the following examples provide practical insight for some of them.

```{example, exampleJJ, name = "Johnson and Johnson Quarterly Earnings"}
A first example of a time series is the quarterly earnings of the company Johnson and Johnson. In the graph below, we present these earnings between 1960 and 1980.
```

```{r, fig.height = 5, fig.width = 6.5, cache = TRUE, fig.align='center', fig.cap='Johnson and Johnson Quarterly Earnings'}
# Load simts package
library(simts)

# Load data
data(jj, package = "astsa")

# Construct gts object
jj = gts(jj, start = 1960, freq = 4)

# Plot time series
plot(jj, main = "Johnson and Johnson Quarterly Earnings",
     xlab = "Time (year)",
     ylab = "Quarterly Earnings per Share ($)") 
```

As we can see from the plot, the data contains a non-linear increasing trend as well as a seasonal component highlighted by the almost regularly-spaced peaks and valleys along time. In addition, we can notice that the variability of the data seems to increase with time (the seasonal variations appear to be larger towards the end of the plot). Hence, this simple visual representation can deliver important insight as to the behaviour of the time series and consequently determine the steps to take for further analysis (e.g. consider a non-linear model to explain the trend and consider approaches to model changing variance). <div style="text-align: right"> $\LARGE{\bullet}$ </div> 
<br>

```{example, examplePrecipitation, name = "Monthly Precipitation Data"}
Let us consider another data set coming from the domain of hydrology. The data records monthly precipitation (in mm) over a certain period of time (1907 to 1972) and is interesting for hydrologists for the purpose of studying water cycles. The data are presented in the plot below:
```

```{r, fig.height = 5, fig.width = 6.5, cache = TRUE, fig.align='center', fig.cap='Monthly Precipitation Data'}
# Load data
data(hydro, package = "simts")

# Construct gts object
hydro = gts(hydro, start = 1907, freq = 12)

# Plot time series
plot(hydro, main = "Monthly Precipitation Data",
     xlab = "Time (year)",
     ylab = "Mean Monthly Precipitation (mm)")
```

The time series plot above differs considerably from the previous one since the values of the time series, being always non-negative, remain almost always between 0 and 1 (no apparent trend) and randomly shows some larger observations that go beyond the value of 2 (or even 3). The latter appear to be extreme observations which could qualify as outliers, i.e. observations that are not representative of the true underlying model that generates the time series and can considerably affect the statistical analysis if not dealt with. <div style="text-align: right"> $\LARGE{\bullet}$ </div>
<br>

```{example, exampleInertialSensor, name = "Inertial Sensor Data"}
Another example is provided by the data coming from the calibration procedure of an Inertial Measurement Unit (IMU). The signals (or time series) coming from an IMU are usually measured at high frequencies over a long time and are often characterized by linear trends and numerous underlying stochastic processes. The plot below represents the time series of an error signal coming from a gyroscope belonging to an IMU.
```

```{r, fig.height = 5, fig.width = 6.5, cache = TRUE, fig.align='center', fig.cap='Inertial Sensor Data'}
# Load data
data(imu6, package = "imudata")

# Construct gts object
imu = gts(imu6[,1], freq = 100*60*60)

# Plot time series
plot(imu, main = "Inertial Sensor Data",
     ylab = expression(paste("Angular Velocity ", (rad/s))),
     xlab = "Time (h)")
```

As we can see from the plot, there wouldn't appear to be any linear trend, seasonality or increased variation in the time series. Indeed, from this plot it is difficult to detect any particular characteristic of this time series although a linear trend and other processes are present in this data. Therefore, especially when the length of the time series is considerable, this representation of time series can only give insight into few aspects (if none) and consequently is not suitable, on its own, to adequately inform the subsequent statistical analysis. <div style="text-align: right"> $\LARGE{\bullet}$ </div>
<br>

In order to deliver a more appropriate (or more complete) representation of a time series, it is important to study the concept of dependence since (in a linear vision of time) past observations have an influence on present and (possibly) future ones. Hence, the next section gives an overview of this concept.

## Dependence within Time Series

As mentioned above, it is straightforward to assume that observations measured through time are dependent on each other (in that observations at time $t$ have some form of impact on observations at time $t+1$ or beyond). Due to this characteristic, one of the main interests in time series is prediction where, if $(X_t)_{t=1,\ldots,T}$ is an identically distributed but not independent sequence, we often want to know the value of ${X}_{T+h}$ for $h > 0$ (i.e. an estimator of $\mathbb{E}[X_{T+h}| X_T,...]$). In order to tackle this challenge, we first need to understand the dependence between $X_{1},\ldots,X_{T}$ and, even before this, we have to formally define what **independence** is.

```{definition, IndepEvents, name = "Independence of Events"}
Two events $A$ and $B$ are independent if 
\begin{align*}
\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B),
\end{align*}
with $\mathbb{P}(A)$ denoting the probability of event $A$ occuring and $\mathbb{P}(A \cap B)$ denoting the joint probability (i.e. the probability that events $A$ and $B$ occur jointly). In general, $A_{1},\ldots,A_{n}$ are independent if 
\begin{align*}
\mathbb{P}(A_1 \ldots A_n) = \mathbb{P}(A_1) \ldots \mathbb{P}(A_n) \;\; \forall \; A_i \in S, \;\; i=1,\ldots,n
\end{align*}
where $S$ is the sample space.
```
<br>

```{definition, IndepRV, name = "Independence of Random Variables"}
Two random variables $X$ and $Y$ with Cumulative Distribution Functions (CDF) $F_X(x)$ and $F_Y(y)$, respectively, are independent if and only if their joint CDF $F_{X,Y}(x,y)$ is such that 
\begin{align*}
F_{X,Y}(x,y) = F_{X}(x) F_{Y}(y).
\end{align*}
In general, random variables $X_1, \ldots, X_n$ with CDF $F_{X_1}(x_1), \ldots, F_{X_n}(x_n)$ are respectively independent if and only if their joint CDF $F_{X_1, \ldots, X_n}(x_1, \ldots, x_n)$ is such that
\begin{align*}
F_{X_1,\ldots,X_n}(x_1,\ldots,x_n) = F_{X_1}(x_1) \ldots F_{X_n}(x_n).
\end{align*}
```
<br>

```{definition, iid, name = "iid sequence"}
The sequence $X_{1},X_{2},\ldots,X_{T}$ is said to be independent and identically distributed (i.e. iid) if and only if
\begin{align*}
\mathbb{P}(X_{i}<x) = \mathbb{P}(X_{j}<x) \;\; \forall x \in \mathbb{R}, \forall i,j \in \{1,\ldots,T\},
\end{align*}
and
\begin{align*}
\mathbb{P}(X_{1}<x_{1},X_{2}<x_{2},\ldots,X_{T}<x_{T})=\mathbb{P}(X_{1}<x_1) \ldots \mathbb{P}(X_{T}<x_T) \;\; \forall T\geq2, x_1, \ldots, x_T \in \mathbb{R}.
\end{align*}
```
<br>

The basic idea behind the above definitions of independence is the fact that the probability of an event regarding variable $X_i$ remains unaltered no matter what occurs for variable $X_j$ (for $i \neq j$). From this definition, we can now start exploring the concept of dependence, starting from **linear dependence** within a time series. For this purpose, below we define a quantity called AutoCovariance (ACV). 

```{definition, ACV, name="AutoCovariance"}
Autocovariance denoted as $\gamma_X(t, t+h)$ is defined as
\begin{align*}
\gamma_X(t, t+h) = \text{Cov}(X_{t},X_{t+h})= 	\mathbb{E}(X_{t}X_{t+h})-\mathbb{E}(X_{t})\mathbb{E}(X_{t+h}),
\end{align*}
where $\text{Cov}(\cdot)$ denotes the covariance and
\begin{align*}
\mathbb{E}(X_{t}) = \int_{-\infty}^{\infty}x \, f(x) \, dx \;\; \text{and} \;\; \mathbb{E}(X_{t},X_{t+h}) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}x_{t}\,x_{t+h}\, f(x_{t},x_{t+h}) \, dx_{t}\,dx_{t+h},
\end{align*}
where $f(x)$ denotes the density of $X_t$ and $f(x_{t},x_{t+h})$ denotes the joint density of $X_{t}$ and $X_{t+h}$.
```
<br>

Notice that, when two variable are independent, $\mathbb{E}(X_{t}X_{t+h}) = \mathbb{E}(X_{t})\mathbb{E}(X_{t+h})$ and hence $\gamma_X(t, t+h) = 0$. In a nutshell, the ACV measures the degree to which the mean-behaviour of a variable (e.g. $X_{t+h}$) changes when another one changes (e.g. $X_t$) and hence, to what extent they are linearly dependent.  

```{exercise, propertiesACV, name="Properties of ACV"}
\phantom{a}

1. ACV is symmetric, i.e. $\gamma_X(t, t+h) = \gamma_X(t+h, t)$ as $\text{Cov}(X_{t},X_{t+h}) = \text{Cov}(X_{t+h},X_{t})$. Under stationarity (will be discussed very soon, see Section \@ref(stationarity) for more details), $\gamma_X(h) = \gamma_X(-h)$, i.e. ACV is an even function. 

2. Variance of the process $\text{Var}(X_t) = \gamma_X(t, t) \geq 0$. Under stationarity, $\text{Var}(X_t) = \gamma_X(0)$ and $\mid \gamma_X(h) \mid \leq \gamma_X(0)$ by Cauchy-Schwarz inequality. 

3. Scale dependent: ACV $\gamma_X(t, t+h)$ is scale dependent like any covariance. So $\gamma_X(t, t+h) \in \mathbb{R}$. 

    - If $\mid \gamma_X(t, t+h) \mid$ is "close" to 0, then $X_{t}$ and $X_{t+h}$ are "less (linearly) dependent".
    - If $\mid \gamma_X(t, t+h) \mid$ is "far" from 0, then $X_{t}$ and $X_{t+h}$ are "more (linearly) dependent".

However in general, it is difficult to assess what "close" and "far" from zero mean.

4. In general, $\gamma_X(t, t+h)=0$ does not imply $X_{t}$ and $X_{t+h}$ are independent. However, if $X_{t}$ and $X_{t+h}$ are joint normally distributed, then $\gamma_X(t, t+h)=0$ implies that $X_{t}$ and $X_{t+h}$ are independent.
```
<br>

However the ACV is a quantity whose bounds are not known *a priori* and it can consequently be impossible to determine whether the degree of linear dependence is large or not. For this reason another measure of linear dependence can be used which is related to the ACV. Indeed, the AutoCorrelation Function (ACF) is a commonly used metric in time series analysis and is defined below.

```{definition, ACF, name="AutoCorrelation"}
AutoCorrelation (ACF) denoted as $\rho_X(t, t+h)$ is defined as 
\begin{align*}
\rho_X(t,t+h) = \text{Corr}(X_{t},X_{t+h}) = \frac{\text{Cov}(X_{t},X_{t+h})}{\sqrt{\text{Var}(X_{t})} \sqrt{\text{Var}(X_{t+h})}},
\end{align*}
where $\text{Var}(\cdot)$ denotes the variance.
```
<br>

```{exercise, propertiesACF, name = "Properties of ACF"}


1. $\mid \rho_X(t,t+h) \mid \leq 1$ and $\mid \rho_X(t,t) \mid = 1$. 

2. ACF is symmetric, i.e. $\rho_X(t, t+h) = \rho_X(t+h, t)$ as $\text{Corr}(X_{t},X_{t+h}) = \text{Corr}(X_{t+h},X_{t})$. Under stationarity, $\gamma_X(h) = \gamma_X(-h)$, i.e. ACF is an even function. 

3. Scale invariant: ACF $\rho_X(t,t+h)$ is scale free like any correlation. Moreover, if $\rho_X(t,t+h)$ is "close" to $\pm 1$, then this implies that there is "strong" (linear) dependence between $X_{t}$ and $X_{t+h}$.
```
<br>

Since the ACV is bounded by the product of the standard deviations of the two variables, we have that the ACF is bounded between 1 and -1. Therefore it is possible to better interpret the linear dependence where an ACF of zero indicates an absence of linear dependence while an ACF close to 1 or -1 indicates respectively a strong positive or negative linear dependence. 

As underlined up to now, both ACV and ACF are appropriate to measure linear dependence only. Therefore if the ACV and ACF are both zero, this does not imply that there isn't dependence between the variables. Indeed, aside from linear dependence, other forms of dependence such as monotonic or nonlinear dependence also exist which the ACV or ACF don't measure directly. The figure below shows scatterplots (one variable on the x-axis and another on the y-axis) along with the values of the ACF:

```{r, fig.height = 5, fig.width = 6.5, cache = TRUE, fig.align='center', out.width='80%', fig.cap='Different forms of dependence and their ACF values', echo=FALSE}
knitr::include_graphics("images/Cor.png")
```

As can be seen, the first row in the figure shows a consistent behaviour of the ACF value since the absolute value goes closer to 1 the more the points go closer to forming a line. However, the following rows show plots where there is clearly a strong relation (dependence) between the variables but the ACF doesn't detect this characteristic since this relation is not linear.

Finally, it is worth noting that correlation does NOT imply causation. For example, if $\rho(t, t+h) \neq 0$, this does not imply that $X_t \to X_{t+h}$ is causal. More specifically, the presence (or absence) of causality cannot be detected through statistical tools although some approximated metrics have been proposed to measure this concept [e.g Granger causality, see  @granger1969investigating]. 

For the following sections we will simplify the notations $\gamma_X(t, t+h)$ and $\rho_X(t, t+h)$ to be $\gamma(h)$ and $\rho(h)$ when there is no ambiguity (i.e. only one time series is considered and the ACF only depends on the time-lag $h$).
 
## Stationarity {#stationarity}

In this section we are going to introduce the concept of stationarity, one of the most important characteristics of time series data. First let us consider an example of **non-stationary processes**.

```{example, exampleNonStationary, name = "Non-Stationary Process"}
\begin{equation*}
X_t  \sim \mathcal{N} \left(0, Y_t^2\right) \;\; \text{where $Y_t$ is unobserved and such that} \;\; Y_t  \overset{iid}{\sim} \mathcal{N} \left(0, 1\right).
\end{equation*}

In this case, it is clear that the estimation of $\text{Var}(X_t)$ is difficult since only $X_t$ is useful for the estimation. So in fact, $X_t^2$ is our best guess for $\text{Var}(X_t)$. 
```

On the other hand, let us consider an example of **stationary processes** where averaging becomes meaningful for such process.

```{example, exampleStationary, name = "Stationary Process"}
\begin{equation*}
X_t = \theta W_{t-1} + W_t \;\;\; \text{where} \;\;\;  W_t  \stackrel{iid}{\sim} \mathcal{N} \left(0, 1\right).
\end{equation*}

In this case, we can guess that a natural estimator of $\text{Var}(X_t)$ can be $\hat{\sigma}^2 = \frac{1}{T} \sum_{i = 1}^T X_i^2$. That is, now averages are meaningful for such process. 
```

We formalize the above idea by introducing the concept of stationarity. There exist two forms of stationarity, which are defined below:

```{definition, StrongStationarity, name = "Strong Stationarity"}
The time series $X_{t}$ is strongly stationary if the joint probability distribution is invariant under a shift in time, i.e.
\begin{equation*}
\mathbb{P}(X_{t}\leq x_{0},\ldots,X_{t+k}\leq x_{k}) = \mathbb{P}(X_{t+h}\leq x_{0} ,\ldots,X_{t+h+k}\leq x_{k})
\end{equation*}
for any time shift $h$ and any $x_{0}, x_{1},x_{2},\cdots,x_{k}$ belong to the domain of $X_t,\cdots,X_{t+k}$ and $X_{t+h},\cdots,X_{t+h+k}$. 
```

```{definition, WeakStationarity, name = "Weak Stationarity"}
The time series $(X_{t})_{t \in \mathbb{N}}$ is weakly stationary if the mean and autocovariance are finite and invariant under a shift in time, i.e.
\begin{equation*}
\begin{aligned}
\mathbb{E}\left[X_t\right] &= \mu < \infty,\\
\mathbb{E}\left[X_t^2\right]  &= \mu_2 < \infty,\\
\text{Cov}(X_{t},X_{t+h})&= \text{Cov}(X_{t + k},X_{t+h + k}) = \gamma( h ).
\end{aligned}
\end{equation*}
for any time shift $h$. For convenience, we use the abbreviation "stationary" to indicate "weakly stationary" by default.
```

The stationarity of $X_{t}$ is important because it provides a framework in which averaging makes sense. The concept of averaging is essentially meaningless unless properties like mean and covariance are either fixed or evolve in a known manner. 

```{exercise, StatImpAcvAcf, name = "Implication on the ACV and ACF"}
If a process is weakly stationary or strongly stationary and $\text{Cov}(X_{t},X_{t+h})$ exists for all $h \in \mathbb{Z}$, then we have both ACV and ACF only depend on the lag between observations, i.e.
\begin{equation*}
\begin{aligned}
\gamma(t, t+h) &= \text{Cov}(X_{t},X_{t+h})= \text{Cov}(X_{t + k},X_{t+h + k}) = \gamma(t+k, t+h+k) = \gamma(h),\\
\rho(t, t+h) &= \text{Corr}(X_{t},X_{t+h})= \text{Corr}(X_{t + k},X_{t+h + k}) = \rho(t+k, t+h+k) = \rho(h).
\end{aligned}
\end{equation*}
```

```{exercise, RelationStationary, name = "Relation between Strong and Weak Stationarity"}


In general, neither type of stationarity implies the other one. However, 

- If $X_{t}$ is Normal (Gaussian) with $\sigma^2 = \text{Var} (X_{t}) < \infty$, then weak stationarity implies strong stationarity.
- If $X_{t}$ is strongly stationary, $\mathbb{E}(X_t) < \infty$ and $\mathbb{E}(X_t^2) < \infty$, then $X_{t}$ is weakly stationary.
```

```{example, exampleRelationStat1, name = "Strong Stationarity does NOT imply Weak Stationarity"}
An iid Cauchy process is strongly but not weakly stationary as the mean of the process does not exist.
```

```{example, exampleRelationStat2, name = "Weak Stationarity does NOT imply Strong Stationarity"}
Let $X_t \overset{iid}{\sim} \exp(1)$ (i.e. exponential distribution with $\lambda = 1$) and $Y_t \overset{iid}{\sim} \mathcal{N}(1,1)$. Then, let
\begin{equation*}
Z_t = \left\{
\begin{array}{cl}
X_t &\text{if } t \in \left\{2k | k \in \mathbb{N}\right\}\\
Y_t &\text{if }  t \in \left\{2k + 1 | k \in \mathbb{N}\right\},
\end{array}
\right.
\end{equation*}
we have $Z_t$ is weakly stationary but not strongly stationary.
```

```{exercise, StatProcesses, name = "Stationarity of Latent Time Series Processes"}


- (Weakly) Stationary: WN, QN, AR1
- (Weakly) Non-Stationary: DR, RW
```

```{proof, proofAR1stationary, name="AR1 is weakly stationary"}
Consider an AR1 process defined as:
\begin{equation*}
X_t = \phi X_{t-1} + Z_t, \;\;\; Z_t \overset{iid}{\sim} \mathcal{N}(0,\nu^2),
\end{equation*}
where $\mid \phi \mid < 1$ and $\nu^2 < \infty$. Then we have
\[\begin{aligned}
{X_t}  &=   {\phi }{X_{t - 1}} + {Z_t} = \phi \left[ {\phi {X_{t - 2}} + {Z_{t - 1}}} \right] + {Z_t} =  {\phi ^2}{X_{t - 2}} + \phi {Z_{t - 1}} + {Z_t}  \\
& \; \vdots  \\
&=   {\phi ^k}{X_{t-k}} + \sum\limits_{j = 0}^{k - 1} {{\phi ^j}{Z_{t - j}}} .
\end{aligned} \]
By taking the limit in $k$ (which is perfectly valid as we 
assume $t \in \mathbb{Z}$), we obtain 
\begin{equation*}
\begin{aligned}
X_t = \mathop {\lim }\limits_{k \to \infty} \; {X_t}  =  \sum\limits_{j = 0}^{\infty} {{\phi ^j}{Z_{t - j}}}.
\end{aligned}
\end{equation*}
So we have
\begin{equation*}
\begin{aligned}
\mathbb{E}\left[X_t\right] &=  \sum\limits_{j = 0}^{\infty} {{\phi ^j}{\mathbb{E} [Z_{t - j}]}} = 0, \\
\text{Var}\left(X_t\right) &= \text{Var}\left(\sum\limits_{j = 0}^{\infty} {{\phi ^j}{Z_{t - j}}}\right) = \sum\limits_{j = 0}^{\infty} {\phi^{2j}} \text{Var}\left(Z_{t-j}\right) = \nu^2 \sum\limits_{j = 0}^{\infty} {\phi^{2j}} = \frac{\nu^2}{1-\phi^2} < \infty.
\end{aligned}
\end{equation*}
Moreover, assuming for notational simplicity that $h > 1$, we obtain
\begin{equation*}
\begin{aligned}
\text{Cov}\left(X_t, X_{t+h}\right) &= \phi \text{Cov}\left(X_t, X_{t+h-1}\right) = \phi^2 \text{Cov}\left(X_t, X_{t+h-2}\right) = \ldots = \phi^h \text{Cov}(X_t, X_t).
\end{aligned}
\end{equation*}
In general, when $h \in \mathbb{Z}$ we obtain
\begin{equation*}
\begin{aligned}
\text{Cov}\left(X_t, X_{t+h}\right) & = \phi^{|h|} \text{Cov}(X_t, X_t) = \phi^{|h|} \frac{\nu^2}{1-\phi^2},
\end{aligned}
\end{equation*}
which is a function of the lag $h$ only. Therefore, this AR1 process is weakly stationary.
```

## Linear Processes
In this section we introduce the concept of linear processes. As a matter of fact, considered stationary models can all, so far, be represented as linear processes. 

```{definition, LinearProc, name = "Linear Processes"}
A stochastic process $(X_t)$ is said to be a linear process if it can be expressed as a linear combination of an iid Gaussian sequence (i.e. white noise process), i.e.:
\[{X_t} = \mu + \sum\limits_{j =  - \infty }^\infty  {{\psi _j}{W_{t - j}}} \]
where $W_t \overset{iid}{\sim} \mathcal{N}(0, \sigma^2)$ and $\sum\limits_{j =  - \infty }^\infty  {\left| {{\psi _j}} \right|}  < \infty$.
```

Notice that the condition $\sum\limits_{j = - \infty }^\infty {\left| {{\psi _j}} \right|} < \infty$ is required in the definition of linear processes in order to ensure that the series has a limit and is related to the absolutely summable covariance structure, which is defined below.

```{definition, AbsSumCov, name = "Absolutely Summable Covariance Structure"}
A process $(X_t)$ is said to have an absolutely summable covariance structure if $\sum\limits_{h = - \infty }^\infty {\left| \gamma_X(h) \right|} < \infty$.
```


```{exercise, propertiesLinearProc, name = "Properties of Linear Processes"}


1. All linear processes are stationary since
\[\begin{aligned}
\mathbb{E}[X_t] &= \mu, \\
\gamma(h) &= \sigma^2\sum\limits_{j =  - \infty }^\infty  {{\psi _j}{\psi _{j+h}}}.
\end{aligned}\]

2. All linear processes have absolutely summable covariance structures. 
```

```{proof, ACVofLP, name = "ACV of Linear Processes"}
\begin{align*}
\gamma(h) &= \text{Cov}(X_t, X_{t+h}) = \text{Cov}(\mu+\sum_{j=-\infty}^{\infty} \psi_j W_{t-j}, \mu+\sum_{j=-\infty}^{\infty} \psi_j W_{t+h-j})\\
&= \text{Cov}(\sum_{j=-\infty}^{\infty} \psi_j W_{t-j}, \sum_{j=-\infty}^{\infty} \psi_j W_{t-(j-h)}) \\
&= \text{Cov}(\sum_{j=-\infty}^{\infty} \psi_j W_{t-j}, \sum_{j=-\infty}^{\infty} \psi_{j+h} W_{t-j}) \\
&= \sum_{j=-\infty}^{\infty} \psi_j \psi_{j+h} \text{Cov}(W_{t-j}, W_{t-j}) \\
&= \sigma^2\sum\limits_{j =  - \infty }^\infty  {{\psi _j}{\psi _{j+h}}}.
\end{align*}
```

```{proof, AbsSumCovLP, name = "All linear processes have absolutely summable covariance structures."}
\begin{align*}
\sum_{h=-\infty}^{\infty} \mid \gamma(h) \mid &= \sum_{h=-\infty}^{\infty} \sigma^2 \mid \sum_{j=-\infty}^{\infty} \psi_j \psi_{j+h} \mid \\
&\leq \sigma^2 \sum_{h=-\infty}^{\infty} \sum_{j=-\infty}^{\infty} \mid \psi_j \psi_{j+h} \mid \\
&= \sigma^2 \sum_{j=-\infty}^{\infty} \sum_{h=-\infty}^{\infty} \mid \psi_j \mid \cdot \mid \psi_{j+h} \mid \\
&= \sigma^2 \sum_{j=-\infty}^{\infty} \mid \psi_j \mid \sum_{h=-\infty}^{\infty} \mid \psi_{j+h} \mid \\
&= \sigma^2 \big( \sum_{j=-\infty}^{\infty} \mid \psi_j \mid \big)^2 < \infty
\end{align*}
So with the assumption that $\sum_{j=-\infty}^{\infty} \mid \psi_j \mid < \infty$, we obtain that all linear processes have absolutely summable covariance structures. Notice that here we have shown that the condition $\sum\limits_{j = - \infty }^\infty {\left| {{\psi _j}} \right|} < \infty$ is actually stronger than $\sum\limits_{h = - \infty }^\infty {\left| \gamma(h) \right|} < \infty$.
```

```{example, AR1isLP, name = "AR1 is a linear process"}
When we prove above that AR1 is weakly stationary, we have shown that for an AR1 process $X_t = \phi X_{t-1} + Z_t, \;\;\; Z_t \overset{iid}{\sim} \mathcal{N}(0,\nu^2)$, it can be represented as 
\begin{align*}
X_t = \sum\limits_{j = 0}^{\infty} {{\phi ^j}{Z_{t - j}}}.
\end{align*}
Therefore, AR1 is a linear process.
```


## Basic Time Series Models

We first introduce some latent time series models that are commonly used, especially in the calibration procedure of inertial sensors.

```{definition, WN, name = "Gaussian White Noise"}
The Gaussian White Noise (WN) process with parameter $\sigma^2 \in \mathbb{R}^+$ is defined as 
\begin{equation*}
		X_t  \overset{iid}{\sim} \mathcal{N}\left(0, \sigma^2 \right)
\end{equation*}
where "iid" stands for "independent and identically distributed".
```

```{definition, QN, name = "Quantization Noise"}
The Quantization Noise (QN) process with parameter $Q^2 \in \mathbb{R}^+$ is a process with Power Spectral Density (PSD) of the form 
\begin{equation*}
		S_{X}(f) = 4 Q^2 \sin^2 \left( \frac{\pi f}{\Delta t} \right) \Delta t, \;\; f < \frac{\Delta t}{2}.
\end{equation*}
```

```{definition, DR, name = "Drift"}
The Drift (DR) process with parameter $\omega \in \Omega$, where $\Omega$ is either $\mathbb{R}^+$ or $\mathbb{R}^-$, is defined as 
\begin{equation*}
X_t = \omega t.
\end{equation*}
```

```{definition, RW, name = "Random Walk"}
The Random Walk (RW) process with parameter $\gamma^2 \in \mathbb{R}^+$ is defined as 
\begin{equation*}
X_t = X_{t-1} + \epsilon_t \;\; \text{where}\;\; \epsilon_t  \overset{iid}{\sim} \mathcal{N}\left(0, \gamma^2 \right)\;\; \text{and}\;\; X_0 = 0.
\end{equation*}
```

```{definition, AR, name = "Auto-Regressive"}
The Auto-Regressive process of Order 1 (AR1) with parameter $\phi \in (-1, +1)$ and $\upsilon^2 \in \mathbb{R}^+$ is defined as 
\begin{equation*}
X_t = \phi X_{t-1} + Z_t, \;\;\; Z_t \overset{iid}{\sim} \mathcal{N}(0,\upsilon^2).
\end{equation*}
```

```{definition, GM, name = "Gauss Markov"}
The Gauss Markov process of Order 1 (GM) with parameter $\beta \in \mathbb{R}$ and $\sigma_G^2 \in \mathbb{R}^+$ is defined as 
\begin{equation*}
      X_t = \exp(-\beta \Delta_t) X_{t-1} + Z_t, \;\;\; 
      Z_t \overset{iid}{\sim} \mathcal{N}(0,\sigma^2_{G}(1-\exp(-2\beta\Delta t)))
\end{equation*}
where $\Delta t$ denotes the time between $X_t$ and $X_{t-1}$.
```

```{exercise, GMandAR1, name = "GM and AR1"}
A GM process is a one-to-one reparametrization of an AR1 process. In the following, we will only discuss AR1 processes but all results remain valid for GM processes.
```

With the above defined latent time series processes, we introduce the composite stochastic process, which is widely used in the estimation procedure of inertial sensor stochastic calibration.

```{definition, CompStocProc, name = "Composite Stochastic Process"}
A composite stochastic process is a sum of latent processes. We implicitly assume that these latent processes are independent.
```

```{example, exampleCompStocProc, name = "2*AR1 + WN"}
The composite stochastic process of "2*AR1 + WN" is given as
\begin{align}
Y_t &= \phi_1 Y_{t-1} + Z_t, \;\;\; Z_t \overset{iid}{\sim} \mathcal{N}(0,\upsilon_1^2),\\
W_t &= \phi_2 W_{t-1} + U_t, \;\;\; U_t \overset{iid}{\sim} \mathcal{N}(0,\upsilon_2^2),\\
Q_t &\overset{iid}{\sim} \mathcal{N}(0,\sigma^2),\\
X_t &= Y_t + W_t + Q_t,
\end{align}
where $Y_t$, $W_t$ and $Q_t$ are independent and only $X_t$ is observed.
```



## Fundamental Representations of Time Series
We conclude this chapter by summarizing the fundamental representations of time series. If two processes have the same fundamental representations, then these two processes are the same. There are two most commonly used fundamental representations of time series, i.e.

- ACV and ACF;
- Power Spectral Density (PSD).

```{exercise, FundaRepreACVACF, name = "ACV and ACF as fundamental representation"}
If we consider a zero mean normally distributed process, it is clear that its joint distribution is fully characterized by the autocovariances $\mathbb{E}[X_t X_{t+h}]$ since the joint probability density only depends on these covariances. Once we know the autocovariances we know everything there is to know about the process and therefore: if two processes have the same autocovariance function, then they are the same process.
```

```{exercise, FundaReprePSD, name = "PSD as fundamental representation"}
The PSD is defined as
\begin{equation*}
    S_X(f) = \int_{- \infty}^{\infty} \gamma_{X}(h)\,e^{-ifh}dh,
\end{equation*}
where $f$ is a frequency. Hence, the PSD is a Fourier Transform (FT) of the autocovariance function which describes the variance of a time series over frequencies (with respect to lags $h$).

Given that the definition of the PSD, as for the autocovariance function, once we know the PSD we know everything there is to know about the process and therefore: if two processes have the same PSD, then they are the same process.
```


## Estimation Problems with Time Series 
Estimation in the context of time series is not as straightforward as in the iid case. For example, let us consider the easiest case of estimation: the sample mean of a stationary time series. 

```{example, EstSampleMean, name = "Estimation with Sample Mean"}
Let $(X_t)$ be a stationary time series, so we have that $\mathbb{E}[X_t] = \mu$ and the value of $\mu$ can be estimated by the sample mean, i.e.
\begin{equation*}
	\bar{X} = \frac{1}{T} \sum_{t = 1}^T X_t.
\end{equation*}
Using the properties of a stationary process, we obtain
\begin{equation*}
\text{Var} \left(\bar{X}\right) = \frac{\gamma(0)}{T} \sum_{h = -T}^{T} \left(1 - \frac{|h|}{T}\right) \rho(h)
\end{equation*}
since
\begin{align*}
\text{Var}(\bar{X}) &= \frac{1}{T^2} \text{Var}(\sum_{t=1}^T X_t) = \frac{1}{T^2} (\sum_{t=1}^T \text{Var}(X_t) + 2\underset{1 \leq t<s\leq T}{\sum\sum} \text{Cov}(X_t, X_s)) \\
&= \frac{1}{T^2} (T\gamma(0) + 2(T-1)\gamma(1) + 2(T-2)\gamma(2) + \ldots + 2\gamma(T-1)) \\
&= \frac{\gamma(0)}{T^2} (T+ 2(T-1)\rho(1) + \ldots + 2\rho(T-1)) \\
&= \frac{\gamma(0)}{T} + \frac{2\gamma(0)}{T} \sum_{h=1}^{T-1} (1-\frac{h}{T})\rho(h) \\
&= \frac{\gamma(0)}{T} \sum_{h = -(T-1)}^{T-1} \left(1 - \frac{|h|}{T}\right) \rho(h) \\
&= \frac{\gamma(0)}{T} \sum_{h = -T}^{T} \left(1 - \frac{|h|}{T}\right) \rho(h).
\end{align*}
```

```{example, EstSampleMeaninAR1, name = "Estimation with Sample Mean in AR1"}
As in the previous example, let us consider a stationary AR1 process, i.e.
\begin{equation*}
X_t = \phi X_{t-1} + Z_t, \;\;\; \text{where} \;\;\; |\phi| < 1 \;\;\; \text{and} \;\;\;  Z_t  \overset{iid}{\sim} \mathcal{N} \left(0, \nu^2\right).
\end{equation*}

We have obtained before that in AR1, $\gamma(h) = \phi^h \sigma^2 \left(1 - \phi^2\right)^{-1}$. Therefore, we obtain (after some computations):

\begin{equation*}
\text{Var} \left( {\bar X} \right) = \frac{\nu^2 \left( T - 2\phi - T \phi^2 + 2 \phi^{T + 1}\right)}{T^2\left(1-\phi^2\right)\left(1-\phi\right)^2}.
\end{equation*}

Unfortunately, deriving such an exact formula is often difficult when considering more complex models. Therefore, asymptotic approximations are often employed to simplify the calculation. For example, in this AR1 case we have
\begin{equation*}
\lim_{T \to \infty } \; T \text{Var} \left( {\bar X} \right) = \frac{\nu^2}{\left(1-\phi\right)^2},
\end{equation*}
providing the following approximate formula
\begin{equation*}
\text{Var} \left( {\bar X} \right) \approx \frac{\nu^2}{T \left(1-\phi\right)^2}.
\end{equation*}


Alternatively, simulation methods can also be employed. For example, one could compute $\text{Var} \left( {\bar X} \right)$ as follows:

Step 1: Simulate under the assumed model, i.e. $X_t^* \sim F_{\theta_0}$, where $F_{\theta_0}$ denotes the true model (in this case an AR1 process).

Step 2: Compute ${\bar X^*}$ (i.e. average based on $(X_t^*)$).

Step 3: Repeat Steps 1 and 2 $B$ times.

Step 4: Compute the empirical variance ${\bar X^*}$ (based on $B$ independent replications).

The above procedure is known as Monte-Carlo method (in this case it is actually a Monte-Carlo integral) and is closely related to the concept of parametric bootstrap (see @efron1994introduction) which is a very popular tool in statistics.

```

Now we define the classical estimators of $\gamma(h)$ and $\rho(h)$ for AutoCovariance and AutoCorrelation functions. 

```{definition, sampleACV, name = "Sample AutoCovariance Function"}
The sample autocovariance function is defined as
\begin{equation*}
\hat{\gamma}(h) = \frac{1}{T} \sum_{t = 1}^{T-h} \left(X_{t} - \bar{X}\right) \left(X_{t+h} - \bar{X}\right)
\end{equation*}
with $\hat{\gamma}(h) = \hat{\gamma}(-h)$ for $h = 0, 1, ..., k$, where $k$ is a fixed integer.
```

```{definition, sampleACF, name = "Sample AutoCorrelation Function"}
The sample autocorrelation function is defined as
\begin{equation*}
\hat{\rho}(h) = \frac{\hat{\gamma}(h)}{\hat{\gamma}(0)}
\end{equation*}
with $\hat{\rho}(h) = \hat{\rho}(-h)$ for $h = 0, 1, ..., k$, where $k$ is a fixed integer.
```

We will discuss more about the properties of these estimators in the next chapter.



<!--chapter:end:01-timeseries.Rmd-->

# Properties of Statistical Estimators

In this chapter, we will provide a review of the properties of statistical estimators. This chapter is organized with the following outline:

- Extremum estimators;
- Consistency;
- Asymptotic normality.

## Extremum Estimators
In this section, we will introduce a commonly used class of estimators, extremum estimators, and some examples of it. 

```{definition, EstreEst, name = "Extremum Estimators"}
Many estimators have a common structure, which is often useful to study their asymptotic properties. One structure or framework is the class of estimators that maximize some objective function, referred to as extremum estimators, which can can be defined as follows:
\begin{equation*}
\hat{\boldsymbol{\theta}} \equiv \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; \hat{Q}_n(\boldsymbol{\theta})
\end{equation*}
where $\boldsymbol{\theta}$ and $\boldsymbol{\Theta}$ denote, respectively, the parameter vector of interest and its set of possible values.
```

The vast majority of statistical estimators can be represented as extremum estimators. For example, least squares, maximum likelihood or (generalized) method of moment estimators can all be represented as extremum estimators.

```{example, ExtreEstLSE, name = "Least Squares Estimator as Extremum Estimator"}
Consider the linear model $\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta}_0 + \boldsymbol{\epsilon}$ where $\boldsymbol{X} \in \mathbb{R}^{n \times p}$ is a full rank constant matrix, $\boldsymbol{\beta} \in {\mathcal{B}} \subseteq \mathbb{R}^p$ and $\epsilon_i \overset{iid}{\sim} \mathcal{N}(0, \sigma_\epsilon^2)$. Let $\hat{\boldsymbol{\beta}}$ denote the Least Squares Estimator (LSE) of $\boldsymbol{\beta}_0$, i.e.
\begin{equation*}
\hat{\boldsymbol{\beta}} = \left(\boldsymbol{X}^T \boldsymbol{X} \right)^{-1} \boldsymbol{X}^T \boldsymbol{y}. 
\end{equation*}
This LSE is an extremum estimator since it can be expressed as 
\begin{equation*}
\hat{\boldsymbol{\beta}} = \underset{\boldsymbol{\beta} \in \mathcal{B}}{\text{argmax}} \; -||\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} ||_2^2.
\end{equation*}
```

```{example, ExtreEstMLE, name = "Maximum Likelihood Estimator as Extremum Estimator"}
Let $Z_1, \ldots, Z_n$ be an iid sample with pdf $f(z|\boldsymbol{\theta}_0)$. The Maximum Likelihood Estimator (MLE) is given by
\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; \frac{1}{n} \sum_{i = 1}^{n} \text{log}\left[f\left(z_i| \boldsymbol{\theta}\right)\right].
\end{equation*}

Here instead of the actual log-likelihood, we are actually using a normalized log-likelihood, which has no impact on the estimator but the normalized form is more convenient to use when we let $n \to \infty$.

Therefore, the MLE can be seen as an extremum estimator with 
\begin{equation*}
\hat{Q}_n(\boldsymbol{\theta}) = \frac{1}{n} \sum_{i = 1}^{n} \text{log}\left[f\left(z_i| \boldsymbol{\theta}\right)\right].
\end{equation*}
```

```{definition, GMM, name = "Generalized Method of Moments"}
Let $Z_1, \ldots, Z_n$ be an iid sample with pdf $f(z|\boldsymbol{\theta}_0)$. Suppose that there is a moment function vector $\boldsymbol{g}(z | \boldsymbol{\theta})$ such that $\mathbb{E}[\boldsymbol{g} (z | \boldsymbol{\theta}_0)] = 0$. Then the Generalized Method of Moments (GMM) estimator of $\boldsymbol{\theta}_0$ is defined as 
\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; - \left[\frac{1}{n} \sum_{i = 1}^n \boldsymbol{g}(z_i | \boldsymbol{\theta}) \right]^T \widehat{\boldsymbol{W}} \left[\frac{1}{n} \sum_{i = 1}^n \boldsymbol{g}(z_i | \boldsymbol{\theta}) \right],
\end{equation*}
where $\widehat{\boldsymbol{W}}$ is an positive definite matrix of appropriate dimension.

Alternatively (but equivalently), we can define GMM estimator as 
\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; - || \hat{\boldsymbol{\mu}} - \boldsymbol{\mu}(\boldsymbol{\theta}) ||_{\widehat{\boldsymbol{W}}}^2,
\end{equation*}
where $|| \boldsymbol{x} ||_{\boldsymbol{A}}^2 = \boldsymbol{x}^T \boldsymbol{A} \boldsymbol{x}$, and where $\hat{\boldsymbol{\mu}}$ and $\boldsymbol{\mu}(\boldsymbol{\theta})$ denote, respectively, the empirical and model based moments. We will use this form of definition of GMM estimator more often.
```

```{example, ExtreEstGMM, name = "A Simple GMM Estimator as Extremum Estimator"}
Let $Z_i \overset{iid}{\sim} \mathcal{N}(\mu_0, \sigma^2_0)$ and $\boldsymbol{\theta}_0 = (\mu_0, \sigma_0^2)^T$. Suppose we wish to estimate $\boldsymbol{\theta}_0$ by matching the first three empirical moments with their theoretical counterparts. In this case, a reasonable moment function or condition defining a GMM estimator is given by:
\begin{equation*}
\boldsymbol{g} (Z | \boldsymbol{\theta}) = \begin{bmatrix}
Z - \mu\\
Z^2 - \left(\mu^2 + \sigma^2\right)\\
Z^3 - \left(\mu^3 + 3 \mu \sigma^2\right)
\end{bmatrix}.
\end{equation*}

Notice that $\frac{1}{n} \sum_{i=1}^n \boldsymbol{g} (Z_i | \boldsymbol{\theta}) = \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta})$, where $\hat{\boldsymbol{\gamma}}$ and $\boldsymbol{\gamma}(\boldsymbol{\theta})$ denote, respectively, the empirical and model based moments, i.e.
\begin{equation*}
\hat{\boldsymbol{\gamma}} = \frac{1}{n}  \sum_{i = 1}^n 
\begin{bmatrix}
Z_i\\
Z_i^2\\
Z_i^3\\
\end{bmatrix}, \;\;\;\;
\boldsymbol{\gamma}(\boldsymbol{\theta}) = 	\begin{bmatrix}
\mu\\
\mu^2 + \sigma^2\\
\mu^3 + 3 \mu \sigma^2
\end{bmatrix}.
\end{equation*}

Therefore we can write the GMM estimator of $\boldsymbol{\theta}_0$ as
\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmin}} \; || \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma} (\boldsymbol{\theta}) ||_{\widehat{\boldsymbol{W}}}^2 = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; - || \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma} (\boldsymbol{\theta}) ||_{\widehat{\boldsymbol{W}}}^2 
\end{equation*}
```


## Consistency
In this section, we will introduce one of the most important properties of statistical estimators, consistency. And we will discuss in details about the conditions for consistency of the extremum estimators. 

```{definition, consistency, name = "Consistency"}
An estimator $\hat{\boldsymbol{\theta}}$ is said to be consistent or weakly consistent if it converges in probability to $\boldsymbol{\theta}_0$, i.e.
\begin{equation*}
\underset{n \to \infty}{\text{lim}} \mathbb{P} (||\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0||_2 \geq \epsilon) = 0,
\end{equation*}
for all $\epsilon >0$.
```

In Layman's term, consistency simply means that if $n$ is large enough, then $\hat{\boldsymbol{\theta}}$ will be arbitrarily close to $\boldsymbol{\theta}_0$ (i.e. inside of an hypersphere of radius $\epsilon$ centered at $\boldsymbol{\theta}_0$). This also means the procedure (i.e. our estimator $\hat{\boldsymbol{\theta}}$) based on unlimited data will be able to identify the underlying truth (i.e. $\boldsymbol{\theta}_0$).


```{r, fig.height = 5, fig.width = 6.5, cache = TRUE, fig.align='center', out.width='80%', fig.cap='Interpretation of consistency', echo=FALSE}
knitr::include_graphics("images/consistency.pdf")
```

To show the consistency of estimators, we often make use of the following two important results:

```{theorem, WLLN, name = "Weak Law of Large Number"}
Suppose $X_i$ are iid random variables with finite mean $\mu$ (i.e. $\mathbb{E}[X_i] = \mu$) and finite variance. Let $\bar{X}_n = \frac{1}{n} \sum_{i = 1}^n X_i$, then $\bar{X}_n \overset{\mathcal{P}}{\mapsto} \mu$.
```

```{theorem, CMT, name = "Continuous Mapping Theorem"}
If $Y_n \overset{\mathcal{P}}{\mapsto} \mu$ and $g(\cdot)$ is a continuous function, then $g(Y_n) \overset{\mathcal{P}}{\mapsto} g(\mu)$.
```

Here we present a simple example to prove the consistency of an estimator with the above two results.

```{example, ConsistencyExpDist, name = "Consistency in Exponential Distribution"}
Suppose we have an iid sample from exponential distribution, i.e. $X_i \overset{iid}{\sim} \text{exp}(\lambda_0),\;  \lambda_0 \in \mathbb{R}^+, \; i = 1,..., n$. So assuming $X \geq 0$, the density of $X$ is given by 
\begin{equation*}
f(x|\lambda) = \lambda \text{exp}\left( - \lambda x \right).
\end{equation*}
In this example, we want to show that the MLE for $\lambda_0$ is a consistent estimator of $\lambda_0$.

First, we want to find the MLE for $\lambda_0$. The normalized log-likelihood function is given by
\begin{equation*}
\mathcal{L}(\lambda | X_1, ..., X_n) = \text{log}(\lambda) - \lambda \bar{X}_n.
\end{equation*}
By solving 
\begin{equation*}
\frac{\partial}{\partial \lambda} \; \mathcal{L}(\lambda | X_1, \ldots, X_n) = \frac{1}{\lambda} - \bar{X}_n = 0,
\end{equation*}
we obtain $\hat{\lambda} = \frac{1}{\bar{X}_n}$.
We verify that 
\begin{equation*}
\frac{\partial^2}{\partial \lambda^2} \; \mathcal{L}(\lambda | X_1, \ldots, X_n) = -\frac{1}{\lambda^2} < 0,
\end{equation*}
which implies that $\hat{\lambda}$ is the maxima of $\mathcal{L}(\lambda | X_1, \ldots, X_n)$. Therefore, the MLE for $\lambda_0$ is $\hat{\lambda} = \frac{1}{\bar{X}_n}$.

By Weak Law of Large Number, we have $\bar{X}_n \overset{\mathcal{P}}{\mapsto} \mu$, where \mu is given by 
\begin{equation*}
\mu = \mathbb{E}[X_i] = \int_{0}^{\infty} x \lambda_0 \text{exp}\left( - \lambda_0 x \right) dx = \frac{1}{\lambda_0}.
\end{equation*}
And also since the function $g(x) = 1/x$ is continuous in $\mathbb{R}^+$, we obtain by the Continuous Mapping Theorem that $\hat{\lambda} \overset{\mathcal{P}}{\mapsto} \lambda_0$, which concludes that the MLE for $\lambda_0$ is a consistent estimator of $\lambda_0$ in exponential distribution.
```


### Consistency of Extremum Estimators
When considering real-life problems, the above approach based on Weak Law of Large Number and Continuous Mapping Theorem is in general not flexible enough. Therefore, we often rely on the results as following. 

```{theorem, ConsExtreEst, name = "Consistency of Extremum Estimators"}
If there is a function ${Q}_0 (\boldsymbol{\theta})$ such that:

C1. ${Q}_0 (\boldsymbol{\theta})$ is uniquely maximized in $\boldsymbol{\theta}_0$,

C2. $\boldsymbol{\Theta}$ is compact,

C3. ${Q}_0 (\boldsymbol{\theta})$ is continuous in $\boldsymbol{\theta}$,

C4. $\hat{Q}_n (\boldsymbol{\theta})$ converges uniformly in probability to $Q_0 (\boldsymbol{\theta})$,

then we have $\hat{\boldsymbol{\theta}} \overset{\mathcal{P}}{\mapsto} \boldsymbol{\theta}_0$.
```

```{definition, compact, name = "Compactness"}
We say $\boldsymbol{\Theta}$ is compact if every open cover of $\boldsymbol{\Theta}$ contains a finite subcover. If $\boldsymbol{\Theta}$ is compact, then $\boldsymbol{\Theta}$ is closed (i.e. containing all its limit points) and bounded (i.e. all its points are within some finite distance of each other). 
```

```{definition, UnifCont, name = "Uniform Convergence in Probability"}
$\hat{Q}_n (\boldsymbol{\theta})$ is said to converges uniformly in probability to $Q_0 (\boldsymbol{\theta})$ if $\underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{sup}} \; |\hat{Q}_n (\boldsymbol{\theta}) - {Q}_0 (\boldsymbol{\theta})| \overset{\mathcal{P}}{\mapsto} 0$.  
```

Theorem \@ref(thm:ConsExtreEst) is an important result as it provides a general approach to prove the consistency of the class of extremum estimators. Notice that in this theorem:

- Condition (C1) is **substantive** and there are well-known examples where it fails. We will discuss further on how this assumption can (in some cases) be verified in practice.
- Condition (C2) is also **substantive** as it requires that there exist some known bounds on the parameters. In practice, this assumption is often neglected although it is in most cases unrealistic to assume it.
- Condition (C3) and (C4) are often referred to as **"standard regularity conditions"**. They are typically satisfied. The verification of these conditions will be discussed further later in this section.


```{proof, proofConsExtreEst, name="Consistency of Extremum Estimators"}
Let $\mathcal{G}$ be an $\epsilon$-neighborhood centered at $\boldsymbol{\theta}_0$ for some $\epsilon > 0$, i.e. $\mathcal{G} = \{ \boldsymbol{\theta} \in \boldsymbol{\Theta}: || \boldsymbol{\theta} - \boldsymbol{\theta}_0 ||_2 < \epsilon\}$ for some $\epsilon > 0$.

We want to show $\hat{\boldsymbol{\theta}}  \overset{\mathcal{P}}{\mapsto}  \boldsymbol{\theta}_0$, which is equivalent to show
\begin{equation*}
\underset{n \to \infty}{\text{lim}} \mathbb{P}(||\boldsymbol{\theta} - \boldsymbol{\theta}_0 ||_2 \geq \epsilon) = \underset{n \to \infty}{\text{lim}} \mathbb{P}(\hat{\boldsymbol{\theta}} \notin \mathcal{G}) = 0
\end{equation*}

Define $\gamma = Q_0(\boldsymbol{\theta}_0) - \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta} \setminus \mathcal{G}}{\text{sup}} Q_0({\boldsymbol{\theta}}) > 0 \;\;$ by condition C1.

So $\hat{\boldsymbol{\theta}} \notin \mathcal{G}$ implies that $Q_0(\hat{\boldsymbol{\theta}}) \leq \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta} \setminus \mathcal{G}}{\text{sup}} Q_0({\boldsymbol{\theta}}) = Q_0(\boldsymbol{\theta}_0) - \gamma$.

Let $\mathcal{A} = \{ Q_0(\hat{\boldsymbol{\theta}}) \leq Q_0(\boldsymbol{\theta}_0) - \gamma  \}$. So $\underset{n \to \infty}{\text{lim}}\mathbb{P}(\hat{\boldsymbol{\theta}} \notin \mathcal{G}) \leq \underset{n \to \infty}{\text{lim}}\mathbb{P}(\mathcal{A})$.

Now we define the following events: 
\begin{align*}
\mathcal{B} &= \{|\hat{Q_n}(\hat{\boldsymbol{\theta}}) - Q_0(\hat{\boldsymbol{\theta}})| > \gamma/3  \}, \\
\mathcal{C} &= \{|\hat{Q_n}(\boldsymbol{\theta}_0) - Q_0(\boldsymbol{\theta}_0)| > \gamma/3 \}, \\
\mathcal{D} &= \{\underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{sup}} |\hat{Q_n}(\boldsymbol{\theta}) - Q_0(\boldsymbol{\theta})| >\gamma/3 \}.
\end{align*}

So we have 
\begin{align*}
\mathbb{P} (\mathcal{A}) &\leq \mathbb{P} (\mathcal{A} \cup (\mathcal{B} \cup \mathcal{C})) + \mathbb{P}(\mathcal{B} \cup \mathcal{C}) \\
&= \mathbb{P}(\mathcal{A} \cap \mathcal{B}^c \cap \mathcal{C}^c) + \mathbb{P}(\mathcal{B} \cup \mathcal{C}) \\
&= \mathbb{P}(\emptyset) + \mathbb{P}(\mathcal{B} \cup \mathcal{C}) \\
&= \mathbb{P}(\mathcal{B} \cup \mathcal{C}).
\end{align*}

Notice that $\mathcal{A} \cap \mathcal{B}^c \cap \mathcal{C}^c = \emptyset$ because if $\mathcal{A}$, $\mathcal{B}^c$, and $\mathcal{C}^c$ hold simultaneously, then

\begin{align*}
\hat{Q_n}(\hat{\boldsymbol{\theta}}) &= \hat{Q_n}(\hat{\boldsymbol{\theta}}) - Q_0(\hat{\boldsymbol{\theta}}) + Q_0(\hat{\boldsymbol{\theta}}) \\
&\leq |\hat{Q_n}(\hat{\boldsymbol{\theta}}) - Q_0(\hat{\boldsymbol{\theta}})| + Q_0(\hat{\boldsymbol{\theta}}) \\
&\leq \gamma/3 + Q_0(\hat{\boldsymbol{\theta}}) \;\;\;\;\;\; \text{since} \;\; \mathcal{B}^c \;\; \text{holds}\\
&\leq Q_0({\boldsymbol{\theta}}_0) - 2\gamma/3 \;\;\;\;\;\; \text{since} \;\; \mathcal{A} \;\; \text{holds}\\
&\leq Q_0({\boldsymbol{\theta}}_0) - \hat{Q_n}(\hat{\boldsymbol{\theta}}_0) + \hat{Q_n}(\hat{\boldsymbol{\theta}}_0) - 2\gamma/3 \\
&\leq |Q_0({\boldsymbol{\theta}}_0) - \hat{Q_n}({\boldsymbol{\theta}}_0)| + \hat{Q_n}({\boldsymbol{\theta}}_0) - 2\gamma/3 \\
&\leq \hat{Q_n}({\boldsymbol{\theta}}_0)  - \gamma/3 \;\;\;\;\;\; \text{since} \;\; \mathcal{C}^c \;\; \text{holds}\\
&< \hat{Q_n}({\boldsymbol{\theta}}_0)
\end{align*}
which contradicts that $\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \hat{Q_n}({\boldsymbol{\theta}})$.

So 
\begin{align*}
\mathbb{P}(\mathcal{A}) &= \mathbb{P}(\mathcal{B} \cup \mathcal{C}) \\
&= \mathbb{P}(|\hat{Q_n}(\hat{\boldsymbol{\theta}}) - Q_0(\hat{\boldsymbol{\theta}})| > \gamma/3 \;\; \text{or} \;\; |\hat{Q_n}(\boldsymbol{\theta}_0) - Q_0(\boldsymbol{\theta}_0)| > \gamma/3) \\
&= \mathbb{P} (\underset{\boldsymbol{\theta} \in  \{\hat{\boldsymbol{\theta}}, \boldsymbol{\theta}_0\} }{\text{sup}} |\hat{Q_n}(\boldsymbol{\theta}) - Q_0(\boldsymbol{\theta})| > \gamma/3) \\
& \leq \mathbb{P} (\underset{\boldsymbol{\theta} \in  \boldsymbol{\Theta} }{\text{sup}} |\hat{Q_n}(\boldsymbol{\theta}) - Q_0(\boldsymbol{\theta})| > \gamma/3) \\
&= \mathbb{P}(\mathcal{D}).
\end{align*}

So by condition C4, we have
\begin{equation*}
\underset{n \to \infty}{\text{lim}}\mathbb{P}(\hat{\boldsymbol{\theta}} \notin \mathcal{G}) \leq \underset{n \to \infty}{\text{lim}} \mathbb{P}(\mathcal{A}) \leq \underset{n \to \infty}{\text{lim}}\mathbb{P}(\mathcal{D}) = 0.
\end{equation*}

Therefore, $\underset{n \to \infty}{\text{lim}}\mathbb{P}(\hat{\boldsymbol{\theta}} \notin \mathcal{G}) = 0$.
```

### Verification of Condition C1 
In general, the verification of Condition C1 is difficult and is often assumed in the statistical literature. Here we present two results based on @newey1994vlarge and @komunjer2012global respectively, which allow us to verify the condition C1 for GMM-type estimators. A discussion on the verification of this condition for other estimators can for example be found in Chapter 7 of @baltagi2008companion. 

```{lemma, GMMindentification, name = "GMM Identification"}
If $\boldsymbol{W} > 0$ (i.e. positive definite), where $\boldsymbol{W}$ is such that $\widehat{\boldsymbol{W}} \overset{\mathcal{P}}{\mapsto} \boldsymbol{W}$, and for $\boldsymbol{g}_0(\boldsymbol{\theta}) = \mathbb{E}[\boldsymbol{g}(z|\boldsymbol{\theta})]$, we have $\boldsymbol{g}_0(\boldsymbol{\theta}_0)=0$ and $\boldsymbol{g}_0(\boldsymbol{\theta}) \neq 0$ if $\boldsymbol{\theta} \neq \boldsymbol{\theta}_0$, 
then $Q_0(\boldsymbol{\theta}) = -\boldsymbol{g}_0(\boldsymbol{\theta})^T \boldsymbol{W} \boldsymbol{g}_0(\boldsymbol{\theta})$ has a unique maximum at $\boldsymbol{\theta}_0$.
```

```{proof, proofGMMindentification, name = "GMM Identification"}
Since $\boldsymbol{W} >0$ and there exists a unique $\boldsymbol{\theta}_0 \in \boldsymbol{\Theta}$ such that $\boldsymbol{g}_0(\boldsymbol{\theta}) = 0$, we can say that for all $\boldsymbol{\theta} \in \boldsymbol{\Theta}$,
\begin{equation*}
Q_0(\boldsymbol{\theta}) = -\boldsymbol{g}_0(\boldsymbol{\theta})^T \boldsymbol{W} \boldsymbol{g}_0(\boldsymbol{\theta}) \leq -\boldsymbol{g}_0(\boldsymbol{\theta}_0) \boldsymbol{W} \boldsymbol{g}_0(\boldsymbol{\theta}_0) = Q_0(\boldsymbol{\theta_0}).
\end{equation*}
```


Notice that if we write a GMM estimator as in Example \@ref(exm:ExtreEstGMM), then the condition $\boldsymbol{g}_0(\boldsymbol{\theta}) = 0$ if and only if $\boldsymbol{\theta} = \boldsymbol{\theta}_0$ can be replaced by $\boldsymbol{\gamma}(\boldsymbol{\theta}) = \boldsymbol{\gamma}(\boldsymbol{\theta}_0)$ if and only if $\boldsymbol{\theta} = \boldsymbol{\theta}_0$. 


In order to verify the condition in Lemma \@ref(lem:GMMindentification) that $\boldsymbol{g}_0(\boldsymbol{\theta}) = 0$ if and only if $\boldsymbol{\theta} = \boldsymbol{\theta}_0$ (or alternatively $\boldsymbol{\gamma}(\boldsymbol{\theta}) = \boldsymbol{\gamma}(\boldsymbol{\theta}_0)$ if and only if $\boldsymbol{\theta} = \boldsymbol{\theta}_0$), the following theorem based on @komunjer2012global provides us a way.


```{theorem, Homomorphism, name = "Homomorphism"}
Let $\boldsymbol{\theta} \in \boldsymbol{\Theta} \subset \mathbb{R}^p$. Let $\boldsymbol{g}^*(\boldsymbol{\theta})$ denote a subset of $p$ elements of $\boldsymbol{g}_0 (\boldsymbol{\theta}) \in \mathbb{R}^q, \; q \geq p$ such that:

- $\boldsymbol{g}^*(\boldsymbol{\theta})$ is in $\mathcal{C}^2$ (i.e. $\boldsymbol{g}^*(\boldsymbol{\theta})$ can be differentiated twice);
- For every $\boldsymbol{\theta} \in \boldsymbol{\Theta}$, $J(\boldsymbol{\theta})$ is non-negative (or alternatively non-positive), where $J(\boldsymbol{\theta}) \equiv \text{det} (\frac{\partial}{\partial \boldsymbol{\theta}^T}\, \boldsymbol{g}^*(\boldsymbol{\theta}) )$;
- $||\boldsymbol{g}^*(\boldsymbol{\theta})|| \to \infty$ whenever $|| \boldsymbol{\theta} || \to \infty$;
- For every $s \in \mathbb{R}^p$ the equation $\boldsymbol{g}^*(\boldsymbol{\theta}) = s$ has countably many (possibly zero) solutions in $\boldsymbol{\Theta}$;

then $\boldsymbol{g}^*(\boldsymbol{\theta})$ is a homeomorphism (i.e. $\boldsymbol{g}^*(\boldsymbol{\theta})$ is continuous and one-to-one).
```


- A direct consequence of Lemma \@ref(lem:GMMindentification) and Theorem \@ref(thm:Homomorphism) is that any GMM estimator with $\boldsymbol{W} > 0$ satisfying the conditions of Theorem \@ref(thm:Homomorphism) verifies Condition C1 of Theorem \@ref(thm:ConsExtreEst).

- If one can show that $\boldsymbol{g}^c(\boldsymbol{\theta})$ is in $\mathcal{C}$ (where $\boldsymbol{g}^c(\boldsymbol{\theta})$ denotes the element of $\boldsymbol{g}_0(\boldsymbol{\theta})$ that are not in $\boldsymbol{g}^*(\boldsymbol{\theta})$) then Condition C3 of Theorem \@ref(thm:ConsExtreEst) is also verified.

- When considering a GMM estimator of the form used in Example \@ref(exm:ExtreEstGMM), one can simply verify the conditions of Theorem \@ref(thm:ConsExtreEst) with $\boldsymbol{g}(\boldsymbol{\theta}) = \boldsymbol{\gamma} (\boldsymbol{\theta})$.

- For Theorem \@ref(thm:Homomorphism), in @komunjer2012global it is actually assumed that $\boldsymbol{\theta} \in \mathbb{R}^p$ while we assume that $\boldsymbol{\theta} \in \boldsymbol{\Theta}$. We use this simplification to avoid an overly technical treatment of this topic. In fact, we assume here that there exists a one-to-one function $h(\cdot)$ such that $h \, : \,\mathbb{R}^p  \mapsto \boldsymbol{\Theta}$. This condition is typically verified in practice.

```{example, exmC1, name = "An Example to Verify Condition C1"}
In this example we revisit Example \@ref(exm:ExtreEstGMM). Let us say that $\widehat{\boldsymbol{W}}$ is such that $\widehat{\boldsymbol{W}} \overset{\mathcal{P}}{\mapsto} \boldsymbol{W} > 0$. 

We have shown that 
\begin{equation*}
		\boldsymbol{\gamma}(\boldsymbol{\theta}) = 	\begin{bmatrix}
			 \mu\\
			\mu^2 + \sigma^2\\
			\mu^3 + 3 \mu \sigma^2
	 		 \end{bmatrix}.
\end{equation*}

So we define that 
\begin{equation*}
    \boldsymbol{g}^*(\boldsymbol{\theta}) = \begin{bmatrix}
			 \mu\\
			\mu^2 + \sigma^2\\
	 		 \end{bmatrix}
	 		 \;\;\;\;\; \text{and} \;\;\;\;\; g^c(\boldsymbol{\theta}) = \begin{bmatrix}
			 \mu^3 + 3 \mu \sigma^2\\
	 		 \end{bmatrix}.
\end{equation*}

Since the elements of $\boldsymbol{g}^*(\boldsymbol{\theta})$ are polynomial in $\boldsymbol{\Theta}$, the condition $\boldsymbol{g}^*(\boldsymbol{\theta}) \in \mathcal{C}^2$ is trivially satisfied.

Next, since 
\begin{equation*}
		\frac{\partial}{\partial \boldsymbol{\theta}^T}\, \boldsymbol{g}^*(\boldsymbol{\theta}) = 	\begin{bmatrix}
			 1 & 0\\
			2\mu & 1
	 		 \end{bmatrix},
\end{equation*}
we have $J(\boldsymbol{\theta}) \equiv \text{det} (\frac{\partial}{\partial \boldsymbol{\theta}^T}\, \boldsymbol{g}^*(\boldsymbol{\theta}) ) = 1$.

Finally, the last two conditions of Theorem \@ref(thm:Homomorphism) are trivially satisfied since $||\boldsymbol{g}^*(\boldsymbol{\theta})||$ can only diverge if $|| \boldsymbol{\theta} ||$ diverges and since $\boldsymbol{g}^*(\boldsymbol{\theta}) = s$ has (one) countably solutions in $\boldsymbol{\Theta}$.

Therefore, it follows from Lemma \@ref(lem:GMMindentification) and Theorem \@ref(thm:Homomorphism) that the function $Q_0(\boldsymbol{\theta}) = - || \boldsymbol{\gamma}(\boldsymbol{\theta}_0)  - \boldsymbol{\gamma}(\boldsymbol{\theta})  ||^2_{\boldsymbol{W}}$ is uniquely maximized in $\boldsymbol{\theta}_0$.
```

### Verification of Condition C4
In general, the verification of Condition C4 requires pointwise convergence and Lipschitz continuity as specified in the following theorem, which is a slightly adapted version of the Arzela-Ascoli Theorem.

```{theorem, ArzelaAscoli, name = "modified Arzela-Ascoli Theorem"}
Suppose $\boldsymbol{\Theta}$ is compact. If

- for every $\boldsymbol{\theta} \in \boldsymbol{\Theta}$, we have $\hat{Q}_n (\boldsymbol{\theta}) \overset{\mathcal{P}}{\mapsto} Q_0 (\boldsymbol{\theta})$,

- $\hat{Q}_n (\boldsymbol{\theta})$ is almost surely Lipschitz continuous, i.e. $\underset{\boldsymbol{\theta}_1, \boldsymbol{\theta}_2 \in \boldsymbol{\Theta}}{\text{sup}} |\hat{Q}_n (\boldsymbol{\theta}_1) - \hat{Q}_n (\boldsymbol{\theta}_2)| \leq H ||\boldsymbol{\theta}_1 - \boldsymbol{\theta}_2||$ where $H$ is bounded almost surely,

then $\hat{Q}_n (\boldsymbol{\theta})$ converges uniformly in probability to $Q_0 (\boldsymbol{\theta})$.
```

Here we will not discuss how to prove that $\hat{Q}_n (\boldsymbol{\theta})$ is almost surely Lipschitz continuous and assume it for simplicity (more discussion on this topic can for example be found in @newey1994vlarge). Nevertheless, it worth mentioning that this condition is almost always satisfied in practice and is therefore reasonable to assume for simplicity. 

```{example, exmC4, name = "An Example to Verify Condition C4"}
In this example we revisit Example \@ref(exm:ExtreEstGMM). Since we have $Z_i \overset{iid}{\sim} \mathcal{N}(\mu_0, \sigma^2_0)$, we let
\begin{equation*}
    \boldsymbol{X}_i \equiv  \begin{bmatrix}
    Z_i\\
    Z_i^2\\
    Z_i^3
    \end{bmatrix}.
\end{equation*}
So by the Weak Law of Large Number (Theorem \@ref(thm:WLLN)), we have
\begin{equation*}
    \hat{\boldsymbol{\gamma}} = \frac{1}{n} \sum_{i = 1}^n \boldsymbol{X}_i  \overset{\mathcal{P}}{\mapsto} \mathbb{E}[\boldsymbol{X}_i] = \boldsymbol{\gamma}({\boldsymbol{\theta}_0}) \equiv \boldsymbol{\gamma}_0.
\end{equation*}

Define 
\begin{equation*}
\hat{Q}_n (\boldsymbol{\theta}) = -||\hat{\boldsymbol{\gamma}}- \boldsymbol{\gamma} (\boldsymbol{\theta}) ||_{\widehat{\boldsymbol{W}}}^2
\end{equation*}
where $\widehat{\boldsymbol{W}} \overset{\mathcal{P}}{\mapsto} \boldsymbol{W}$, and also define
\begin{equation*}
Q_0(\boldsymbol{\theta}) = -||\boldsymbol{\gamma}(\boldsymbol{\theta}_0) - \boldsymbol{\gamma}(\boldsymbol{\theta})||_\boldsymbol{W}^2.
\end{equation*}

In order to show that $\hat{Q}_n (\boldsymbol{\theta}) \overset{\mathcal{P}}{\mapsto} Q_0(\boldsymbol{\theta})$ for all $\boldsymbol{\theta} \in \boldsymbol{\Theta}$, we want to show that $|\hat{Q}_n (\boldsymbol{\theta}) - Q_0(\boldsymbol{\theta})| \overset{\mathcal{P}}{\mapsto} 0$.

\begin{equation*}
    \begin{aligned}
         \hat{Q}_n (\boldsymbol{\theta}) - {Q}_0 (\boldsymbol{\theta}) &\leq |\hat{Q}_n (\boldsymbol{\theta}) - {Q}_0 (\boldsymbol{\theta})| =   \left|\underbrace{ ||\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma} (\boldsymbol{\theta}) ||_{\widehat{\boldsymbol{W}}}^2 - ||\gamma_0 - \boldsymbol{\gamma} (\boldsymbol{\theta}) ||_\boldsymbol{W}^2}_\boldsymbol{A}\right|.
    \end{aligned}
\end{equation*}

Without loss of generality, we assume that $\boldsymbol{W}^* = \widehat{\boldsymbol{W}} - \boldsymbol{W}$ and that $\boldsymbol{W}$ is symmetric.

\begin{align*}
\boldsymbol{A} &=  \hat{\boldsymbol{\gamma}}^T \widehat{\boldsymbol{W}}\hat{\boldsymbol{\gamma}} + \boldsymbol{\gamma}(\boldsymbol{\theta})^T \widehat{\boldsymbol{W}} \boldsymbol{\gamma}(\boldsymbol{\theta}) - 2\hat{\boldsymbol{\gamma}}^T\widehat{\boldsymbol{W}}\boldsymbol{\gamma}(\boldsymbol{\theta}) - \boldsymbol{\gamma}_0^T \boldsymbol{W} \boldsymbol{\gamma}_0 - \boldsymbol{\gamma}(\boldsymbol{\theta})^T \boldsymbol{W} \boldsymbol{\gamma}(\boldsymbol{\theta}) + 2\boldsymbol{\gamma}_0^T \boldsymbol{W} \boldsymbol{\gamma}(\boldsymbol{\theta})  \\ 
&= ||\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}_0||_{\widehat{\boldsymbol{W}}}^2 - \boldsymbol{\gamma}_0^T \widehat{\boldsymbol{W}} \boldsymbol{\gamma}_0 + 2\widehat{\boldsymbol{\gamma}}^T \widehat{\boldsymbol{W}} \boldsymbol{\gamma}_0 \\
&+ \boldsymbol{\gamma}(\boldsymbol{\theta})^T \widehat{\boldsymbol{W}} \boldsymbol{\gamma}(\boldsymbol{\theta}) - 2\hat{\boldsymbol{\gamma}}^T\widehat{\boldsymbol{W}}\boldsymbol{\gamma}(\boldsymbol{\theta}) - \boldsymbol{\gamma}_0^T \boldsymbol{W} \boldsymbol{\gamma}_0 - \boldsymbol{\gamma}(\boldsymbol{\theta})^T \boldsymbol{W} \boldsymbol{\gamma}(\boldsymbol{\theta}) + 2\boldsymbol{\gamma}_0^T \boldsymbol{W} \boldsymbol{\gamma}(\boldsymbol{\theta})  \\ 
&= ||\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}_0||_{\widehat{\boldsymbol{W}}}^2 + ||\boldsymbol{\gamma}_0 - \boldsymbol{\gamma}(\boldsymbol{\theta})||_{\boldsymbol{W}^*}^2 \\
&- 2\boldsymbol{\gamma}_0^T\widehat{\boldsymbol{W}}\boldsymbol{\gamma}_0 - 2\hat{\boldsymbol{\gamma}}^T \widehat{\boldsymbol{W}} \boldsymbol{\gamma}(\boldsymbol{\theta}) + 2\boldsymbol{\gamma}_0^T \widehat{\boldsymbol{W}} \boldsymbol{\gamma}(\boldsymbol{\theta}) + 2\hat{\boldsymbol{\gamma}}^T \widehat{\boldsymbol{W}} \boldsymbol{\gamma}_0 \\
&= ||\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}_0||_{\widehat{\boldsymbol{W}}}^2 + ||\boldsymbol{\gamma}_0 - \boldsymbol{\gamma}(\boldsymbol{\theta})||_{\boldsymbol{W}^*}^2 + 2(\boldsymbol{\gamma}_0 - \boldsymbol{\gamma}(\boldsymbol{\theta}))^T \widehat{\boldsymbol{W}} (\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}_0).
\end{align*}

So by triangular inequality, we have 
\begin{align*}
|\hat{Q}_n (\boldsymbol{\theta}) - {Q}_0 (\boldsymbol{\theta})| &\leq  \left| \underbrace{\|\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}_0)  \|_{\widehat{\boldsymbol{W}}}^2}_{\equiv a_1} \right| + \left| \underbrace{\|\boldsymbol{\gamma}(\boldsymbol{\theta}_0) - \boldsymbol{\gamma} (\boldsymbol{\theta}) \|_{{\boldsymbol{W}}^*}^2}_{\equiv a_2} \right| \\ 
&+ \left| \underbrace{2 \left( \boldsymbol{\gamma}(\boldsymbol{\theta}_0) - \boldsymbol{\gamma} (\boldsymbol{\theta})\right)^T \widehat{\boldsymbol{W}} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}_0)\right)}_{\equiv a_3}\right|. 
\end{align*}

Before continuing, suppose that  $\boldsymbol{x}$ is a vector and $\boldsymbol{W}$ the above defined matrix, we have that $\boldsymbol{x}^T \boldsymbol{W} \boldsymbol{x} \leq \lambda_1 ||\boldsymbol{x}||^2$, where $\lambda_1$ is the largest eigenvalue of $\boldsymbol{W}$. Furthermore, let us also define the Frobenius norm of a matrix as $||\boldsymbol{W}|| = (\sum_i^N \sum_j^J w_{i,j}^2)^{\frac{1}{2}}$ and $||\boldsymbol{W}||^2 = \sigma_{\text{max}} \leq ||\boldsymbol{W}||$, where $\sigma_{\text{max}}$ is the largest singular value of $||\boldsymbol{W}||$.

Using those properties, we can investigate the terms in the above equation. 

Considering $a_1$ , we have 
\begin{align*}
a_1 &\leq ||\boldsymbol{\gamma}_0 - \boldsymbol{\gamma} (\boldsymbol{\theta}) ||^2 \lambda_1  \leq ||\boldsymbol{\gamma}_0 - \boldsymbol{\gamma} (\boldsymbol{\theta}) ||^2 \|\boldsymbol{W}^*|| \\ 
&= \sum^3_{i = 1} \left(\boldsymbol{\gamma}_{0_i} - \boldsymbol{\gamma}_i (\boldsymbol{\theta})\right)^2  \sqrt{\sum^3_{i = 1} \sum^3_{j = 1} \left(\hat{w}_{i,j} - w_{i,j}\right)^2} \\
&\leq 3 \underset{i}{\text{max}}  \left(\boldsymbol{\gamma}_{0_i} - \boldsymbol{\gamma}_i (\boldsymbol{\theta})\right)^2 \cdot 3 \underset{i}{\text{max}} \sqrt{\left(\hat{w}_{i,j} - w_{i,j}\right)^2}
\end{align*}

Since $\underset{i}{\text{max}}  \left(\boldsymbol{\gamma}_{0_i} - \boldsymbol{\gamma}_i (\boldsymbol{\theta})\right)^2$ is bounded by conditions C1 to C3, and $\underset{i}{\text{max}} \sqrt{\left(\hat{w}_{i,j} - w_{i,j}\right)^2} \overset{\mathcal{P}}{\mapsto} 0$ by $\widehat{\boldsymbol{W}} \overset{\mathcal{P}}{\mapsto} \boldsymbol{W}$, we show that $a_1 \overset{\mathcal{P}}{\mapsto} 0$.

Similarly, we can also show that $a_2 \overset{\mathcal{P}}{\mapsto} 0$ using the fact that he sample moments converge to the population ones.

Finally, considering the previous results on $a_1$ and $a_2$, we can see that the last term $a_3$ also tends to 0. Therefore, we can say that for all $\boldsymbol{\theta} \in \boldsymbol{\Theta}$, 

\begin{equation*}
|\hat{Q}_n (\boldsymbol{\theta}) - Q_0(\boldsymbol{\theta})| \overset{\mathcal{P}}{\mapsto} 0.
\end{equation*}
```

In general, showing that $\hat{Q}_n (\boldsymbol{\theta}) \overset{\mathcal{P}}{\mapsto} Q_0(\boldsymbol{\theta})$ for all $\boldsymbol{\theta} \in \boldsymbol{\Theta}$ for every $\boldsymbol{\theta} \in \boldsymbol{\Theta}$ is generally done using the Weak Law of Large Number (i.e. Theorem \@ref(thm:WLLN)). However, when $X_i$ which are not iid random variables, Theorem \@ref(thm:WLLN)) cannot be applied. The following theorem (taken from Proposition 7.5 of @hamilton1994time) generalizes this result for (weak) stationary processes with absolutely summable covariance structure. 

```{theorem, WLLNdep, name = "Weak Law of Large Number for Dependent Process"}
Suppose $(X_t)$ is a (weak) stationary process with absolutely summable autocovariance structure, then 
\begin{equation*}
\bar{X}_T \overset{\mathcal{P}}{\mapsto} \mathbb{E}[X_t].
\end{equation*}
```

```{proof, proofWLLNdep, name = "Weak Law of Large Number for Dependent Process"}
Since $\bar{X}_n - \mu$ has mean zero, its variance is
\begin{equation*}
\mathbb{E}[(\bar{X}_n - \mu)^2] = 1/n\sum_{h=-\infty}^{\infty} \gamma(h) \leq 1/n\sum_{h=-\infty}^{\infty} |\gamma(h)| \leq C/n.
\end{equation*}

By Chebychev's inequality, for all $\epsilon > 0$,
\begin{equation*}
\mathbb{P} \left(| \bar{X}_n - \mu | \geq \epsilon \right) \leq \frac{\mathbb{E}[(\bar{X}_n - \mu)^2]}{\epsilon^2} \leq \frac{C}{\epsilon^2n},
\end{equation*}
so that
\begin{equation*}
\underset{n \to \infty}{\text{lim}} \mathbb{P} \left(| \bar{X}_n - \mu | \geq \epsilon \right) \to 0,
\end{equation*}
which concludes the proof.
```


```{example, ConsistSampleMean, name = "Consistency of Sample Mean"}
Consider a stationary AR1 process and suppose we want to study whether its sample mean converges in probability to its expected value. This time we consider a non-zero mean AR1, i.e.
\begin{equation*}
    \left(X_t - \mu\right) = \phi \left(X_{t-1} - \mu\right) + Z_t,
\end{equation*}
where $\mu = \mathbb{E}[X_t]$, $Z_t \overset{iid}{\sim} \mathcal{N} (0, \nu^2)$ and $\nu^2 < \infty$. This process can also be written as a linear process:
\begin{equation*}
    X_i - \mu = \sum_{k=0}^{\infty}\phi^k Z_{i-k}.
\end{equation*}

Since the process is stationary for $|\phi| < 1$, we have
\begin{equation*}
\gamma_h = \frac{\nu^2 \phi^{|h|}}{1-\phi^2},
\end{equation*}
for $h \in \mathbb{Z}$.

Then we have 
\begin{equation*}
\sum_{h = -\infty}^{\infty} |\gamma_k| = \sum_{h = -\infty}^{\infty} \frac{\nu^2 |\phi|^{|h|}}{1-\phi^2} < 2 \lim_{n\to\infty}\frac{\nu^2 (1-|\phi|^{n+1})}{(1-\phi^2)(1-|\phi|)} = \frac{2\nu^2 }{(1-\phi^2)(1-|\phi|)} < \infty,
\end{equation*}
implying that the process has an absolutely summable covariance structure. Therefore, by applying Theorem \@ref(thm:WLLNdep), we can verify that
\begin{equation*}
    \bar{X}_T = \frac{1}{T} \sum_{t = 1}^T \, X_t \overset{\mathcal{P}}{\mapsto} \mathbb{E}[X_t] = \mu.
\end{equation*}
```


### Consistency of Sample AutoCovariance and AutoCorrelation Functions
In Chapter 2, we have defined the sample autocovariance andautocorrelation functions. In the following, we will show that these estimators are both consistent.

```{corollary, ConsistACVACF, name = "Consistency of Sample AutoCovariance and AutoCorrelation Functions"}
Let $(X_t)$ be such that $(X_t)$ is weakly stationary and $(X_t^2)$ has an absolutely summable covariance structure, then for all $|h| < \infty$, we have
\begin{align*}
\hat{\gamma}(h) &\overset{\mathcal{P}}{\mapsto} \gamma(h),\\
\hat{\rho}(h) &\overset{\mathcal{P}}{\mapsto} \rho(h).
\end{align*}
```

```{proof, proofConsistACVACF, name = "Consistency of Sample AutoCovariance and AutoCorrelation Functions"}
$(X_t^2)$ has an absolutely summable covariance structure implies that both $(X_t)$ and $(X_tX_{t-h})$ for all $h \in \mathbb{Z}$ have absolutely summable covariance structures. Under the conditions that $(X_t)$ is weakly stationary and has absolutely summable covariance structure, we have $\bar{X}_T \overset{\mathcal{P}}{\mapsto} \mu$. Let
\begin{equation*}
\tilde{\gamma}(h) = \frac{1}{T} \sum_{t = h+1}^{T} \left(X_{t} - \mu\right) \left(X_{t-h} - \mu\right).
\end{equation*}

Since $(\left(X_{t} - \mu\right) \left(X_{t-h} - \mu\right))$ is stationary and has absolutely summable covariance structure, we have $\tilde{\gamma}(h) \overset{\mathcal{P}}{\mapsto} \gamma(h)$. We can also show that $\sqrt{T} \left(\tilde{\gamma}(h) - \hat{\gamma}(h)  \right) = o_p(1)$. Therefore, we obtain
\begin{equation*}
\hat{\gamma}(h) \overset{\mathcal{P}}{\mapsto} \gamma(h).
\end{equation*}

Similarly, we can show that 
\begin{equation*}
\left( \hat{\gamma}(0), \hat{\gamma}(h) \right)^T \overset{\mathcal{P}}{\mapsto} \left( \gamma(0), \gamma(h) \right)^T.
\end{equation*}
	
Then by Theorem \@ref(thm:CMT), we have
\begin{equation*}
\hat{\rho}(h) \overset{\mathcal{P}}{\mapsto} \rho(h),
\end{equation*}
which concludes the proof.
```




## Asymptotic Normality
The Central Limit Theorem (CLT) takes one step further than the Law of Large Number. It identifies the limiting distribution of the (properly scaled) sum of random variables as the normal distribution, which allows us to do the statistical inference (confidence interval and hypothesis testing). The scale will tell us how fast this approximation converges to the normal distribution. 

### CLT for iid Random Variables
```{theorem, CLT, name = "CLT for iid sequences"}
Suppose $X_i$ are iid random variables with $\mathbb{E}[X_i] = \mu$ and $\text{Var}(X_i) = \sigma^2 < \infty$. Let $\bar{X}_n = \frac{1}{n} \sum_{i = 1}^n X_i$, then $\sqrt{n}\left(\bar{X}_n - \mu\right) \overset{\mathcal{D}}{\mapsto} \mathcal{N}(0, \sigma^2)$.
```

```{theorem, CLTmulti, name = "CLT for iid multivariate sequences"}
Suppose $\boldsymbol{X}_i$ are iid random vectors with $\mathbb{E}[\boldsymbol{X}_i] = \boldsymbol{\mu} \in \mathbb{R}^d$ and $\boldsymbol{\text{Cov}}(\boldsymbol{X}_i) = \boldsymbol{\Sigma} \in \mathbb{R}^{d \times d}$. Then, we have $\sqrt{n}\left(\bar{\boldsymbol{X}}_n - \boldsymbol{\mu}\right) \overset{\mathcal{D}}{\mapsto}  \mathcal{N}(\boldsymbol{0}, \boldsymbol{\Sigma})$.
```

```{theorem, slutsky, name = "Slutsky's Theorem"}
Let $X_n$, $Y_n$ and $X$ be random variables. If $X_n \overset{\mathcal{D}}{\mapsto} X$ and $Y_n \overset{\mathcal{P}}{\mapsto} c$ for some constant $c$, then

- $X_n + Y_n \overset{\mathcal{D}}{\mapsto} X+c$,
- $X_n Y_n \overset{\mathcal{D}}{\mapsto} cX$,
- $X_n / Y_n \overset{\mathcal{D}}{\mapsto} X/c$ if $c \neq 0$.
```


General results on the asymptotic normality of extremum estimators can be found in @newey1994vlarge. In this section, we will only restrict our attention to a simple example of GMM estimators as in Example \@ref(exm:ExtreEstGMM). We will see that the asymptotic normality of extremum estimators is implied by combining the following results and techniques:

- Consistency of $\hat{\boldsymbol{\theta}}$,
- Central Limit Theorem (see Theorem \@ref(thm:CLTmulti)),
- Slutsky's Theorem (see Theorem \@ref(thm:slutsky)),
- Taylor expansions.

```{example, exmAsympNormality, name = "An Example to Prove Asymptotic Normality of Extremum Estimators"}
In this example we revisit Example \@ref(exm:ExtreEstGMM). So we have 

\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \hat{Q}_n(\boldsymbol{\theta}) = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} -||\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}) ||_\widehat{\boldsymbol{W}}^2
\end{equation*}
where
\begin{equation*}
\hat{\boldsymbol{\gamma}} = \frac{1}{n}  \sum_{i = 1}^n 
\begin{bmatrix}
Z_i\\
Z_i^2\\
Z_i^3\\
\end{bmatrix}, \;\;\;\;
\boldsymbol{\gamma}(\boldsymbol{\theta}) = 	\begin{bmatrix}
\mu\\
\mu^2 + \sigma^2\\
\mu^3 + 3 \mu \sigma^2
\end{bmatrix}.
\end{equation*}

So by CLT (Theorem \@ref(thm:CLTmulti)), we have
\begin{equation*}
\sqrt{n}\left(\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}_0)\right)  = \frac{1}{\sqrt{n}}  \sum_{i = 1}^n 
	\begin{bmatrix}
	  Z_i - \mu_0\\
	 Z_i^2 - \mu_0^2 - \sigma_0^2\\
	 Z_i^3 - \mu_0^3 - 3 \mu_0 \sigma_0^2\\
	 \end{bmatrix} \overset{\mathcal{D}}{\mapsto} \mathcal{N}(\boldsymbol{0}, \boldsymbol{\Sigma}),
\end{equation*}

where $\boldsymbol{\Sigma} = \text{Cov}\left( \begin{bmatrix}
	  Z_i & Z_i^2& Z_i^3\\
	 \end{bmatrix}^T \right)$.
	 
Since $\hat{\boldsymbol{\theta}}$ maximizes $\hat{Q}_n(\boldsymbol{\theta})$, we have
\begin{equation*}
\frac{\partial \hat{Q}_n(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \bigg\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}} = \frac{\partial}{\partial \boldsymbol{\theta}} \left(\left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}) \right) \right)^T \widehat{\boldsymbol{W}} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}) \right) \bigg\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}} = \boldsymbol{0}.
\end{equation*}

Using Taylor expansion for $\boldsymbol{\gamma}(\hat{\boldsymbol{\theta}})$ around the true $\boldsymbol{\theta}_0$, we obtain
\begin{equation*}
\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\hat{\boldsymbol{\theta}})  = \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}_0) + \frac{\partial \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}) \right)}{\partial \boldsymbol{\theta}}\bigg\rvert_{\boldsymbol{\theta} = \boldsymbol{\theta}_0} \left( \hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0 \right) + o_p(1).
\end{equation*}

Under certain regularity conditions, we have (using Theorem \@ref(thm:CMT)) that

\begin{equation*}
\frac{\partial}{\partial \boldsymbol{\theta}} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}) \right)\bigg\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}} = - \frac{\partial}{\partial \boldsymbol{\theta}}  \boldsymbol{\gamma}(\boldsymbol{\theta}) \bigg\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}}  \overset{\mathcal{P}}{\mapsto} - \frac{\partial}{\partial \boldsymbol{\theta}}  \boldsymbol{\gamma}(\boldsymbol{\theta}) \bigg\rvert_{\boldsymbol{\theta} = \boldsymbol{\theta}_0}.
\end{equation*}

So under certain regularity conditions, using the above equations and Slutsky's Theorem, we can obtain that 

\begin{equation*}
\sqrt{n}\left( \hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0 \right) \overset{\mathcal{D}}{\mapsto}  \mathcal{N}\left(\boldsymbol{0}, \boldsymbol{D}^T \boldsymbol{\Sigma} \boldsymbol{D}\right),
\end{equation*}

where 

\begin{equation*}
\boldsymbol{D} = \left[ ||\frac{\partial}{\partial \boldsymbol{\theta}} \left(  \boldsymbol{\gamma}(\boldsymbol{\theta})\right)||_{\boldsymbol{W}}\rvert_{\boldsymbol{\theta} = \boldsymbol{\theta}_0} \right]^{-1} \left(\frac{\partial}{\partial \boldsymbol{\theta}}  \boldsymbol{\gamma}(\boldsymbol{\theta})\bigg\rvert_{\boldsymbol{\theta} = \boldsymbol{\theta}_0} \right)^T \boldsymbol{W}
\end{equation*}

due to the fact that 

\begin{equation*}
\begin{aligned}
&\sqrt{n}\left( \hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0 \right) 
        = \overbrace{-\left[ \left(\frac{\partial}{\partial \boldsymbol{\theta}} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}) \right)\bigg\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}} \right)^T \widehat{\boldsymbol{W}} \left( \frac{\partial}{\partial \boldsymbol{\theta}} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}) \right)\bigg\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}} \right) \right]^{-1}}^{ \overset{p}{\to} \left[ ||\frac{\partial}{\partial \boldsymbol{\theta}} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta})\right)||_{\boldsymbol{W}}\rvert_{\boldsymbol{\theta} = \boldsymbol{\theta}_0} \right]^{-1}}\\
&\underbrace{\left(\frac{\partial}{\partial \boldsymbol{\theta}} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}) \right)\bigg\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}} \right)^T \widehat{\boldsymbol{W}}}_{\overset{p}{\to} \left(\frac{\partial}{\partial \boldsymbol{\theta}} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta})\right)\rvert_{\boldsymbol{\theta} = \boldsymbol{\theta}_0} \right)^T \boldsymbol{W}} \sqrt{n} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}_0) \right) + o_p(1).
\end{aligned}
\end{equation*}
```


### CLT for Dependent Processes
For a dependent process, the validity of CLT requires the process to be "mixing" or "asymptotically independent". Suppose two events $G$ and $H$ are independent, then $$|\mathbb{P}(G \cap H) - \mathbb{P}(G) \mathbb{P}(H)| = 0.$$

Based on this idea, many dependence measures have been developed. The most commonly used $\alpha$-mixing coefficient is one of these dependence measures which can be easily verified for certain stochastic processes.

```{definition, mixcoef, name = "Mixing Coefficients"}
For a stochastic process $\{ X_i \}_{i \in \mathbb{Z}}$, we define the strong- or $\alpha$-mixing coefficients as

\begin{equation*}
\alpha(t_1, t_2) = \text{sup}\{ |\mathbb{P}(A \cap B) - \mathbb{P}(A)\mathbb{P}(B)|: \; A\in\mathcal{F}_{-\infty}^{t_1}, B\in\mathcal{F}_{t_2}^{\infty} \},
\end{equation*}
where $\mathcal{F}_{-\infty}^{t_1} = \sigma(X_{-\infty}, \dots, X_{t_1})$ and $\mathcal{F}_{t_2}^{\infty} = \sigma(X_{t_2}, \dots, X_{\infty})$ are $\sigma$-algebras generated by corresponding random variables.
```

```{exercise, MixCoefRemark, name = "Mixing Coefficients"}
If the process is stationary, then $\alpha(t_1, t_2) = \alpha(t_2, t_1) = \alpha(|t_1-t_2|) \equiv \alpha(\tau)$. If $\alpha(\tau) \to 0$ as $\tau \to \infty$, then the process is strong-mixing or $\alpha$-mixing.
```

```{theorem, CLTalpha, name = "Central Limit Theorem and Alpha-Mixing"}
Let $(X_t)$ be a strictly stationary process with $\mathbb{E}[X_t] =0$. $S_n \equiv \sum_{t=1}^{n}X_t$ is the partial sum process with $\sigma_n^2 \equiv \text{Var}(S_n)$. Suppose $(X_t)$ is $\alpha$-mixing, and that for $\delta > 0$, we have 

\begin{equation*}
\mathbb{E}\left[ |X_t|^{2+\delta} \right] \leq \infty, \mbox{ and } \sum_{n = 0}^{\infty} \alpha(n)^{\delta/2 + \delta} \leq \infty.
\end{equation*}

Then 
\begin{equation*}
\underset{n \to \infty}{\text{lim}}\frac{\sigma_n^2}{n} = \mathbb{E}\left[ |X_t|^{2} \right] + 2\sum_{k=1}^{\infty}\mathbb{E}\left[ X_1X_k \right] \equiv \sigma^2.
\end{equation*}

If $\sigma^2 > 0$, $(X_t)$ obeys both the Central Limit Theorem with variance $\sigma^2$, and the functional Central Limit Theorem.
```


```{exercise, ImpAlphaMix, name = "Implication of Alpha-Mixing"}
If a process $(X_t)$ is $\alpha$-mixing, then its covariance structure is absolutely summable.
```


### Asymptotic Normality of Sample AutoCovariance and AutoCorrelation Functions
If $(X_t)$ is a white noise, then $\hat{\rho}(h)$ should be equal to 0 if $h \neq 0$. In practice, this is of course not the case due the estimation error of $\hat{\rho}(h)$. The next result gives us a way to assess whether the data comes from a completely random series or whether correlations are statistically significant at some lags.

```{theorem, DistACF, name = "Distribution of Sample ACF in iid Case"}
If $(X_t)$ is white noise (with finite variance) and $h = 1, ..., H$ where $H$ is fixed but arbitrary, then we have that
\begin{equation*}
\sqrt{T} \left(\hat{\rho}(h) - {\rho}(h)\right) \overset{\mathcal{D}}{\mapsto}  \mathcal{N}\left(0, 1\right).
\end{equation*}
```

The proof of Theorem \@ref(thm:DistACF) is straightforward from the CLT and Delta method. It is therefore omitted here but can for example be found in @hamilton1994time.

Theorem \@ref(thm:DistACF) implies that an approximate confidence interval for $\hat{\rho}(h)$ (in the iid case) is given by
$$\text{CI}({\rho}(h), \alpha) = \hat{\rho}(h) \pm \frac{z_{1-\frac{\alpha}{2}} }{\sqrt{T}}$$
for $0 < h < k < \infty$ and where $z_{1- \frac{\alpha}{2}} \equiv \boldsymbol{\Phi}^{-1}\left( 1- \frac{\alpha}{2} \right)$ is the $(1- \frac{\alpha}{2})$ quantile of a standard normal distribution. Typically, for $\alpha = 0.05$ one would consider the following confidence interval:
$$\text{CI}({\rho}(h), 0.05) = \hat{\rho}(h) \pm \frac{2}{\sqrt{T}}.$$

<!--chapter:end:02-estimators.Rmd-->


# Allan Variance Calibration Techniques

Placeholder


## Spectral Ambiguity of the AV
## Properties of the Allan Variance
## Estimation
### Consistency
### Asymptotic Normality
### Confidence Interval of the MOAV Estimator
## Allan variance based estimation
### Allan Variance log-log Representation
### Allan Deviation of a WN process

<!--chapter:end:03-allanvariance.Rmd-->


# The Generalized Method of Wavelet Moments

Placeholder



<!--chapter:end:04-gmwm.Rmd-->

# Extensions

<!--chapter:end:05-extensions.Rmd-->

`r if (knitr:::is_html_output()) '
# References {-}
'`

<!--chapter:end:06-references.Rmd-->

