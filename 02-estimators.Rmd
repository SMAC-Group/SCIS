# Properties of Statistical Estimators

In this chapter, we will provide a review of the properties of statistical estimators. This chapter is organized with the following outline:

- Extremum estimators;
- Consistency;
- Asymptotic normality.

## Extremum Estimators
In this section, we will introduce a commonly used class of estimators, extremum estimators, and some examples of it. 

```{definition, EstreEst, name = "Extremum Estimators"}
Many estimators have a common structure, which is often useful to study their asymptotic properties. One structure or framework is the class of estimators that maximize some objective function, referred to as extremum estimators, which can can be defined as follows:
\begin{equation*}
\hat{\boldsymbol{\theta}} \equiv \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; \hat{Q}_n(\boldsymbol{\theta})
\end{equation*}
where $\boldsymbol{\theta}$ and $\boldsymbol{\Theta}$ denote, respectively, the parameter vector of interest and its set of possible values.
```

The vast majority of statistical estimators can be represented as extremum estimators. For example, least squares, maximum likelihood or (generalized) method of moment estimators can all be represented as extremum estimators.

```{example, ExtreEstLSE, name = "Least Squares Estimator as Extremum Estimator"}
Consider the linear model $\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta}_0 + \boldsymbol{\epsilon}$ where $\boldsymbol{X} \in \mathbb{R}^{n \times p}$ is a full rank constant matrix, $\boldsymbol{\beta} \in {\mathcal{B}} \subseteq \mathbb{R}^p$ and $\epsilon_i \overset{iid}{\sim} \mathcal{N}(0, \sigma_\epsilon^2)$. Let $\hat{\boldsymbol{\beta}}$ denote the Least Squares Estimator (LSE) of $\boldsymbol{\beta}_0$, i.e.
\begin{equation*}
\hat{\boldsymbol{\beta}} = \left(\boldsymbol{X}^T \boldsymbol{X} \right)^{-1} \boldsymbol{X}^T \boldsymbol{y}. 
\end{equation*}
This LSE is an extremum estimator since it can be expressed as 
\begin{equation*}
\hat{\boldsymbol{\beta}} = \underset{\boldsymbol{\beta} \in \mathcal{B}}{\text{argmax}} \; -||\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} ||_2^2.
\end{equation*}
```

```{example, ExtreEstMLE, name = "Maximum Likelihood Estimator as Extremum Estimator"}
Let $Z_1, \ldots, Z_n$ be an iid sample with pdf $f(z|\boldsymbol{\theta}_0)$. The Maximum Likelihood Estimator (MLE) is given by
\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; \frac{1}{n} \sum_{i = 1}^{n} \text{log}\left[f\left(z_i| \boldsymbol{\theta}\right)\right].
\end{equation*}

Here instead of the actual log-likelihood, we are actually using a normalized log-likelihood, which has no impact on the estimator but the normalized form is more convenient to use when we let $n \to \infty$.

Therefore, the MLE can be seen as an extremum estimator with 
\begin{equation*}
\hat{Q}_n(\boldsymbol{\theta}) = \frac{1}{n} \sum_{i = 1}^{n} \text{log}\left[f\left(z_i| \boldsymbol{\theta}\right)\right].
\end{equation*}
```

```{definition, GMM, name = "Generalized Method of Moments"}
Let $Z_1, \ldots, Z_n$ be an iid sample with pdf $f(z|\boldsymbol{\theta}_0)$. Suppose that there is a moment function vector $\boldsymbol{g}(z | \boldsymbol{\theta})$ such that $\mathbb{E}[\boldsymbol{g} (z | \boldsymbol{\theta}_0)] = 0$. Then the Generalized Method of Moments (GMM) estimator of $\boldsymbol{\theta}_0$ is defined as 
\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; - \left[\frac{1}{n} \sum_{i = 1}^n \boldsymbol{g}(z_i | \boldsymbol{\theta}) \right]^T \widehat{\boldsymbol{W}} \left[\frac{1}{n} \sum_{i = 1}^n \boldsymbol{g}(z_i | \boldsymbol{\theta}) \right],
\end{equation*}
where $\widehat{\boldsymbol{W}}$ is an positive definite matrix of appropriate dimension.

Alternatively (but equivalently), we can define GMM estimator as 
\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; - || \hat{\boldsymbol{\mu}} - \boldsymbol{\mu}(\boldsymbol{\theta}) ||_{\widehat{\boldsymbol{W}}}^2,
\end{equation*}
where $|| \boldsymbol{x} ||_{\boldsymbol{A}}^2 = \boldsymbol{x}^T \boldsymbol{A} \boldsymbol{x}$, and where $\hat{\boldsymbol{\mu}}$ and $\boldsymbol{\mu}(\boldsymbol{\theta})$ denote, respectively, the empirical and model based moments. We will use this form of definition of GMM estimator more often.
```

```{example, ExtreEstGMM, name = "A Simple GMM Estimator as Extremum Estimator"}
Let $Z_i \overset{iid}{\sim} \mathcal{N}(\mu_0, \sigma^2_0)$ and $\boldsymbol{\theta}_0 = (\mu_0, \sigma_0^2)^T$. Suppose we wish to estimate $\boldsymbol{\theta}_0$ by matching the first three empirical moments with their theoretical counterparts. In this case, a reasonable moment function or condition defining a GMM estimator is given by:
\begin{equation*}
\boldsymbol{g} (Z | \boldsymbol{\theta}) = \begin{bmatrix}
Z - \mu\\
Z^2 - \left(\mu^2 + \sigma^2\right)\\
Z^3 - \left(\mu^3 + 3 \mu \sigma^2\right)
\end{bmatrix}.
\end{equation*}

Notice that $\frac{1}{n} \sum_{i=1}^n \boldsymbol{g} (Z_i | \boldsymbol{\theta}) = \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta})$, where $\hat{\boldsymbol{\gamma}}$ and $\boldsymbol{\gamma}(\boldsymbol{\theta})$ denote, respectively, the empirical and model based moments, i.e.
\begin{equation*}
\hat{\boldsymbol{\gamma}} = \frac{1}{n}  \sum_{i = 1}^n 
\begin{bmatrix}
Z_i\\
Z_i^2\\
Z_i^3\\
\end{bmatrix}, \;\;\;\;
\boldsymbol{\gamma}(\boldsymbol{\theta}) = 	\begin{bmatrix}
\mu\\
\mu^2 + \sigma^2\\
\mu^3 + 3 \mu \sigma^2
\end{bmatrix}.
\end{equation*}

Therefore we can write the GMM estimator of $\boldsymbol{\theta}_0$ as
\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmin}} \; || \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma} (\boldsymbol{\theta}) ||_{\widehat{\boldsymbol{W}}}^2 = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \; - || \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma} (\boldsymbol{\theta}) ||_{\widehat{\boldsymbol{W}}}^2 
\end{equation*}
```


## Consistency
In this section, we will introduce one of the most important properties of statistical estimators, consistency. And we will discuss in details about the conditions for consistency of the extremum estimators. 

```{definition, consistency, name = "Consistency"}
An estimator $\hat{\boldsymbol{\theta}}$ is said to be consistent or weakly consistent if it converges in probability to $\boldsymbol{\theta}_0$, i.e.
\begin{equation*}
\underset{n \to \infty}{\text{lim}} \mathbb{P} (||\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0||_2 \geq \epsilon) = 0,
\end{equation*}
for all $\epsilon >0$.
```

In Layman's term, consistency simply means that if $n$ is large enough, then $\hat{\boldsymbol{\theta}}$ will be arbitrarily close to $\boldsymbol{\theta}_0$ (i.e. inside of an hypersphere of radius $\epsilon$ centered at $\boldsymbol{\theta}_0$). This also means the procedure (i.e. our estimator $\hat{\boldsymbol{\theta}}$) based on unlimited data will be able to identify the underlying truth (i.e. $\boldsymbol{\theta}_0$).


```{r, fig.height = 5, fig.width = 6.5, cache = TRUE, fig.align='center', out.width='80%', fig.cap='Interpretation of consistency', echo=FALSE}
knitr::include_graphics("images/consistency.pdf")
```

To show the consistency of estimators, we often make use of the following two important results:

```{theorem, WLLN, name = "Weak Law of Large Number"}
Suppose $X_i$ are iid random variables with finite mean $\mu$ (i.e. $\mathbb{E}[X_i] = \mu$) and finite variance. Let $\bar{X}_n = \frac{1}{n} \sum_{i = 1}^n X_i$, then $\bar{X}_n \overset{\mathcal{P}}{\mapsto} \mu$.
```

```{theorem, CMT, name = "Continuous Mapping Theorem"}
If $Y_n \overset{\mathcal{P}}{\mapsto} \mu$ and $g(\cdot)$ is a continuous function, then $g(Y_n) \overset{\mathcal{P}}{\mapsto} g(\mu)$.
```

Here we present a simple example to prove the consistency of an estimator with the above two results.

```{example, ConsistencyExpDist, name = "Consistency in Exponential Distribution"}
Suppose we have an iid sample from exponential distribution, i.e. $X_i \overset{iid}{\sim} \text{exp}(\lambda_0),\;  \lambda_0 \in \mathbb{R}^+, \; i = 1,..., n$. So assuming $X \geq 0$, the density of $X$ is given by 
\begin{equation*}
f(x|\lambda) = \lambda \text{exp}\left( - \lambda x \right).
\end{equation*}
In this example, we want to show that the MLE for $\lambda_0$ is a consistent estimator of $\lambda_0$.

First, we want to find the MLE for $\lambda_0$. The normalized log-likelihood function is given by
\begin{equation*}
\mathcal{L}(\lambda | X_1, ..., X_n) = \text{log}(\lambda) - \lambda \bar{X}_n.
\end{equation*}
By solving 
\begin{equation*}
\frac{\partial}{\partial \lambda} \; \mathcal{L}(\lambda | X_1, \ldots, X_n) = \frac{1}{\lambda} - \bar{X}_n = 0,
\end{equation*}
we obtain $\hat{\lambda} = \frac{1}{\bar{X}_n}$.
We verify that 
\begin{equation*}
\frac{\partial^2}{\partial \lambda^2} \; \mathcal{L}(\lambda | X_1, \ldots, X_n) = -\frac{1}{\lambda^2} < 0,
\end{equation*}
which implies that $\hat{\lambda}$ is the maxima of $\mathcal{L}(\lambda | X_1, \ldots, X_n)$. Therefore, the MLE for $\lambda_0$ is $\hat{\lambda} = \frac{1}{\bar{X}_n}$.

By Weak Law of Large Number, we have $\bar{X}_n \overset{\mathcal{P}}{\mapsto} \mu$, where \mu is given by 
\begin{equation*}
\mu = \mathbb{E}[X_i] = \int_{0}^{\infty} x \lambda_0 \text{exp}\left( - \lambda_0 x \right) dx = \frac{1}{\lambda_0}.
\end{equation*}
And also since the function $g(x) = 1/x$ is continuous in $\mathbb{R}^+$, we obtain by the Continuous Mapping Theorem that $\hat{\lambda} \overset{\mathcal{P}}{\mapsto} \lambda_0$, which concludes that the MLE for $\lambda_0$ is a consistent estimator of $\lambda_0$ in exponential distribution.
```


### Consistency of Extremum Estimators
When considering real-life problems, the above approach based on Weak Law of Large Number and Continuous Mapping Theorem is in general not flexible enough. Therefore, we often rely on the results as following. 

```{theorem, ConsExtreEst, name = "Consistency of Extremum Estimators"}
If there is a function ${Q}_0 (\boldsymbol{\theta})$ such that:

C1. ${Q}_0 (\boldsymbol{\theta})$ is uniquely maximized in $\boldsymbol{\theta}_0$,

C2. $\boldsymbol{\Theta}$ is compact,

C3. ${Q}_0 (\boldsymbol{\theta})$ is continuous in $\boldsymbol{\theta}$,

C4. $\hat{Q}_n (\boldsymbol{\theta})$ converges uniformly in probability to $Q_0 (\boldsymbol{\theta})$,

then we have $\hat{\boldsymbol{\theta}} \overset{\mathcal{P}}{\mapsto} \boldsymbol{\theta}_0$.
```

```{definition, compact, name = "Compactness"}
We say $\boldsymbol{\Theta}$ is compact if every open cover of $\boldsymbol{\Theta}$ contains a finite subcover. If $\boldsymbol{\Theta}$ is compact, then $\boldsymbol{\Theta}$ is closed (i.e. containing all its limit points) and bounded (i.e. all its points are within some finite distance of each other). 
```

```{definition, UnifCont, name = "Uniform Convergence in Probability"}
$\hat{Q}_n (\boldsymbol{\theta})$ is said to converges uniformly in probability to $Q_0 (\boldsymbol{\theta})$ if $\underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{sup}} \; |\hat{Q}_n (\boldsymbol{\theta}) - {Q}_0 (\boldsymbol{\theta})| \overset{\mathcal{P}}{\mapsto} 0$.  
```

Theorem \@ref(thm:ConsExtreEst) is an important result as it provides a general approach to prove the consistency of the class of extremum estimators. Notice that in this theorem:

- Condition (C1) is **substantive** and there are well-known examples where it fails. We will discuss further on how this assumption can (in some cases) be verified in practice.
- Condition (C2) is also **substantive** as it requires that there exist some known bounds on the parameters. In practice, this assumption is often neglected although it is in most cases unrealistic to assume it.
- Condition (C3) and (C4) are often referred to as **"standard regularity conditions"**. They are typically satisfied. The verification of these conditions will be discussed further later in this section.


```{proof, proofConsExtreEst, name="Consistency of Extremum Estimators"}
Let $\mathcal{G}$ be an $\epsilon$-neighborhood centered at $\boldsymbol{\theta}_0$ for some $\epsilon > 0$, i.e. $\mathcal{G} = \{ \boldsymbol{\theta} \in \boldsymbol{\Theta}: || \boldsymbol{\theta} - \boldsymbol{\theta}_0 ||_2 < \epsilon\}$ for some $\epsilon > 0$.

We want to show $\hat{\boldsymbol{\theta}}  \overset{\mathcal{P}}{\mapsto}  \boldsymbol{\theta}_0$, which is equivalent to show
\begin{equation*}
\underset{n \to \infty}{\text{lim}} \mathbb{P}(||\boldsymbol{\theta} - \boldsymbol{\theta}_0 ||_2 \geq \epsilon) = \underset{n \to \infty}{\text{lim}} \mathbb{P}(\hat{\boldsymbol{\theta}} \notin \mathcal{G}) = 0
\end{equation*}

Define $\gamma = Q_0(\boldsymbol{\theta}_0) - \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta} \setminus \mathcal{G}}{\text{sup}} Q_0({\boldsymbol{\theta}}) > 0 \;\;$ by condition C1.

So $\hat{\boldsymbol{\theta}} \notin \mathcal{G}$ implies that $Q_0(\hat{\boldsymbol{\theta}}) \leq \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta} \setminus \mathcal{G}}{\text{sup}} Q_0({\boldsymbol{\theta}}) = Q_0(\boldsymbol{\theta}_0) - \gamma$.

Let $\mathcal{A} = \{ Q_0(\hat{\boldsymbol{\theta}}) \leq Q_0(\boldsymbol{\theta}_0) - \gamma  \}$. So $\underset{n \to \infty}{\text{lim}}\mathbb{P}(\hat{\boldsymbol{\theta}} \notin \mathcal{G}) \leq \underset{n \to \infty}{\text{lim}}\mathbb{P}(\mathcal{A})$.

Now we define the following events: 
\begin{align*}
\mathcal{B} &= \{|\hat{Q_n}(\hat{\boldsymbol{\theta}}) - Q_0(\hat{\boldsymbol{\theta}})| > \gamma/3  \}, \\
\mathcal{C} &= \{|\hat{Q_n}(\boldsymbol{\theta}_0) - Q_0(\boldsymbol{\theta}_0)| > \gamma/3 \}, \\
\mathcal{D} &= \{\underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{sup}} |\hat{Q_n}(\boldsymbol{\theta}) - Q_0(\boldsymbol{\theta})| >\gamma/3 \}.
\end{align*}

So we have 
\begin{align*}
\mathbb{P} (\mathcal{A}) &\leq \mathbb{P} (\mathcal{A} \cup (\mathcal{B} \cup \mathcal{C})) + \mathbb{P}(\mathcal{B} \cup \mathcal{C}) \\
&= \mathbb{P}(\mathcal{A} \cap \mathcal{B}^c \cap \mathcal{C}^c) + \mathbb{P}(\mathcal{B} \cup \mathcal{C}) \\
&= \mathbb{P}(\emptyset) + \mathbb{P}(\mathcal{B} \cup \mathcal{C}) \\
&= \mathbb{P}(\mathcal{B} \cup \mathcal{C}).
\end{align*}

Notice that $\mathcal{A} \cap \mathcal{B}^c \cap \mathcal{C}^c = \emptyset$ because if $\mathcal{A}$, $\mathcal{B}^c$, and $\mathcal{C}^c$ hold simultaneously, then

\begin{align*}
\hat{Q_n}(\hat{\boldsymbol{\theta}}) &= \hat{Q_n}(\hat{\boldsymbol{\theta}}) - Q_0(\hat{\boldsymbol{\theta}}) + Q_0(\hat{\boldsymbol{\theta}}) \\
&\leq |\hat{Q_n}(\hat{\boldsymbol{\theta}}) - Q_0(\hat{\boldsymbol{\theta}})| + Q_0(\hat{\boldsymbol{\theta}}) \\
&\leq \gamma/3 + Q_0(\hat{\boldsymbol{\theta}}) \;\;\;\;\;\; \text{since} \;\; \mathcal{B}^c \;\; \text{holds}\\
&\leq Q_0({\boldsymbol{\theta}}_0) - 2\gamma/3 \;\;\;\;\;\; \text{since} \;\; \mathcal{A} \;\; \text{holds}\\
&\leq Q_0({\boldsymbol{\theta}}_0) - \hat{Q_n}(\hat{\boldsymbol{\theta}}_0) + \hat{Q_n}(\hat{\boldsymbol{\theta}}_0) - 2\gamma/3 \\
&\leq |Q_0({\boldsymbol{\theta}}_0) - \hat{Q_n}({\boldsymbol{\theta}}_0)| + \hat{Q_n}({\boldsymbol{\theta}}_0) - 2\gamma/3 \\
&\leq \hat{Q_n}({\boldsymbol{\theta}}_0)  - \gamma/3 \;\;\;\;\;\; \text{since} \;\; \mathcal{C}^c \;\; \text{holds}\\
&< \hat{Q_n}({\boldsymbol{\theta}}_0)
\end{align*}
which contradicts that $\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \hat{Q_n}({\boldsymbol{\theta}})$.

So 
\begin{align*}
\mathbb{P}(\mathcal{A}) &= \mathbb{P}(\mathcal{B} \cup \mathcal{C}) \\
&= \mathbb{P}(|\hat{Q_n}(\hat{\boldsymbol{\theta}}) - Q_0(\hat{\boldsymbol{\theta}})| > \gamma/3 \;\; \text{or} \;\; |\hat{Q_n}(\boldsymbol{\theta}_0) - Q_0(\boldsymbol{\theta}_0)| > \gamma/3) \\
&= \mathbb{P} (\underset{\boldsymbol{\theta} \in  \{\hat{\boldsymbol{\theta}}, \boldsymbol{\theta}_0\} }{\text{sup}} |\hat{Q_n}(\boldsymbol{\theta}) - Q_0(\boldsymbol{\theta})| > \gamma/3) \\
& \leq \mathbb{P} (\underset{\boldsymbol{\theta} \in  \boldsymbol{\Theta} }{\text{sup}} |\hat{Q_n}(\boldsymbol{\theta}) - Q_0(\boldsymbol{\theta})| > \gamma/3) \\
&= \mathbb{P}(\mathcal{D}).
\end{align*}

So by condition C4, we have
\begin{equation*}
\underset{n \to \infty}{\text{lim}}\mathbb{P}(\hat{\boldsymbol{\theta}} \notin \mathcal{G}) \leq \underset{n \to \infty}{\text{lim}} \mathbb{P}(\mathcal{A}) \leq \underset{n \to \infty}{\text{lim}}\mathbb{P}(\mathcal{D}) = 0.
\end{equation*}

Therefore, $\underset{n \to \infty}{\text{lim}}\mathbb{P}(\hat{\boldsymbol{\theta}} \notin \mathcal{G}) = 0$.
```

### Verification of Condition C1 
In general, the verification of Condition C1 is difficult and is often assumed in the statistical literature. Here we present two results based on @newey1994vlarge and @komunjer2012global respectively, which allow us to verify the condition C1 for GMM-type estimators. A discussion on the verification of this condition for other estimators can for example be found in Chapter 7 of @baltagi2008companion. 

```{lemma, GMMindentification, name = "GMM Identification"}
If $\boldsymbol{W} > 0$ (i.e. positive definite), where $\boldsymbol{W}$ is such that $\widehat{\boldsymbol{W}} \overset{\mathcal{P}}{\mapsto} \boldsymbol{W}$, and for $\boldsymbol{g}_0(\boldsymbol{\theta}) = \mathbb{E}[\boldsymbol{g}(z|\boldsymbol{\theta})]$, we have $\boldsymbol{g}_0(\boldsymbol{\theta}_0)=0$ and $\boldsymbol{g}_0(\boldsymbol{\theta}) \neq 0$ if $\boldsymbol{\theta} \neq \boldsymbol{\theta}_0$, 
then $Q_0(\boldsymbol{\theta}) = -\boldsymbol{g}_0(\boldsymbol{\theta})^T \boldsymbol{W} \boldsymbol{g}_0(\boldsymbol{\theta})$ has a unique maximum at $\boldsymbol{\theta}_0$.
```

```{proof, proofGMMindentification, name = "GMM Identification"}
Since $\boldsymbol{W} >0$ and there exists a unique $\boldsymbol{\theta}_0 \in \boldsymbol{\Theta}$ such that $\boldsymbol{g}_0(\boldsymbol{\theta}) = 0$, we can say that for all $\boldsymbol{\theta} \in \boldsymbol{\Theta}$,
\begin{equation*}
Q_0(\boldsymbol{\theta}) = -\boldsymbol{g}_0(\boldsymbol{\theta})^T \boldsymbol{W} \boldsymbol{g}_0(\boldsymbol{\theta}) \leq -\boldsymbol{g}_0(\boldsymbol{\theta}_0) \boldsymbol{W} \boldsymbol{g}_0(\boldsymbol{\theta}_0) = Q_0(\boldsymbol{\theta_0}).
\end{equation*}
```


Notice that if we write a GMM estimator as in Example \@ref(exm:ExtreEstGMM), then the condition $\boldsymbol{g}_0(\boldsymbol{\theta}) = 0$ if and only if $\boldsymbol{\theta} = \boldsymbol{\theta}_0$ can be replaced by $\boldsymbol{\gamma}(\boldsymbol{\theta}) = \boldsymbol{\gamma}(\boldsymbol{\theta}_0)$ if and only if $\boldsymbol{\theta} = \boldsymbol{\theta}_0$. 


In order to verify the condition in Lemma \@ref(lem:GMMindentification) that $\boldsymbol{g}_0(\boldsymbol{\theta}) = 0$ if and only if $\boldsymbol{\theta} = \boldsymbol{\theta}_0$ (or alternatively $\boldsymbol{\gamma}(\boldsymbol{\theta}) = \boldsymbol{\gamma}(\boldsymbol{\theta}_0)$ if and only if $\boldsymbol{\theta} = \boldsymbol{\theta}_0$), the following theorem based on @komunjer2012global provides us a way.


```{theorem, Homomorphism, name = "Homomorphism"}
Let $\boldsymbol{\theta} \in \boldsymbol{\Theta} \subset \mathbb{R}^p$. Let $\boldsymbol{g}^*(\boldsymbol{\theta})$ denote a subset of $p$ elements of $\boldsymbol{g}_0 (\boldsymbol{\theta}) \in \mathbb{R}^q, \; q \geq p$ such that:

- $\boldsymbol{g}^*(\boldsymbol{\theta})$ is in $\mathcal{C}^2$ (i.e. $\boldsymbol{g}^*(\boldsymbol{\theta})$ can be differentiated twice);
- For every $\boldsymbol{\theta} \in \boldsymbol{\Theta}$, $J(\boldsymbol{\theta})$ is non-negative (or alternatively non-positive), where $J(\boldsymbol{\theta}) \equiv \text{det} (\frac{\partial}{\partial \boldsymbol{\theta}^T}\, \boldsymbol{g}^*(\boldsymbol{\theta}) )$;
- $||\boldsymbol{g}^*(\boldsymbol{\theta})|| \to \infty$ whenever $|| \boldsymbol{\theta} || \to \infty$;
- For every $s \in \mathbb{R}^p$ the equation $\boldsymbol{g}^*(\boldsymbol{\theta}) = s$ has countably many (possibly zero) solutions in $\boldsymbol{\Theta}$;

then $\boldsymbol{g}^*(\boldsymbol{\theta})$ is a homeomorphism (i.e. $\boldsymbol{g}^*(\boldsymbol{\theta})$ is continuous and one-to-one).
```


- A direct consequence of Lemma \@ref(lem:GMMindentification) and Theorem \@ref(thm:Homomorphism) is that any GMM estimator with $\boldsymbol{W} > 0$ satisfying the conditions of Theorem \@ref(thm:Homomorphism) verifies Condition C1 of Theorem \@ref(thm:ConsExtreEst).

- If one can show that $\boldsymbol{g}^c(\boldsymbol{\theta})$ is in $\mathcal{C}$ (where $\boldsymbol{g}^c(\boldsymbol{\theta})$ denotes the element of $\boldsymbol{g}_0(\boldsymbol{\theta})$ that are not in $\boldsymbol{g}^*(\boldsymbol{\theta})$) then Condition C3 of Theorem \@ref(thm:ConsExtreEst) is also verified.

- When considering a GMM estimator of the form used in Example \@ref(exm:ExtreEstGMM), one can simply verify the conditions of Theorem \@ref(thm:ConsExtreEst) with $\boldsymbol{g}(\boldsymbol{\theta}) = \boldsymbol{\gamma} (\boldsymbol{\theta})$.

- For Theorem \@ref(thm:Homomorphism), in @komunjer2012global it is actually assumed that $\boldsymbol{\theta} \in \mathbb{R}^p$ while we assume that $\boldsymbol{\theta} \in \boldsymbol{\Theta}$. We use this simplification to avoid an overly technical treatment of this topic. In fact, we assume here that there exists a one-to-one function $h(\cdot)$ such that $h \, : \,\mathbb{R}^p  \mapsto \boldsymbol{\Theta}$. This condition is typically verified in practice.

```{example, exmC1, name = "An Example to Verify Condition C1"}
In this example we revisit Example \@ref(exm:ExtreEstGMM). Let us say that $\widehat{\boldsymbol{W}}$ is such that $\widehat{\boldsymbol{W}} \overset{\mathcal{P}}{\mapsto} \boldsymbol{W} > 0$. 

We have shown that 
\begin{equation*}
		\boldsymbol{\gamma}(\boldsymbol{\theta}) = 	\begin{bmatrix}
			 \mu\\
			\mu^2 + \sigma^2\\
			\mu^3 + 3 \mu \sigma^2
	 		 \end{bmatrix}.
\end{equation*}

So we define that 
\begin{equation*}
    \boldsymbol{g}^*(\boldsymbol{\theta}) = \begin{bmatrix}
			 \mu\\
			\mu^2 + \sigma^2\\
	 		 \end{bmatrix}
	 		 \;\;\;\;\; \text{and} \;\;\;\;\; g^c(\boldsymbol{\theta}) = \begin{bmatrix}
			 \mu^3 + 3 \mu \sigma^2\\
	 		 \end{bmatrix}.
\end{equation*}

Since the elements of $\boldsymbol{g}^*(\boldsymbol{\theta})$ are polynomial in $\boldsymbol{\Theta}$, the condition $\boldsymbol{g}^*(\boldsymbol{\theta}) \in \mathcal{C}^2$ is trivially satisfied.

Next, since 
\begin{equation*}
		\frac{\partial}{\partial \boldsymbol{\theta}^T}\, \boldsymbol{g}^*(\boldsymbol{\theta}) = 	\begin{bmatrix}
			 1 & 0\\
			2\mu & 1
	 		 \end{bmatrix},
\end{equation*}
we have $J(\boldsymbol{\theta}) \equiv \text{det} (\frac{\partial}{\partial \boldsymbol{\theta}^T}\, \boldsymbol{g}^*(\boldsymbol{\theta}) ) = 1$.

Finally, the last two conditions of Theorem \@ref(thm:Homomorphism) are trivially satisfied since $||\boldsymbol{g}^*(\boldsymbol{\theta})||$ can only diverge if $|| \boldsymbol{\theta} ||$ diverges and since $\boldsymbol{g}^*(\boldsymbol{\theta}) = s$ has (one) countably solutions in $\boldsymbol{\Theta}$.

Therefore, it follows from Lemma \@ref(lem:GMMindentification) and Theorem \@ref(thm:Homomorphism) that the function $Q_0(\boldsymbol{\theta}) = - || \boldsymbol{\gamma}(\boldsymbol{\theta}_0)  - \boldsymbol{\gamma}(\boldsymbol{\theta})  ||^2_{\boldsymbol{W}}$ is uniquely maximized in $\boldsymbol{\theta}_0$.
```

### Verification of Condition C4
In general, the verification of Condition C4 requires pointwise convergence and Lipschitz continuity as specified in the following theorem, which is a slightly adapted version of the Arzela-Ascoli Theorem.

```{theorem, ArzelaAscoli, name = "modified Arzela-Ascoli Theorem"}
Suppose $\boldsymbol{\Theta}$ is compact. If

- for every $\boldsymbol{\theta} \in \boldsymbol{\Theta}$, we have $\hat{Q}_n (\boldsymbol{\theta}) \overset{\mathcal{P}}{\mapsto} Q_0 (\boldsymbol{\theta})$,

- $\hat{Q}_n (\boldsymbol{\theta})$ is almost surely Lipschitz continuous, i.e. $\underset{\boldsymbol{\theta}_1, \boldsymbol{\theta}_2 \in \boldsymbol{\Theta}}{\text{sup}} |\hat{Q}_n (\boldsymbol{\theta}_1) - \hat{Q}_n (\boldsymbol{\theta}_2)| \leq H ||\boldsymbol{\theta}_1 - \boldsymbol{\theta}_2||$ where $H$ is bounded almost surely,

then $\hat{Q}_n (\boldsymbol{\theta})$ converges uniformly in probability to $Q_0 (\boldsymbol{\theta})$.
```

Here we will not discuss how to prove that $\hat{Q}_n (\boldsymbol{\theta})$ is almost surely Lipschitz continuous and assume it for simplicity (more discussion on this topic can for example be found in @newey1994vlarge). Nevertheless, it worth mentioning that this condition is almost always satisfied in practice and is therefore reasonable to assume for simplicity. 

```{example, exmC4, name = "An Example to Verify Condition C4"}
In this example we revisit Example \@ref(exm:ExtreEstGMM). Since we have $Z_i \overset{iid}{\sim} \mathcal{N}(\mu_0, \sigma^2_0)$, we let
\begin{equation*}
    \boldsymbol{X}_i \equiv  \begin{bmatrix}
    Z_i\\
    Z_i^2\\
    Z_i^3
    \end{bmatrix}.
\end{equation*}
So by the Weak Law of Large Number (Theorem \@ref(thm:WLLN)), we have
\begin{equation*}
    \hat{\boldsymbol{\gamma}} = \frac{1}{n} \sum_{i = 1}^n \boldsymbol{X}_i  \overset{\mathcal{P}}{\mapsto} \mathbb{E}[\boldsymbol{X}_i] = \boldsymbol{\gamma}({\boldsymbol{\theta}_0}) \equiv \boldsymbol{\gamma}_0.
\end{equation*}

Define 
\begin{equation*}
\hat{Q}_n (\boldsymbol{\theta}) = -||\hat{\boldsymbol{\gamma}}- \boldsymbol{\gamma} (\boldsymbol{\theta}) ||_{\widehat{\boldsymbol{W}}}^2
\end{equation*}
where $\widehat{\boldsymbol{W}} \overset{\mathcal{P}}{\mapsto} \boldsymbol{W}$, and also define
\begin{equation*}
Q_0(\boldsymbol{\theta}) = -||\boldsymbol{\gamma}(\boldsymbol{\theta}_0) - \boldsymbol{\gamma}(\boldsymbol{\theta})||_\boldsymbol{W}^2.
\end{equation*}

In order to show that $\hat{Q}_n (\boldsymbol{\theta}) \overset{\mathcal{P}}{\mapsto} Q_0(\boldsymbol{\theta})$ for all $\boldsymbol{\theta} \in \boldsymbol{\Theta}$, we want to show that $|\hat{Q}_n (\boldsymbol{\theta}) - Q_0(\boldsymbol{\theta})| \overset{\mathcal{P}}{\mapsto} 0$.

\begin{equation*}
    \begin{aligned}
         \hat{Q}_n (\boldsymbol{\theta}) - {Q}_0 (\boldsymbol{\theta}) &\leq |\hat{Q}_n (\boldsymbol{\theta}) - {Q}_0 (\boldsymbol{\theta})| =   \left|\underbrace{ ||\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma} (\boldsymbol{\theta}) ||_{\widehat{\boldsymbol{W}}}^2 - ||\gamma_0 - \boldsymbol{\gamma} (\boldsymbol{\theta}) ||_\boldsymbol{W}^2}_\boldsymbol{A}\right|.
    \end{aligned}
\end{equation*}

Without loss of generality, we assume that $\boldsymbol{W}^* = \widehat{\boldsymbol{W}} - \boldsymbol{W}$ and that $\boldsymbol{W}$ is symmetric.

\begin{align*}
\boldsymbol{A} &=  \hat{\boldsymbol{\gamma}}^T \widehat{\boldsymbol{W}}\hat{\boldsymbol{\gamma}} + \boldsymbol{\gamma}(\boldsymbol{\theta})^T \widehat{\boldsymbol{W}} \boldsymbol{\gamma}(\boldsymbol{\theta}) - 2\hat{\boldsymbol{\gamma}}^T\widehat{\boldsymbol{W}}\boldsymbol{\gamma}(\boldsymbol{\theta}) - \boldsymbol{\gamma}_0^T \boldsymbol{W} \boldsymbol{\gamma}_0 - \boldsymbol{\gamma}(\boldsymbol{\theta})^T \boldsymbol{W} \boldsymbol{\gamma}(\boldsymbol{\theta}) + 2\boldsymbol{\gamma}_0^T \boldsymbol{W} \boldsymbol{\gamma}(\boldsymbol{\theta})  \\ 
&= ||\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}_0||_{\widehat{\boldsymbol{W}}}^2 - \boldsymbol{\gamma}_0^T \widehat{\boldsymbol{W}} \boldsymbol{\gamma}_0 + 2\widehat{\boldsymbol{\gamma}}^T \widehat{\boldsymbol{W}} \boldsymbol{\gamma}_0 \\
&+ \boldsymbol{\gamma}(\boldsymbol{\theta})^T \widehat{\boldsymbol{W}} \boldsymbol{\gamma}(\boldsymbol{\theta}) - 2\hat{\boldsymbol{\gamma}}^T\widehat{\boldsymbol{W}}\boldsymbol{\gamma}(\boldsymbol{\theta}) - \boldsymbol{\gamma}_0^T \boldsymbol{W} \boldsymbol{\gamma}_0 - \boldsymbol{\gamma}(\boldsymbol{\theta})^T \boldsymbol{W} \boldsymbol{\gamma}(\boldsymbol{\theta}) + 2\boldsymbol{\gamma}_0^T \boldsymbol{W} \boldsymbol{\gamma}(\boldsymbol{\theta})  \\ 
&= ||\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}_0||_{\widehat{\boldsymbol{W}}}^2 + ||\boldsymbol{\gamma}_0 - \boldsymbol{\gamma}(\boldsymbol{\theta})||_{\boldsymbol{W}^*}^2 \\
&- 2\boldsymbol{\gamma}_0^T\widehat{\boldsymbol{W}}\boldsymbol{\gamma}_0 - 2\hat{\boldsymbol{\gamma}}^T \widehat{\boldsymbol{W}} \boldsymbol{\gamma}(\boldsymbol{\theta}) + 2\boldsymbol{\gamma}_0^T \widehat{\boldsymbol{W}} \boldsymbol{\gamma}(\boldsymbol{\theta}) + 2\hat{\boldsymbol{\gamma}}^T \widehat{\boldsymbol{W}} \boldsymbol{\gamma}_0 \\
&= ||\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}_0||_{\widehat{\boldsymbol{W}}}^2 + ||\boldsymbol{\gamma}_0 - \boldsymbol{\gamma}(\boldsymbol{\theta})||_{\boldsymbol{W}^*}^2 + 2(\boldsymbol{\gamma}_0 - \boldsymbol{\gamma}(\boldsymbol{\theta}))^T \widehat{\boldsymbol{W}} (\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}_0).
\end{align*}

So by triangular inequality, we have 
\begin{align*}
|\hat{Q}_n (\boldsymbol{\theta}) - {Q}_0 (\boldsymbol{\theta})| &\leq  \left| \underbrace{\|\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}_0)  \|_{\widehat{\boldsymbol{W}}}^2}_{\equiv a_1} \right| + \left| \underbrace{\|\boldsymbol{\gamma}(\boldsymbol{\theta}_0) - \boldsymbol{\gamma} (\boldsymbol{\theta}) \|_{{\boldsymbol{W}}^*}^2}_{\equiv a_2} \right| \\ 
&+ \left| \underbrace{2 \left( \boldsymbol{\gamma}(\boldsymbol{\theta}_0) - \boldsymbol{\gamma} (\boldsymbol{\theta})\right)^T \widehat{\boldsymbol{W}} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}_0)\right)}_{\equiv a_3}\right|. 
\end{align*}

Before continuing, suppose that  $\boldsymbol{x}$ is a vector and $\boldsymbol{W}$ the above defined matrix, we have that $\boldsymbol{x}^T \boldsymbol{W} \boldsymbol{x} \leq \lambda_1 ||\boldsymbol{x}||^2$, where $\lambda_1$ is the largest eigenvalue of $\boldsymbol{W}$. Furthermore, let us also define the Frobenius norm of a matrix as $||\boldsymbol{W}|| = (\sum_i^N \sum_j^J w_{i,j}^2)^{\frac{1}{2}}$ and $||\boldsymbol{W}||^2 = \sigma_{\text{max}} \leq ||\boldsymbol{W}||$, where $\sigma_{\text{max}}$ is the largest singular value of $||\boldsymbol{W}||$.

Using those properties, we can investigate the terms in the above equation. 

Considering $a_1$ , we have 
\begin{align*}
a_1 &\leq ||\boldsymbol{\gamma}_0 - \boldsymbol{\gamma} (\boldsymbol{\theta}) ||^2 \lambda_1  \leq ||\boldsymbol{\gamma}_0 - \boldsymbol{\gamma} (\boldsymbol{\theta}) ||^2 \|\boldsymbol{W}^*|| \\ 
&= \sum^3_{i = 1} \left(\boldsymbol{\gamma}_{0_i} - \boldsymbol{\gamma}_i (\boldsymbol{\theta})\right)^2  \sqrt{\sum^3_{i = 1} \sum^3_{j = 1} \left(\hat{w}_{i,j} - w_{i,j}\right)^2} \\
&\leq 3 \underset{i}{\text{max}}  \left(\boldsymbol{\gamma}_{0_i} - \boldsymbol{\gamma}_i (\boldsymbol{\theta})\right)^2 \cdot 3 \underset{i}{\text{max}} \sqrt{\left(\hat{w}_{i,j} - w_{i,j}\right)^2}
\end{align*}

Since $\underset{i}{\text{max}}  \left(\boldsymbol{\gamma}_{0_i} - \boldsymbol{\gamma}_i (\boldsymbol{\theta})\right)^2$ is bounded by conditions C1 to C3, and $\underset{i}{\text{max}} \sqrt{\left(\hat{w}_{i,j} - w_{i,j}\right)^2} \overset{\mathcal{P}}{\mapsto} 0$ by $\widehat{\boldsymbol{W}} \overset{\mathcal{P}}{\mapsto} \boldsymbol{W}$, we show that $a_1 \overset{\mathcal{P}}{\mapsto} 0$.

Similarly, we can also show that $a_2 \overset{\mathcal{P}}{\mapsto} 0$ using the fact that he sample moments converge to the population ones.

Finally, considering the previous results on $a_1$ and $a_2$, we can see that the last term $a_3$ also tends to 0. Therefore, we can say that for all $\boldsymbol{\theta} \in \boldsymbol{\Theta}$, 

\begin{equation*}
|\hat{Q}_n (\boldsymbol{\theta}) - Q_0(\boldsymbol{\theta})| \overset{\mathcal{P}}{\mapsto} 0.
\end{equation*}
```

In general, showing that $\hat{Q}_n (\boldsymbol{\theta}) \overset{\mathcal{P}}{\mapsto} Q_0(\boldsymbol{\theta})$ for all $\boldsymbol{\theta} \in \boldsymbol{\Theta}$ for every $\boldsymbol{\theta} \in \boldsymbol{\Theta}$ is generally done using the Weak Law of Large Number (i.e. Theorem \@ref(thm:WLLN)). However, when $X_i$ which are not iid random variables, Theorem \@ref(thm:WLLN)) cannot be applied. The following theorem (taken from Proposition 7.5 of @hamilton1994time) generalizes this result for (weak) stationary processes with absolutely summable covariance structure. 

```{theorem, WLLNdep, name = "Weak Law of Large Number for Dependent Process"}
Suppose $(X_t)$ is a (weak) stationary process with absolutely summable autocovariance structure, then 
\begin{equation*}
\bar{X}_T \overset{\mathcal{P}}{\mapsto} \mathbb{E}[X_t].
\end{equation*}
```

```{proof, proofWLLNdep, name = "Weak Law of Large Number for Dependent Process"}
Since $\bar{X}_n - \mu$ has mean zero, its variance is
\begin{equation*}
\mathbb{E}[(\bar{X}_n - \mu)^2] = 1/n\sum_{h=-\infty}^{\infty} \gamma(h) \leq 1/n\sum_{h=-\infty}^{\infty} |\gamma(h)| \leq C/n.
\end{equation*}

By Chebychev's inequality, for all $\epsilon > 0$,
\begin{equation*}
\mathbb{P} \left(| \bar{X}_n - \mu | \geq \epsilon \right) \leq \frac{\mathbb{E}[(\bar{X}_n - \mu)^2]}{\epsilon^2} \leq \frac{C}{\epsilon^2n},
\end{equation*}
so that
\begin{equation*}
\underset{n \to \infty}{\text{lim}} \mathbb{P} \left(| \bar{X}_n - \mu | \geq \epsilon \right) \to 0,
\end{equation*}
which concludes the proof.
```


```{example, ConsistSampleMean, name = "Consistency of Sample Mean"}
Consider a stationary AR1 process and suppose we want to study whether its sample mean converges in probability to its expected value. This time we consider a non-zero mean AR1, i.e.
\begin{equation*}
    \left(X_t - \mu\right) = \phi \left(X_{t-1} - \mu\right) + Z_t,
\end{equation*}
where $\mu = \mathbb{E}[X_t]$, $Z_t \overset{iid}{\sim} \mathcal{N} (0, \nu^2)$ and $\nu^2 < \infty$. This process can also be written as a linear process:
\begin{equation*}
    X_i - \mu = \sum_{k=0}^{\infty}\phi^k Z_{i-k}.
\end{equation*}

Since the process is stationary for $|\phi| < 1$, we have
\begin{equation*}
\gamma_h = \frac{\nu^2 \phi^{|h|}}{1-\phi^2},
\end{equation*}
for $h \in \mathbb{Z}$.

Then we have 
\begin{equation*}
\sum_{h = -\infty}^{\infty} |\gamma_k| = \sum_{h = -\infty}^{\infty} \frac{\nu^2 |\phi|^{|h|}}{1-\phi^2} < 2 \lim_{n\to\infty}\frac{\nu^2 (1-|\phi|^{n+1})}{(1-\phi^2)(1-|\phi|)} = \frac{2\nu^2 }{(1-\phi^2)(1-|\phi|)} < \infty,
\end{equation*}
implying that the process has an absolutely summable covariance structure. Therefore, by applying Theorem \@ref(thm:WLLNdep), we can verify that
\begin{equation*}
    \bar{X}_T = \frac{1}{T} \sum_{t = 1}^T \, X_t \overset{\mathcal{P}}{\mapsto} \mathbb{E}[X_t] = \mu.
\end{equation*}
```


### Consistency of Sample AutoCovariance and AutoCorrelation Functions
In Chapter 2, we have defined the sample autocovariance andautocorrelation functions. In the following, we will show that these estimators are both consistent.

```{corollary, ConsistACVACF, name = "Consistency of Sample AutoCovariance and AutoCorrelation Functions"}
Let $(X_t)$ be such that $(X_t)$ is weakly stationary and $(X_t^2)$ has an absolutely summable covariance structure, then for all $|h| < \infty$, we have
\begin{align*}
\hat{\gamma}(h) &\overset{\mathcal{P}}{\mapsto} \gamma(h),\\
\hat{\rho}(h) &\overset{\mathcal{P}}{\mapsto} \rho(h).
\end{align*}
```

```{proof, proofConsistACVACF, name = "Consistency of Sample AutoCovariance and AutoCorrelation Functions"}
$(X_t^2)$ has an absolutely summable covariance structure implies that both $(X_t)$ and $(X_tX_{t-h})$ for all $h \in \mathbb{Z}$ have absolutely summable covariance structures. Under the conditions that $(X_t)$ is weakly stationary and has absolutely summable covariance structure, we have $\bar{X}_T \overset{\mathcal{P}}{\mapsto} \mu$. Let
\begin{equation*}
\tilde{\gamma}(h) = \frac{1}{T} \sum_{t = h+1}^{T} \left(X_{t} - \mu\right) \left(X_{t-h} - \mu\right).
\end{equation*}

Since $(\left(X_{t} - \mu\right) \left(X_{t-h} - \mu\right))$ is stationary and has absolutely summable covariance structure, we have $\tilde{\gamma}(h) \overset{\mathcal{P}}{\mapsto} \gamma(h)$. We can also show that $\sqrt{T} \left(\tilde{\gamma}(h) - \hat{\gamma}(h)  \right) = o_p(1)$. Therefore, we obtain
\begin{equation*}
\hat{\gamma}(h) \overset{\mathcal{P}}{\mapsto} \gamma(h).
\end{equation*}

Similarly, we can show that 
\begin{equation*}
\left( \hat{\gamma}(0), \hat{\gamma}(h) \right)^T \overset{\mathcal{P}}{\mapsto} \left( \gamma(0), \gamma(h) \right)^T.
\end{equation*}
	
Then by Theorem \@ref(thm:CMT), we have
\begin{equation*}
\hat{\rho}(h) \overset{\mathcal{P}}{\mapsto} \rho(h),
\end{equation*}
which concludes the proof.
```




## Asymptotic Normality
The Central Limit Theorem (CLT) takes one step further than the Law of Large Number. It identifies the limiting distribution of the (properly scaled) sum of random variables as the normal distribution, which allows us to do the statistical inference (confidence interval and hypothesis testing). The scale will tell us how fast this approximation converges to the normal distribution. 

### CLT for iid Random Variables
```{theorem, CLT, name = "CLT for iid sequences"}
Suppose $X_i$ are iid random variables with $\mathbb{E}[X_i] = \mu$ and $\text{Var}(X_i) = \sigma^2 < \infty$. Let $\bar{X}_n = \frac{1}{n} \sum_{i = 1}^n X_i$, then $\sqrt{n}\left(\bar{X}_n - \mu\right) \overset{\mathcal{D}}{\mapsto} \mathcal{N}(0, \sigma^2)$.
```

```{theorem, CLTmulti, name = "CLT for iid multivariate sequences"}
Suppose $\boldsymbol{X}_i$ are iid random vectors with $\mathbb{E}[\boldsymbol{X}_i] = \boldsymbol{\mu} \in \mathbb{R}^d$ and $\boldsymbol{\text{Cov}}(\boldsymbol{X}_i) = \boldsymbol{\Sigma} \in \mathbb{R}^{d \times d}$. Then, we have $\sqrt{n}\left(\bar{\boldsymbol{X}}_n - \boldsymbol{\mu}\right) \overset{\mathcal{D}}{\mapsto}  \mathcal{N}(\boldsymbol{0}, \boldsymbol{\Sigma})$.
```

```{theorem, slutsky, name = "Slutsky's Theorem"}
Let $X_n$, $Y_n$ and $X$ be random variables. If $X_n \overset{\mathcal{D}}{\mapsto} X$ and $Y_n \overset{\mathcal{P}}{\mapsto} c$ for some constant $c$, then

- $X_n + Y_n \overset{\mathcal{D}}{\mapsto} X+c$,
- $X_n Y_n \overset{\mathcal{D}}{\mapsto} cX$,
- $X_n / Y_n \overset{\mathcal{D}}{\mapsto} X/c$ if $c \neq 0$.
```


General results on the asymptotic normality of extremum estimators can be found in @newey1994vlarge. In this section, we will only restrict our attention to a simple example of GMM estimators as in Example \@ref(exm:ExtreEstGMM). We will see that the asymptotic normality of extremum estimators is implied by combining the following results and techniques:

- Consistency of $\hat{\boldsymbol{\theta}}$,
- Central Limit Theorem (see Theorem \@ref(thm:CLTmulti)),
- Slutsky's Theorem (see Theorem \@ref(thm:slutsky)),
- Taylor expansions.

```{example, exmAsympNormality, name = "An Example to Prove Asymptotic Normality of Extremum Estimators"}
In this example we revisit Example \@ref(exm:ExtreEstGMM). So we have 

\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} \hat{Q}_n(\boldsymbol{\theta}) = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmax}} -||\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}) ||_\widehat{\boldsymbol{W}}^2
\end{equation*}
where
\begin{equation*}
\hat{\boldsymbol{\gamma}} = \frac{1}{n}  \sum_{i = 1}^n 
\begin{bmatrix}
Z_i\\
Z_i^2\\
Z_i^3\\
\end{bmatrix}, \;\;\;\;
\boldsymbol{\gamma}(\boldsymbol{\theta}) = 	\begin{bmatrix}
\mu\\
\mu^2 + \sigma^2\\
\mu^3 + 3 \mu \sigma^2
\end{bmatrix}.
\end{equation*}

So by CLT (Theorem \@ref(thm:CLTmulti)), we have
\begin{equation*}
\sqrt{n}\left(\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}_0)\right)  = \frac{1}{\sqrt{n}}  \sum_{i = 1}^n 
	\begin{bmatrix}
	  Z_i - \mu_0\\
	 Z_i^2 - \mu_0^2 - \sigma_0^2\\
	 Z_i^3 - \mu_0^3 - 3 \mu_0 \sigma_0^2\\
	 \end{bmatrix} \overset{\mathcal{D}}{\mapsto} \mathcal{N}(\boldsymbol{0}, \boldsymbol{\Sigma}),
\end{equation*}

where $\boldsymbol{\Sigma} = \text{Cov}\left( \begin{bmatrix}
	  Z_i & Z_i^2& Z_i^3\\
	 \end{bmatrix}^T \right)$.
	 
Since $\hat{\boldsymbol{\theta}}$ maximizes $\hat{Q}_n(\boldsymbol{\theta})$, we have
\begin{equation*}
\frac{\partial \hat{Q}_n(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \bigg\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}} = \frac{\partial}{\partial \boldsymbol{\theta}} \left(\left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}) \right) \right)^T \widehat{\boldsymbol{W}} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}) \right) \bigg\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}} = \boldsymbol{0}.
\end{equation*}

Using Taylor expansion for $\boldsymbol{\gamma}(\hat{\boldsymbol{\theta}})$ around the true $\boldsymbol{\theta}_0$, we obtain
\begin{equation*}
\hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\hat{\boldsymbol{\theta}})  = \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}_0) + \frac{\partial \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}) \right)}{\partial \boldsymbol{\theta}}\bigg\rvert_{\boldsymbol{\theta} = \boldsymbol{\theta}_0} \left( \hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0 \right) + o_p(1).
\end{equation*}

Under certain regularity conditions, we have (using Theorem \@ref(thm:CMT)) that

\begin{equation*}
\frac{\partial}{\partial \boldsymbol{\theta}} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}) \right)\bigg\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}} = - \frac{\partial}{\partial \boldsymbol{\theta}}  \boldsymbol{\gamma}(\boldsymbol{\theta}) \bigg\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}}  \overset{\mathcal{P}}{\mapsto} - \frac{\partial}{\partial \boldsymbol{\theta}}  \boldsymbol{\gamma}(\boldsymbol{\theta}) \bigg\rvert_{\boldsymbol{\theta} = \boldsymbol{\theta}_0}.
\end{equation*}

So under certain regularity conditions, using the above equations and Slutsky's Theorem, we can obtain that 

\begin{equation*}
\sqrt{n}\left( \hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0 \right) \overset{\mathcal{D}}{\mapsto}  \mathcal{N}\left(\boldsymbol{0}, \boldsymbol{D}^T \boldsymbol{\Sigma} \boldsymbol{D}\right),
\end{equation*}

where 

\begin{equation*}
\boldsymbol{D} = \left[ ||\frac{\partial}{\partial \boldsymbol{\theta}} \left(  \boldsymbol{\gamma}(\boldsymbol{\theta})\right)||_{\boldsymbol{W}}\rvert_{\boldsymbol{\theta} = \boldsymbol{\theta}_0} \right]^{-1} \left(\frac{\partial}{\partial \boldsymbol{\theta}}  \boldsymbol{\gamma}(\boldsymbol{\theta})\bigg\rvert_{\boldsymbol{\theta} = \boldsymbol{\theta}_0} \right)^T \boldsymbol{W}
\end{equation*}

due to the fact that 

\begin{equation*}
\begin{aligned}
&\sqrt{n}\left( \hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0 \right) 
        = \overbrace{-\left[ \left(\frac{\partial}{\partial \boldsymbol{\theta}} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}) \right)\bigg\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}} \right)^T \widehat{\boldsymbol{W}} \left( \frac{\partial}{\partial \boldsymbol{\theta}} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}) \right)\bigg\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}} \right) \right]^{-1}}^{ \overset{p}{\to} \left[ ||\frac{\partial}{\partial \boldsymbol{\theta}} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta})\right)||_{\boldsymbol{W}}\rvert_{\boldsymbol{\theta} = \boldsymbol{\theta}_0} \right]^{-1}}\\
&\underbrace{\left(\frac{\partial}{\partial \boldsymbol{\theta}} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}) \right)\bigg\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}} \right)^T \widehat{\boldsymbol{W}}}_{\overset{p}{\to} \left(\frac{\partial}{\partial \boldsymbol{\theta}} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta})\right)\rvert_{\boldsymbol{\theta} = \boldsymbol{\theta}_0} \right)^T \boldsymbol{W}} \sqrt{n} \left( \hat{\boldsymbol{\gamma}} - \boldsymbol{\gamma}(\boldsymbol{\theta}_0) \right) + o_p(1).
\end{aligned}
\end{equation*}
```


### CLT for Dependent Processes
For a dependent process, the validity of CLT requires the process to be "mixing" or "asymptotically independent". Suppose two events $G$ and $H$ are independent, then $$|\mathbb{P}(G \cap H) - \mathbb{P}(G) \mathbb{P}(H)| = 0.$$

Based on this idea, many dependence measures have been developed. The most commonly used $\alpha$-mixing coefficient is one of these dependence measures which can be easily verified for certain stochastic processes.

```{definition, mixcoef, name = "Mixing Coefficients"}
For a stochastic process $\{ X_i \}_{i \in \mathbb{Z}}$, we define the strong- or $\alpha$-mixing coefficients as

\begin{equation*}
\alpha(t_1, t_2) = \text{sup}\{ |\mathbb{P}(A \cap B) - \mathbb{P}(A)\mathbb{P}(B)|: \; A\in\mathcal{F}_{-\infty}^{t_1}, B\in\mathcal{F}_{t_2}^{\infty} \},
\end{equation*}
where $\mathcal{F}_{-\infty}^{t_1} = \sigma(X_{-\infty}, \dots, X_{t_1})$ and $\mathcal{F}_{t_2}^{\infty} = \sigma(X_{t_2}, \dots, X_{\infty})$ are $\sigma$-algebras generated by corresponding random variables.
```

```{exercise, MixCoefRemark, name = "Mixing Coefficients"}
If the process is stationary, then $\alpha(t_1, t_2) = \alpha(t_2, t_1) = \alpha(|t_1-t_2|) \equiv \alpha(\tau)$. If $\alpha(\tau) \to 0$ as $\tau \to \infty$, then the process is strong-mixing or $\alpha$-mixing.
```

```{theorem, CLTalpha, name = "Central Limit Theorem and Alpha-Mixing"}
Let $(X_t)$ be a strictly stationary process with $\mathbb{E}[X_t] =0$. $S_n \equiv \sum_{t=1}^{n}X_t$ is the partial sum process with $\sigma_n^2 \equiv \text{Var}(S_n)$. Suppose $(X_t)$ is $\alpha$-mixing, and that for $\delta > 0$, we have 

\begin{equation*}
\mathbb{E}\left[ |X_t|^{2+\delta} \right] \leq \infty, \mbox{ and } \sum_{n = 0}^{\infty} \alpha(n)^{\delta/2 + \delta} \leq \infty.
\end{equation*}

Then 
\begin{equation*}
\underset{n \to \infty}{\text{lim}}\frac{\sigma_n^2}{n} = \mathbb{E}\left[ |X_t|^{2} \right] + 2\sum_{k=1}^{\infty}\mathbb{E}\left[ X_1X_k \right] \equiv \sigma^2.
\end{equation*}

If $\sigma^2 > 0$, $(X_t)$ obeys both the Central Limit Theorem with variance $\sigma^2$, and the functional Central Limit Theorem.
```


```{exercise, ImpAlphaMix, name = "Implication of Alpha-Mixing"}
If a process $(X_t)$ is $\alpha$-mixing, then its covariance structure is absolutely summable.
```


### Asymptotic Normality of Sample AutoCovariance and AutoCorrelation Functions
If $(X_t)$ is a white noise, then $\hat{\rho}(h)$ should be equal to 0 if $h \neq 0$. In practice, this is of course not the case due the estimation error of $\hat{\rho}(h)$. The next result gives us a way to assess whether the data comes from a completely random series or whether correlations are statistically significant at some lags.

```{theorem, DistACF, name = "Distribution of Sample ACF in iid Case"}
If $(X_t)$ is white noise (with finite variance) and $h = 1, ..., H$ where $H$ is fixed but arbitrary, then we have that
\begin{equation*}
\sqrt{T} \left(\hat{\rho}(h) - {\rho}(h)\right) \overset{\mathcal{D}}{\mapsto}  \mathcal{N}\left(0, 1\right).
\end{equation*}
```

The proof of Theorem \@ref(thm:DistACF) is straightforward from the CLT and Delta method. It is therefore omitted here but can for example be found in @hamilton1994time.

Theorem \@ref(thm:DistACF) implies that an approximate confidence interval for $\hat{\rho}(h)$ (in the iid case) is given by
$$\text{CI}({\rho}(h), \alpha) = \hat{\rho}(h) \pm \frac{z_{1-\frac{\alpha}{2}} }{\sqrt{T}}$$
for $0 < h < k < \infty$ and where $z_{1- \frac{\alpha}{2}} \equiv \boldsymbol{\Phi}^{-1}\left( 1- \frac{\alpha}{2} \right)$ is the $(1- \frac{\alpha}{2})$ quantile of a standard normal distribution. Typically, for $\alpha = 0.05$ one would consider the following confidence interval:
$$\text{CI}({\rho}(h), 0.05) = \hat{\rho}(h) \pm \frac{2}{\sqrt{T}}.$$